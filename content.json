{"posts":[{"title":"Bspline","text":"Bspline上任意点的计算公式 Bspline和贝塞尔曲线类似，也是将所有控制点用基函数\\(N_{i}^{k}\\)进行加权得到曲线上的点。也就是说，Bspline有\\(n+1\\)个控制点，\\(n+1\\)个基函数，\\(k\\)为次数（t在基函数中出现的最高次幂），\\(i\\in[0,n]\\)。比如有10个控制点\\(p_0,p_1,p_2,p_3,...,p_{9}\\)，使用4次的Bspline基函数\\(N^{4}_{0}(t),N^{4}_{1}(t),N^{4}_{2}(t),N^{4}_{3}(t),...,N^{4}_{9}(t)\\)，那么曲线上任意一点可以表示为： \\[ \\begin{align*} f(t)&amp;=\\sum_{i=0}^{n}N_{i}^{k}(t)p_i\\\\ &amp;=N_{0}^{4}p_0+N_{1}^{4}p_1+N_{2}^{4}p_2...+N_{9}^{4}p_{9}\\tag{1} \\end{align*} \\] 节点 与贝塞尔曲线不同的是，Bspline除了控制点，基函数之外还有\\(m+1\\)个节点\\(t_0,t_1,...,t_m\\)。B样条基函数是分段k阶（k-1次）多项式，它们由节点向量(knot vector)唯一决定。\\(n\\)和\\(m,k\\)有如下关系： \\[ m=n+k\\tag{2} \\] 注意这里的k是指基函数中t的最高次幂而不是阶数，最高次幂为0时，阶数为1。如果阶数是k+1，那么关系为\\(m=n+k+1\\)。给出公式后就可以解释为什么有这个关系。 也就是说，在上面这个例子里，\\(m=9+4=13\\)，节点个数是14。为了方便理解，我们可以认为节点是用来生成基函数的辅助量。节点向量则是一串非减(non-decreasing)的实数序列，比如\\([0,0,\\frac{1}{11},\\frac{2}{11},\\frac{3}{11},\\frac{4}{11},\\frac{5}{11},\\frac{6}{11},\\frac{7}{11},\\frac{8}{11},\\frac{9}{11},\\frac{10}{11},1,1]\\)。一般来说节点的值是在\\([0,1]\\)之间，但是也可以不是。 Bspline基函数的定义 有了节点就可以计算基函数了，基函数的公式是递归定义的： \\[ N_{j}^{0}(t):=\\left\\{ {\\begin{matrix}1&amp;\\quad t_{j}&lt;t&lt;t_{ {j+1} }\\\\0&amp; {...}\\end{matrix} }\\right.\\\\ N_{j}^{k}(t):={\\frac {t-t_{ {j} }}{t_{ {j+k} }-t_{j} }}N_{j}^{k-1}(t)+{\\frac {t_{ {j+k+1} }-t}{t_{ {j+k+1} }-t_{ {j+1} }} }N_{j+1}^{k-1}(t).\\tag{3} \\] 看起来比较复杂，实际上从中可以看出对于节点\\(t_j\\)的几个特点： 0次的时候是一个仅在\\([t_j,t_{j+1}]\\)区间为1，其他地方的值为0的0次函数。从递归形式可以看出递归次数和次幂相同。 \\(k\\)阶的基函数是由\\(k-1\\)阶的第\\(j\\)和\\(j+1\\)个基函数决定的。而且前一个基函数的权重是正的，后一个是负的（前一个t是正号，后一个的t是负号）。 从第一点可以看出0次的时候每个\\(t_j\\)分别在各自的\\([t_j,t_{j+1}]\\)上是1，其他地方是0。结合1，2点可以看出，\\(f(t)\\)在\\([t_0,t_{m+1}]\\)上不为0，其他位置为0。\\(N_j^k (t)\\)是\\([t_j,t_{j+k} ]\\)上的分段非零多项式。 随着\\(N\\)的次数（上标）增加到\\(k\\)，第\\(j\\)个节点的值受到第\\(j\\)到\\(j+k\\)个节点的值的影响。这个特点结合来自曲线篇：深刻理解B 样条曲线（上）的下图看更直观。 基函数的递归计算图 因为只有\\(N_j^{k}(t)\\)不为0时第\\(j\\)个控制点才有对计算贡献。从第四个特点可以得到一个重要的观察：在\\([t_j,t_{j+k+1}]\\)这个区间上第\\(j\\)个控制点才会用上，即Bspline具有局部性：每个控制点仅在\\([t_j,t_{j+k+1}]\\)内对曲线的形状有影响。若n非常小，改变了一个控制点的位置，影响的范围也是非常小的。 该图中还可以得到另一个重要的观察：当我们使用\\(k\\)次的基函数计算公式\\((1)\\)时，区间\\([t_j,t_{j+1}]\\)上仅有\\(N_{j-k}^k,...,N_{j}^k\\)有贡献，也就是说，这个区间上的曲线的形状由\\(k\\)个控制点决定。 上面的两个观察可以概括为Bspline的局部支持性： 区间\\(t\\in[t_j,t_{j+1}]\\)上的曲线仅由至多k个控制点决定。 修改控制点\\(p_i\\) 仅会影响到区间\\([t_j,t_{j+k+1}]\\)上的曲线。 这里就可以解释为什么\\(m=n+k\\)。因为我们计算曲线上的点使用了公式\\((1)\\)，它需要\\(N_{0}^{k},N_{1}^{k},...,N_{n}^{k}\\)。但是从\\(N_{0}^{0},...,N_{m}^{0}\\)开始递归，每一次递归都会少一项，因为\\(N_{m}^{1}\\)只能由\\(N_{m}^{1},N_{m+1}^{1}\\)得到，但是\\(N_{m+1}^{1}\\)不存在，所以实际上当计算到\\(k=1\\)时，只能计算到\\(N_{0}^{1},...,N_{m-1}^{1}\\)。以此类推，计算到\\(k\\)时，只能计算到\\(N_{0}^{k},...,N_{m-k}^{k}\\)，最后的这一项就对应\\(N_{n}^{k}\\)。 在下面这个例子中，有四个控制点，最高三次，即\\(k=3,n=3\\)，总共6个节点。图中只画出部分基函数的曲线。 从图中可以看到，同一个颜色的曲线对应着同一个序号\\(j\\)的\\(N_j^k(t)\\)，他们的左端点都是不变的，因为\\((2)\\)公式左端始终乘的是\\(t-t_j\\)，也就是说\\(t=t_j\\)时的值一定为0。 Bspline插值 已知\\(m+1\\)个点\\((t_0,p_0),...,(t_m,p_m)\\)，计算一条穿过所有点的Bspline的公式\\(f(t)\\)。 因为\\(f(t)\\)需要在\\(t\\in[t_j,t_{j+1}]\\)上有定义，需要左右各增加\\(k\\)（次数）个节点。所有要确定节点向量\\([\\lambda_0,...\\lambda_{m+2k}]\\)，为方便计算，节点向量均匀分布，为 \\[ \\begin{align*} \\mathbf{\\lambda}&amp;=\\begin{bmatrix} t_{-3}&amp;t_{-2}&amp;t_{-1}&amp; \\vdots&amp; t_0 &amp;... &amp;t_{m} &amp;\\vdots&amp;t_{m+1}&amp;t_{m+2}&amp;t_{m+3}\\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} \\lambda_{0}&amp;\\lambda_{1}&amp; \\lambda_2 &amp;... &amp;\\lambda_{m+2n}\\end{bmatrix} \\end{align*} \\] 由此可以计算矩阵\\(\\mathbf{A}\\)。 For the cubic case \\((k = 3)\\) with natural spline endpoint conditions (i.e. second derivative = 0 at \\(t_0\\) and \\(t_m\\))。 \\[ \\mathbf{A}=\\begin{bmatrix} { }^{(2)} N_0^3(t_0)&amp; { }^{(2)} N_1^3(t_0)&amp; { }^{(2)} N_2^3(t_0)&amp; { }^{(2)} N_3^3(t_0)\\\\ N_0^3(t_0) &amp; N_1^3(t_0) &amp; N_2^3(t_0) &amp; N_3^3(t_0)\\\\ &amp; N_1^3(t_1) &amp; N_2^3(t_1) &amp; N_3^3(t_1) &amp; N_4^3(t_1)\\\\ &amp; &amp; N_2^3(t_2) &amp; N_3^3(t_2) &amp; N_4^3(t_2) &amp; N_5^3(t_2)\\\\ \\\\ &amp; &amp;\\ddots&amp; &amp; \\ddots &amp; \\ddots &amp; &amp;\\\\ \\\\ &amp; &amp; N_{m-3}^3(t_{m-3}) &amp; N_{m-2}^3(t_{m-3}) &amp; N_{m-1}^3(t_{m-3}) &amp; N_m^3(t_{m-3}) \\\\ &amp; &amp; &amp;N_{m-2}^3(t_{m-2}) &amp; N_{m-1}^3(t_{m-2}) &amp; N_m^3(t_{m-2}) &amp; N_{m+1}^3(t_{m-2}) \\\\ &amp; &amp; &amp; &amp; N_{m-1}^3(t_{m-1}) &amp; N_m^3(t_{m-1}) &amp; N_{m+1}^3(t_{m-1}) &amp; N_{m+2}^3(t_{m-1}) \\\\ &amp; &amp; &amp; &amp; N_{m-1}^3(t_m) &amp; N_m^3(t_m) &amp; N_{m+1}^3(t_m) &amp; N_{m+2}^3(t_m) \\\\ &amp; &amp; &amp; &amp; { }^{(2)} N_{m-1}^3(t_m) &amp;{ }^{(2)} N_m^3(t_m) &amp;{ }^{(2)} N_{m+1}^3(t_m) &amp; { }^{(2)} N_{m+2}^3(t_m) \\\\ &amp; \\end{bmatrix} \\] 当\\(k=3\\)时，矩阵\\(\\mathbf{A}\\)为\\((m+3)\\times(m+3)\\)。 向量\\(d=\\begin{bmatrix}0 &amp;p_0&amp;p_1&amp;...&amp;p_m&amp;0\\end{bmatrix}\\)。 求得系数为： \\[ c=\\mathbf{A}^{-1}d,\\\\ \\] 则可以得到Bspline的公式： \\[ f(t)=\\sum_{j=0}^{m+k-1}c_jN_j^k(t) \\] 参考 曲线篇：深刻理解B 样条曲线（上）","link":"/2023/03/04/BSpline/"},{"title":"FEM中的梯度计算公式证明与matlab验证","text":"证明 首先，已知 \\[ \\vec {\\mathbf{x}} = \\begin{bmatrix} \\vec x_1 &amp; \\vec x_2 &amp; \\vec x_3 \\end{bmatrix}\\\\ D_s = \\begin{bmatrix} \\vec x_1-\\vec x_4 &amp; \\vec x_2-\\vec x_4 &amp; \\vec x_3-\\vec x_4 \\end{bmatrix}\\\\ \\begin{aligned} &amp;\\frac{\\partial (D_s)_{kl}}{\\partial \\vec {\\mathbf{x}_{ij}}} e_i\\otimes e_j\\otimes e_k\\otimes e_l\\\\ &amp;=\\frac{\\partial \\vec {\\mathbf{x}}_{kl}}{\\partial \\vec {\\mathbf{x}}_{ij}} e_i\\otimes e_j\\otimes e_k\\otimes e_l\\\\ &amp;= \\delta_{ik}\\delta_{jl} e_i\\otimes e_j\\otimes e_k\\otimes e_l \\end{aligned} \\] \\(D_m^{-1}\\)的分量表示为\\(d_{mn}\\)，\\(P\\)的分量表示为\\(P_{rs}\\)，则能量密度函数\\(\\Psi\\)关于位置\\(\\vec {\\mathbf{x}}\\) 的梯度为： \\[ \\begin{aligned} \\frac{\\partial \\Psi}{\\partial \\vec {\\mathbf{x}}} &amp;= \\frac{\\partial F}{\\partial \\vec {\\mathbf{x}}} : \\frac{\\partial \\Psi}{\\partial F}\\\\ &amp;= : \\frac{\\partial F}{\\partial \\vec {\\mathbf{x}}}: P \\\\ &amp;= : (\\frac{\\partial D_s}{\\partial \\vec {\\mathbf{x}}} D_m^{-1}): P \\\\ &amp;= : (\\delta_{ik}\\delta_{jl} e_i\\otimes e_j\\otimes e_k\\otimes e_l\\cdot (d_{mn} e_m\\otimes e_n)): P_{rs} e_r\\otimes e_s \\\\ &amp;= : (\\delta_{ik}\\delta_{jl}\\delta_{lm} d_{mn} e_i\\otimes e_j\\otimes e_k\\otimes e_n): P_{rs} e_r\\otimes e_s \\\\ &amp;= : (\\delta_{ik} d_{jn} e_i\\otimes e_j\\otimes e_k\\otimes e_n): P_{rs} e_r\\otimes e_s \\\\ &amp;= P_{kn} \\delta_{ik} d_{jn} e_i\\otimes e_j\\\\ &amp;= P_{in} d_{jn} e_i\\otimes e_j\\\\ &amp;= P D_m^{-T} \\end{aligned} \\] Matlab验证 下面的验证里将\\(\\vec {\\mathbf{x}}\\)展开为一个12维的向量。 \\[ F=D_s D_m^{-1}\\\\ D_s = \\begin{bmatrix} \\vec x_1-\\vec x_4 &amp; \\vec x_2-\\vec x_4 &amp; \\vec x_3-\\vec x_4 \\end{bmatrix}\\\\ D_m = \\begin{bmatrix} \\vec x_{10}-\\vec x_{40} &amp; \\vec x_{20}-\\vec x_{40} &amp; \\vec x_{30}-\\vec x_{40} \\end{bmatrix}\\\\ \\vec {\\mathbf{x}} = \\begin{bmatrix} \\vec x_1 \\\\ \\vec x_2 \\\\ \\vec x_3 \\\\ \\vec x_4 \\end{bmatrix}\\in \\mathbb{R}^{12} \\] 所以 \\[ \\frac{\\partial F}{\\partial \\vec {\\mathbf{x}}} = \\frac{\\partial D_s}{\\partial \\vec {\\mathbf{x}}} D_m^{-1} \\] \\(\\frac{\\partial D_s}{\\partial \\vec {\\mathbf{x}}}\\)是一个\\(3\\times 3\\times 12\\)的张量： \\[ \\frac{\\partial D_s}{\\partial \\vec {\\mathbf{x}}} = \\begin{bmatrix} \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\\\ \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\\\ \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}\\\\ \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\\\ \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\\\ \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix}\\\\ \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\\\ \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix}\\\\ \\begin{bmatrix} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\\\ \\begin{bmatrix} -1 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; 0 \\end{bmatrix}\\\\ \\begin{bmatrix} 0 &amp; -1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}\\\\ \\begin{bmatrix} 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; -1 \\end{bmatrix}\\\\ \\end{bmatrix} \\] 因为\\(D_m, P\\)都是固定的，所以把他们用符号表示就可以验证了： 12345678910111213141516171819202122232425262728293031syms p11 p12 p13 p21 p22 p23 p31 p32 p33 realsyms d11 d12 d13 d21 d22 d23 d31 d32 d33 realDmInv = [d11, d12, d13; d21, d22, d23; d31, d32, d33];dDsdx = sym(zeros(3, 3, 12));% 填充前九个矩阵，每个矩阵在对应位置置为1for k = 1:9 i = mod(k-1, 3) + 1; % 计算行索引 j = floor((k-1)/3) + 1; % 计算列索引 dDsdx(i, j, k) = 1;end% 填充最后三个矩阵dDsdx(:,:,10) = [-1 -1 -1; 0 0 0; 0 0 0];dDsdx(:,:,11) = [0 0 0; -1 -1 -1; 0 0 0];dDsdx(:,:,12) = [0 0 0; 0 0 0; -1 -1 -1];P = [p11, p12, p13; p21, p22, p23; p31, p32, p33];dPdx = sym(zeros(12, 1));% 计算double contractionfor k = 1:12 % 算出vec(dFdx)前右乘DmInv dFdx = dDsdx(:,:,k) * DmInv; dFdx = dFdx(:); dPdx(k) = dFdx' * P(:);enddPdx = reshape(dPdx,3, 4);dPdx(:,1:3) - P * DmInv' 最后发现dPdx的前三列和P * DmInv'是一样的，也就验证了这个公式。","link":"/2024/05/02/FEM-gradient/"},{"title":"三篇图像动画论文的比较(FOMM,AA,SDEMT)","text":"FOMM和AA是很经典的论文，都来自作者Aliaksandr Siarohin，最近有一篇新论文在此基础上做了较大的改进，给出的效果看起来也很好，因此总结三个论文的内容。 FOMM AA Self-appearance-aided Differential Evolution for Motion Transfer ## FOMM motion representation 包含稀疏动作估计和稠密动作估计。 稀疏动作 使用\\(\\mathcal{T}_{S\\leftarrow R}(p_k),\\mathcal{T}_{D\\leftarrow R}(p_k)\\)来计算D中每个关键点处到S的仿射变换，具体方法是用泰勒展开，然后将涉及到\\(\\mathcal{T}_{S\\leftarrow R}(p_k),\\mathcal{T}_{D\\leftarrow R}(p_k)\\)之外的变化都用\\(\\mathcal{T}_{S\\leftarrow R}(p_k),\\mathcal{T}_{D\\leftarrow R}(p_k)\\)来代替。 \\[ \\mathcal{T_{S\\leftarrow D}}(z) \\approx \\mathcal{T_{S\\leftarrow R}}(p_k) + J_k(z-\\mathcal{T_{D\\leftarrow R}}(p_k)) \\] 稠密动作 用\\(\\mathcal{T}_{S\\leftarrow R}(p_k),\\mathcal{T}_{D\\leftarrow R}(p_k)\\)的高斯分布之差计算热力图来找出变换发生的位置。对S进行稀疏动作的逆变换，得到K个结果(每个关键点一个变换)。然后将热力图和S变换结果一起喂给U-net，预测K+1个mask作为每个关键点上稀疏动作的权值，多出的一个mask用于处理背景。稠密动作就是各个关键点的稀疏动作的加权和。 extraction 使用u-net分别提取参考帧到两个输入图像的仿射变换\\(\\mathcal{T}_{S\\leftarrow R}(p_k),\\mathcal{T}_{D\\leftarrow R}(p_k)\\)，每个关键点对应一个变换。还额外输出一个遮挡图的通道。 transfer 用稠密动作warp输入图像，然后用遮挡图计算阿达马乘积，最后通过一个解码器处理得到最终结果。 Articulated Animation motion representation 包含稀疏动作估计和稠密动作估计。论文和代码里热力图的概念被重复用在两个地方，一个是指输入图像提取出来的热力图，相当于特征点，另一个是稠密动作里用高斯分布得到的热力图，用于找出发生变换的位置。 稀疏动作 使用PCA处理热力图的协方差得到仿射变换的线性变换的部分，热力图的均值作为仿射变换的偏移量，总共5个自由度，加上背景的恒等变换就得到稀疏动作。 稠密动作 和FOMM的做法完全一样（包括使用的神经网络结构），稠密动作就是各个关键点的稀疏动作的加权和。 extraction 使用U-net结构提取输入图像的特征图，经过一层卷积和softmax，得到热力图。 transfer 和FOMM一样。 Self-appearance-aided Differential Evolution for Motion Transfer motion representation 计算关键点的差值作为神经网络的输入，回归出系数来加权关键点的差值作为稀疏动作，同时作为微分方程 \\[ \\frac{d \\mathscr{T}}{d t}=\\mathscr{F}_{E}\\left(\\mathscr{T}^{(t)}, t\\right), \\quad \\text { for } t \\in[0,1] \\] 的初值\\(\\mathscr{F}^{(0)}\\)：，两侧积分就得到稠密动作。 extraction 用编码器解码器网络获得S和D的关键点。 transfer 用特征提取网络提取输入图像的特征\\(\\mathbb{F}_{\\mathbb{S}}\\)。 将\\(\\mathbb{F}_{\\mathbb{S}}\\)用稠密动作进行扭曲得到\\(\\widetilde{\\mathbb{F}}_{\\mathbb{S D}}\\)。 扭曲后用\\(\\mathscr{F}_A\\)来预测self-appearance流变形场\\(\\mathscr{T}_{App}\\)，\\(\\mathscr{T}_{App}\\)是用来扭曲原区域到缺失区域的（流）特征的。 计算\\(\\mathbb{F}_{A p p}=\\mathscr{T}_{A p p} \\circ \\widetilde{\\mathbb{F}}_{\\mathbb{S D}}\\)。 用生成器\\(\\mathscr{F}_G\\)为N个不同视角的view输出置信度掩码\\(C^{(j)}\\)，每个视角的置信度掩码各自进行归一化：\\(\\widetilde{C}^{(j)}(\\mathbf{x})=C^{(j)}(\\mathbf{x}) / \\sum_{j=0}^{N} C^{(j)}(\\mathbf{x})\\)。 用置信度掩码对\\(\\mathbb{F}_{A p p} ,\\widetilde{\\mathbb{F}}_{\\mathbb{S D}}\\)分别计算加权和： \\[ \\overline{\\mathbb{F}}_{A p p}=\\sum_{j=1}^{N} \\widetilde{C}^{(j)} \\mathbb{F}_{A p p}^{(j)}, \\quad \\overline{\\widetilde{\\mathbb{F}}}_{\\mathbb{D}}=\\sum_{j=1}^{N} \\widetilde{C}^{(j)} \\widetilde{\\mathbb{F}}_\\mathbb{SD}^{(j)} \\] 只有一个视图时，用\\(\\mathbb{F}_{A p p} ,\\widetilde{\\mathbb{F}}_{\\mathbb{S D}}\\)连接起来喂给\\(\\mathscr{F}_G\\)的编码器部分用于生成图像。有多个时，用加权后的连接起来喂给\\(\\mathscr{F}_G\\)的编码器部分用于生成图像。 总结 差别 FOMM和Articulated Animation是同一个人的工作，只有稀疏动作表示是不一样的，提取特征里后者只是多了生成热力图的步骤，Articulated Animation简化了计算稀疏动作的过程，用PCA方法和热力图的均值直接获得仿射变换，非常简洁。 Self-appearance-aided Differential Evolution for Motion Transfer使用解微分方程的方法计算动作，原因是neural-ODEs已被证明能够捕捉复杂的变换，可微动作演化可以泛化稀疏动作的预测（我觉得意思是不会因为不同对象或者数据集效果差别很大），同时避免雅克比矩阵和SVD的大量计算。 优点 Articulated Animation认为PCA可以更好的描述动作，所以用PCA来获得仿射变换矩阵的线性部分的参数。 Self-appearance-aided Differential Evolution for Motion Transfer 里指出它的方法泛化了FOMM，AA，Monkey-Net方法。 Self-appearance-aided Differential Evolution for Motion Transfer 在CSIM上与FOMM,AA拉开很大的距离。即该方法相比FOMM,AA能更好的保留原图像的特征。 Self-appearance-aided Differential Evolution for Motion Transfer 泛化能力很好，在A训练集上训练后，在B训练集上依然效果很好。比如卡通动画生成中，该论文的方法能很好的保留个体特征，而FOMM和AA很差。 缺点 FOMM在提取稀疏动作的时候用了泰勒展开，非常繁琐。 Articulated Animation里指出FOMM这类用关键点的方法在处理物体边界内的动作会出现不真实的效果。 Articulated Animation认为该论文的方法的泛化能力较弱，生成非活物的动画有难度，也就是说生成素描动画效果不会很好。 Self-appearance-aided Differential Evolution for Motion Transfer的网络结构太大，训练时间久。","link":"/2021/12/14/FOMM-AA-SDEMT/"},{"title":"JOKR笔记","text":"JOKR: Joint Keypoint Representation for Unsupervised Cross-Domain Motion Retargeting 摘要 原视频和目标视频形状不同时，之前专注于特定对象先验的方法就会失败，作者提出联合关键点表达可以捕捉原视频和目标视频都有的动作，而且不需要物体先验或者数据采集。使用domain confusion项有利于对于两个domain的动作的一致的部分的解耦，可区分的外观和动作使得捕捉其中一个视频动作同时描绘另一个视频的风格的视频得以生成。 为处理物体有不同比例缩放或者不同方向的情形，作者应用了JOKR之间的仿射变换。这使得表达具有仿射不变性。 方法 目标视频使用的外观来自视频A，动作来自视频B，共\\(N_a\\)和\\(N_b\\)帧。分割图\\(s_{a,i},s_{b,i}\\)是给定的数据，或者是通过现成的图像分割网络得到。 形状不变表达 使用JOKR作为瓶颈。用无监督关键点提取器E，提取K个关键点\\(k_{a,i}\\)，提取器采用U-Net的做法，提取热力图\\(h_{a,i}\\)（a代表视频，i代表帧序号）来确定关键点位置。 为做到几何和外观的解耦，生成过程有两步： 给定\\(h_{a,i}\\)，生成器\\(G_{A}\\)被训练来输出一个同时对应提取到的关键点和物体形状的剪影。为减少参数数量，\\(G_{A},G_{B}\\)共用权重，除了最后一层。类似的，E也被用于两个视频。给定帧\\(a_i,b_j\\)，生成的剪影要最小化这个MSE loss。 \\[ \\mathcal{L}_{seg}=\\sum_{i=0}^{N_A-1}||G_A(E(a_i))-s_{a,i}||_2+\\sum_{j=0}^{N_B-1}||G_B(E(b_i))-s_{b,j}||_2 \\] 训练生成器\\(R_A,R_B\\)来转换得到的分割图到原图上，因此添加了纹理。重建和感知损失为： \\[ \\begin{gathered} \\mathcal{L}_{L 1}=\\sum_{i=0}^{N_{A}-1}\\left\\|R_{A}\\left(G_{A}\\left(E\\left(a_{i}\\right)\\right)\\right)-a_{i}\\right\\|_{1}+\\sum_{j=0}^{N_{B}-1}\\left\\|R_{A}\\left(G_{B}\\left(E\\left(b_{j}\\right)\\right)\\right)-b_{j}\\right\\|_{1} \\\\ \\mathcal{L}_{\\text {LPIPS }}=\\sum_{i=0}^{N_{A}-1}\\left\\|\\mathcal{F}\\left(R_{A}\\left(G_{A}\\left(E\\left(a_{i}\\right)\\right)\\right)\\right)-\\mathcal{F}\\left(a_{i}\\right)\\right\\|_{2}+\\sum_{j=0}^{N_{B}-1}\\left\\|\\mathcal{F}\\left(R_{A}\\left(G_{B}\\left(E\\left(b_{j}\\right)\\right)\\right)\\right)-\\mathcal{F}\\left(b_{j}\\right)\\right\\|_{2} \\end{gathered} \\] \\(\\mathcal{F}\\)是特征提取器。 共享表达 AB视频描绘的物体可能来自不同domain，所以提取的关键点对于各自的视频可能有不同的语义信息。因此作者强制编码的关键点来自共享的分布，从而鼓励关键点捕捉同时来自两个视频的动作。AB的特定风格在生成器的权重里编码。为施加共享的分布，作者使用domain confusion loss。 此外还用一个判别器来区分A和B的domain的关键点，编码器被训练来欺骗他。 \\[ \\mathcal{L}_{\\mathrm{DC}}=\\sum_{i=0}^{N_{A}-1} \\ell_{\\text {bce }}\\left(D\\left(k_{a, i}\\right), 1\\right)+\\sum_{j=0}^{N_{B}-1} \\ell_{\\text {bce }}\\left(D\\left(k_{b, j}\\right), 1\\right) \\] \\(\\ell_{bce}=-(q\\log(p)+(1-q)\\log(1-p))\\)是二元交叉熵损失函数。 关键点提取器尝试使得关键点分布无法区分，同时判别器做对抗它的训练： \\[ \\mathcal{L}_{\\mathrm{D}}=\\sum_{i=0}^{N_{A}-1} \\ell_{\\text {bce }}\\left(D\\left(k_{a, i}\\right), 0\\right)+\\sum_{j=0}^{N_{B}-1} \\ell_{\\text {bce }}\\left(D\\left(k_{b, j}\\right), 1\\right) \\] 时间连贯性 为保证生成的视频是时间连贯的，即生成的动作平滑无抖动。应用时间正则化在生成的关键点上，最小化相邻帧之间的关键点的距离。 \\[ \\mathcal{L}_{\\mathrm{tmp}}=\\sum_{i=0}^{N_{A}-1}\\left\\|k_{a, i}-k_{a, i+1}\\right\\|_{2}+\\sum_{j=0}^{N_{B}-1}\\left\\|k_{b, j}-k_{b, j+1}\\right\\|_{2} \\] 因为物体有大动作的时候关键点的含义可能会变化（比如由后腿变成尾巴），所以作者应用随机仿射变换，比较变换后的关键点和变换后的图像提取出的关键点，来保证生成的关键点对任意仿射变换是等变的（equivariant）。即保证了每个关键点语义的一致性，对于一个仿射变换T，等变loss的公式是： \\[ \\mathcal{L}_{\\mathrm{eq}}=\\sum_{i=0}^{N_{A}-1}\\left\\|T\\left(E\\left(a_{i}\\right)\\right)-E\\left(T\\left(a_{i}\\right)\\right)\\right\\|_{1}+\\sum_{j=0}^{N_{B}-1}\\left\\|T\\left(E\\left(b_{j}\\right)\\right)-E\\left(T\\left(b_{j}\\right)\\right)\\right\\|_{1} \\] 关键点正则化 关键点们可能会缩成一个点，因此有额外的两个损失项。 首先惩罚两个过近的关键点： \\[ \\mathcal{L}_{\\mathrm{sep}}=\\frac{1}{K^{2}} \\sum_{\\ell=0}^{K-1} \\sum_{\\ell \\neq r}\\left(\\sum_{i=0}^{N_{A}-1} \\max \\left(0, \\delta-\\left\\|k_{a, i}^{\\ell}-k_{a, i}^{r}\\right\\|^{2}\\right)+\\sum_{j=0}^{N_{B}-1} \\max \\left(0, \\delta-\\left\\|k_{b, j}^{\\ell}-k_{b, j}^{r}\\right\\|^{2}\\right)\\right) \\] 然后用剪影的损失鼓励关键点待在物体上： \\[ \\mathcal{L}_{\\mathrm{sill}}=\\frac{1}{K} \\sum_{\\ell=0}^{K-1}\\left(\\sum_{i=0}^{N_{A}-1}-\\log \\sum_{u, v} s_{a, i}(u, v) H_{a, i}^{\\ell}(u, v)+\\sum_{j=0}^{N_{B}-1}-\\log \\sum_{u, v} s_{b, j}(u, v) H_{b, j}^{\\ell}(u, v)\\right) \\] 两步优化 大部分损失和形状有关，和纹理无关，所以可以用两步优化目标函数。首先用\\(\\mathcal{L}_D\\)训练判别器，同时训练E，\\(G_A,G_B\\)。第二步训练\\(R_A,R_B\\)。 增强 数据有限时可能导致mode collapse。所以作者用随机仿射变换进行增强。因为保留背景是必要的，而这些增强可能会给生成的帧带来伪影（artifact）,所以这些增强直接作用在关键点上，在其传给判别器之前。 预测 网络结构 得到E,T,G,R之后，用下面的公式得到结果： \\[ a b_{j}=R_{A}\\left(G_{A}\\left(T\\left(E\\left(b_{j}\\right)\\right)\\right)\\right) \\] 即有a的外观b的动作的第j帧。","link":"/2021/12/14/JOKR-report/"},{"title":"VkPresentModeKHR-呈现模式","text":"在构建交换链 vk::SwapchainCreateInfoKHR时，有一个设置是 VkPresentModeKHR。根据官方手册VkPresentModeKHR(3) Manual Page，呈现模式有6种。 相关术语 垂直空白间隙(Vertical Blank Interval) 指在扫描线技术中（例如隔行扫描）电子束从前一画面的右下角移至后一画面的左上角时的时间差所引起的空白画面。这一情况出现在使用阴极射线管的显示器、使用视频图形阵列标准的显示器中。下面简称VBI。 在现代的显示器（比如LCD）中，虽然没有电子枪进行隔行扫描，但是同时控制上百万个像素点是不可能的，还是需要动态扫描进行刷新。这个刷新过程可以理解为从上往下的逐行扫描。因此VBI的含义是一段可以安全的换出\"显示到显示器上的图像\"的时间，可以简单理解为在从上到下扫描完帧缓冲，即可以认为屏幕上显示了完整画面（实际上某些情况下有撕裂）后的一段空白时间。这个扫描过程只受帧同步信号（VSYNC）影响。 撕裂(tearing) 在显示引擎正在显示一张图像时，如果它不再显示当前这张图像，而是显示另一张图像（某些模式下会有这样做），那么屏幕上有一部分是旧的图像（比如上面一半），另一部分是新的图像（比如下面一半）。这个现象就是撕裂。 显示模式 总结 VK_PRESENT_MODE_和 _KHR在表中省略。 ✅是 ☑️大部分时候是 ❌否 ❓不确定 模式 是否有队列 是否等待VBI再更新当前图像 是否支持并发访问单个图像 可能观察到撕裂(tearing) IMMEDIATE ❌ ❌ ❌ ✅ MAILBOX ✅单项目队列 ✅ ❌ ❌ FIFO ✅ ✅ ❌ ❌ FIFO_RELAXED ✅ ☑️ ❌ ✅ SHARED_DEMAND_REFRESH ❓ ❓ ✅ ✅ SHARED_CONTINUOUS_REFRESH ❓ ❓ ✅ ✅ 三个有队列的显示模式MAILBOX，FIFO，FIFO_RELAXED的差别 FIFO_RELAXED和FIFO的差别主要体现在应用过慢的情况下。当应用绘制的很慢的时候，有可能在显示引擎显示了第一个图像之后的VBI的过程中，应用并没有生成好下一个图像。这时如果没有到VBI，即显示引擎处于动态扫描过程中，对于FIFO_RELAXED，在下一个图像生成好之后，就不需要等待VBI，直接让显示引擎显示出来。如果是FIFO，就还需要等到扫描结束后的VBI才绘制。因此FIFO_RELAXED会稍微缓解延迟问题。 MAILBOX虽然也有队列，但是他只能保存一个项目。这在应用过快的情况下非常有用。比如16.6毫秒内如果应用生成了多个图像，那么在下个VBI时只有MAILBOX会显示最新的一个，因为最新的把之前生成好的都覆盖了。但是基于FIFO的就只能等之前的都显示完。 详细描述 VK_PRESENT_MODE_IMMEDIATE_KHR 显示引擎不等待VBI直接更新当前图像。 VK_PRESENT_MODE_MAILBOX_KHR 显示引擎等待下个VBI再更新当前图像。有一个内部的可以容纳一个项目的队列保存等待显示的请求项目。如果队列满时收到新的显示请求，新的会替代现有的项目，与之前的项目关联的任何图像都可以被应用重复使用。在VBI期间队列不为空时，队列中的一个请求会被移除然后在这个期间被处理。 VK_PRESENT_MODE_FIFO_KHR 显示引擎等待下个VBI再更新当前图像。有一个内部的单向队列保存等待显示的请求项目。新请求被放到队尾。在VBI期间队列不为空时，队列中队头的请求会被移除然后在这个期间被处理。这是唯一必须支持的 presentMode的值。 VK_PRESENT_MODE_FIFO_RELAXED_KHR 显示引擎一般会等待下个VBI再更新当前图像。如果从上次更新到当前时刻已经经过了一个或更多VBI，显示引擎就不再等待下一个VBI而直接更新图像，这个模式可以减轻一些应用的视觉延迟。这些应用大部分时间可以在下个VBI之前生成新图像，但是偶尔会迟到，正好在下个VBI之后生成好新图像。有一个内部的单向队列保存等待显示的请求项目。新请求被放到队尾。在VBI期间队列不为空时，队列中队头的请求会被移除然后在这个期间或者之后被处理。 VK_PRESENT_MODE_SHARED_DEMAND_REFRESH_KHR 显示引擎和应用可以并发访问单个图像，这被称为共享的可展示图像 shared presentable image。显示引擎仅需要在收到新的显示请求后更新当前图像。因此无论是否需要更新，应用必须发起显示请求。但是，显示引擎可能在任何时间更新当前图像，意味着可能有撕裂。 VK_PRESENT_MODE_SHARED_CONTINUOUS_REFRESH_KHR 显示引擎和应用可以并发访问单个图像，这被称为共享的可展示图像 shared presentable image。显示引擎在他的有规律的刷新循环下周期性更新当前图像。应用只需要发起初次显示请求，在这个请求之后显示引擎必须更新当前图像，不需要更多的显示请求。应用可以通过发起显示请求表明图像内容已更新，但是这并不能保证更新的实机。如果该图像的渲染实机不正确有可能产生视觉撕裂。 参考 【硬件科普】一个视频带你了解LCD OLED QLED mini-LED等显示技术的区别 从点阵到OLED屏幕——动态扫描显示原理 Lcd（一）显示原理 LCD显示屏-基础篇（屏的角度） Vertical blanking interval VkPresentModeKHR(3) Manual Page Swap Chain | \"Presentation Modes and Swap Chain Setup in Vulkan\" | Vulkan Lecture Series, Episode 2 emoji","link":"/2023/03/07/VkPresentModeKHR/"},{"title":"cmake笔记","text":"记录一个CMake速查表。 运行 cmake： 12cmake -B build -DCMAKE_BUILD_TYPE=Releasecmake --build build -j8 常用函数 函数名 例子 用途 set set(CMAKE_CXX_STANDARD 11) 为变量赋值 option option(USE_VULKAN_UTILS \"use custom vulkan utilities\" ON) 选项（和set功能相同） include include(Ctest) 包含cmake文件 cmake_minimum_required cmake_minimum_required(VERSION 3.10) cmake版本指定 project project(VulkanToy VERSION 1.0) 项目名称，可以包括版本 add_executable add_executable(VulkanToy main.cpp) 添加一个要生成可执行文件的目标 add_library add_library(VulkanUtils utils.cpp) 添加一个要生成库的目标 add_subdirectory add_subdirectory(VulkanUtils) 添加子目录 find_package find_package(Vulkan REQUIRED) 查找包 aux_source_directory aux_source_directory(. sources) 自动搜集有需要的后缀名的文件放入 sources变量 target_include_directories target_include_directories(test PRIVATE VulkanUtils) 包含子库 target_link_libraries target_link_libraries(VulkanToy utilTest) 链接子库目录 target_add_definitions target_add_definitions(VulkanToy PUBLIC MY_MACRO=1) 添加一个宏定义 target_compile_options target_compile_options(VulkanToy PUBLIC -fopenmp) 添加编译器命令行选项 target_sources target_sources(VulkanToy PUBLIC test.cpp other.cpp) 添加要编译的源文件 include_directories include_directories(/opt/cuda/include) 添加头文件搜索目录 link_directories link_directories(/opt/cuda) 添加库文件的搜索路径 add_definitions add_definitions(MY_MACRO=1) 添加一个宏定义 add_compile_options add_compile_options(-fopenmp) 添加编译器命令行选项 configure_file configure_file(VulkanToyConfig.h.in VulkanToyConfig.h) 配置文件 install install(TARGETS VulkanToy DESTINATION bin) 安装 enable_testing enable_testing() 启动测试功能 add_test add_test(NAME VulkanToyRuns COMMAND VulkanToy) 添加一个测试 set_tests_properties set_tests_properties(VulkanToyRuns PROPERTIES PASS_REGULAR_EXPRESSION \".*\") 设置测试的属性（比如结果是否满足条件 add_custom_command add_custom_command(TARGET${target_name} POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy ${PROJECT_SOURCE_DIR}/vert.spv $TARGET_FILE_DIR:${target_name}(TARGET_FILE_DIR:${target_name})) 自定义命令 部分关键字 关键字 例子 用途 GLOB file(GLOB sources CONFIGURE_DEPENDS .cpp .h) 将文件夹下的对应扩展名的所有文件都放入 sources中 GLOB_RECURSE file(GLOB_RECURSE sources CONFIGURE_DEPENDS .cpp .h) 自动包含所有子文件夹下的文件 CONFIGURE_DEPENDS file(GLOB sources CONFIGURE_DEPENDS .cpp .h) 当添加新文件时，自动更新变量 部分说明 指定C++标准 12set(CMAKE_CXX_STANDARD 11)set(CMAKE_CXX_STANDARD_REQUIRED True) 设置构建的类型 有四种构建的类型： Debug 调试模式，完全不优化，生成调试信息，方便调试程序 Release 发布模式，优化程度最高，性能最佳，但是编译比 Debug 慢 MinSizeRel 最小体积发布，生成的文件比 Release 更小，不完全优化，减少二进制体积 RelWithDebInfo 带调试信息发布，生成的文件比 Release 更大，因为带有调试的符号信息 可以检查是否设置了构建类型，没有就可以设置成默认的： 1234if(NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE Release)endif() 在代码中可以使用NDEBUG也就是\"NO DEBUG\"来判断是否是非调试的构建类型，比如用 123#ifndef NDEBUG std::cerr &lt;&lt; phyDevice.getProperties().deviceName &lt;&lt; std::endl;#endif 就可以仅在构建类型为Debug的时候输出调试信息。 ### 添加子目录 子目录中包含自己的 CMakeLists.txt。添加子目录后子目录下的 CMakeLists.txt会被执行，整个项目编译时，这个子目录中的目标也会被编译（比如生成 .lib）。但是不加其他语句（target_link_libraries）的话这个目标与当前的目标没有任何联系。反过来，如果只是链接第三方的库不需要用 add_subdirectory，需要 target_link_libraries。 1add_subdirectory(VulkanUtils) 包含子库 将某个（子）目录加入当前项目，使得在编译时，指定的目录下的头文件能够被访问到。 第一个参数是目标，第二个是作用域关键字，第三个是目录的标识符。如果要包含项目目录中的子目录，第三个参数可以是 \"${PROJECT_SOURCE_DIR}/目录名称\"。但是包含项目目录中的子目录可以有更好的做法：使用 add_library。 第三个参数可以使用相对路径，相对的当前路径时 CMakeLists.txt所在的路径。比如 CMakeLists.txt所在的文件夹中有一个子文件夹 VulkanUtils，如果 test目标想包含 VulkanUtils里的头文件，就可以直接写： 1target_include_directories(test PRIVATE VulkanUtils) 在vs中，这个语句会影响对应目标的属性页的附加包含目录。 链接子库目录 1target_link_libraries(VulkanToy PUBLIC &quot;${PROJECT_BINARY_DIR}&quot;) 在vs中，这个语句会影响对应目标的属性页的附加依赖项。 作用域关键字 当存在多个共享库而且存在依赖关系时，作用域关键字可以用于指定依赖关系。 链接的依赖关系 这里以两个库 foo，bar，目标 app为例，两个库有对应名字的头文件和源文件。假设 app的 CMakeLists.txt中有以下语句： 12target_link_libraries(app bar)target_link_libraries(bar PUBLIC foo) 这里的 PUBLIC可以换成另外两个关键字，含义如下： PRIVATE: bar.h没有包含foo.h，只有bar.cpp包含了foo.h。此时，app没有包含foo.h，因此不能使用foo中定义的符号。换句话说，app只知道bar的存在，完全不知道foo的存在。 INTERFACE: bar.h中包含了foo.h，但是bar.cpp并没有用到foo定义的符号。此时，app包含了foo.h，可以引用foo的符号。换句话说，bar只是作为一个接口、界面，把foo传递给了app。 PUBLIC: 等于PRIVATE加INTERFACE，bar.h包含了foo.h，且bar.cpp引用了foo的符号。此时，app包含了foo.h，可以引用foo的符号。 对于 target_link_libraries(app bar)，使用不同关键字并不会影响编译。 对于编译来说，关键字会影响编译时是否加入某些库(比如 .lib)。 包含的依赖关系 一般 target_include_directories和 target_link_libraries会使用相同的关键字，含义也是一样的。但是 target_link_libraries可以省略关键字，target_include_directories不可以。 总结来说，如果有 foo-&gt;bar-&gt;app这样的依赖关系，如果 bar.cpp使用了 foo.hpp里的符号，且 app.cpp里使用了 foo.hpp里的符号，那么就只能用 PUBLIC，如果只有前者，可以用 PRIVATE，如果只有后者，可以用 INTERFACE``。记住，INTERFACE意味着库的使用者需要，但库的设计者不需要。 测试中发现，如果只有后者，bar.hpp中并不能 #include \"foo.hpp\"（应该是因为如果可以，那么 foo.cpp也可以使用 foo.hpp的符号，就不满足 INTERFACE的要求了），只有 app.cpp可以。 配置文件 编写一个扩展名为 .h.in的文件（本文例子中是 VulkanToyConfig.h.in），里面写这样的内容，两个宏的前缀是项目名： 12#define VulkanToy_VERSION_MAJOR @VulkanToy_VERSION_MAJOR@#define VulkanToy_VERSION_MINOR @VulkanToy_VERSION_MINOR@ CMakeLists.txt中加入下面这行： 1configure_file(VulkanToyConfig.h.in VulkanToyConfig.h) 这行代码会将第一个参数复制到第二个参数这个文件中，这个文件 VulkanToyConfig.h不存在时，cmake会在运行的所在目录生成一个，比如我是在 Build目录下。他可以被包含到当前路径下的源文件里，这样就可以使用定义的宏等。 选项 一个例子是可以决定是否包含某些子目录，为真时加入该子目录（作为一个库，该目录下要有自己的 CMakeLists.txt，会被独立构建）。这个选项会在生成项目时出现，如图： 1678040567107 12345option(USE_VULKAN_UTILS &quot;use custom vulkan utilities&quot; ON)if(USE_VULKAN_UTILS) add_subdirectory(VulkanUtils) list(APPEND EXTRA_LIBS VulkanUtils)endif() list可以修改变量中的内容，在这里用来保存需要额外链接的库 在配置文件 .h.in（本文例子中是 VulkanToyConfig.h.in）加入下面的内容。 1#cmakedefine USE_VULKAN_UTILS 安装 在文件尾添加： 12install(TARGETS VulkanToy DESTINATION bin)install(FILES &quot;${PROJECT_BINARY_DIR}/VulkanToyConfig.h&quot; DESTINATION include) TARGETS指定安装的目标以及安装的形式，FILES会把项目中指定的文件拷贝到目标位置。如果有子目标的话，子目标里的 CMakeLists.txt里也需要添加类似的内容，但是 TARGETS的第三个参数要根据子目标的需求来写（换成 lib）。 加入安装语句后，使用install时默认在本机安装软件的目录下，比如 C:\\Program Files (x86)。因此有时需要管理员权限。可以用 --prefix指定安装位置。 1cmake --install . --config Debug 也可以用下面的命令来安装，可以指定debug或者release。但是不能用 --prefix。 1cmake --build . --target install --config Debug 测试 123enable_testing()add_test(NAME VulkanToyRuns COMMAND VulkanToy)set_tests_properties(VulkanToyRuns PROPERTIES PASS_REGULAR_EXPRESSION &quot;.*&quot;) 在 Build目录运行命令 ctest，如果出现 Test not available without configuration. (Missing \"-C &lt;config&gt;\"?)这样的错误，就可以用 -C指定配置： 12cmake ..ctest -C DEBUG 自定义命令 add_custom_command有两种，一种用于生成输出文件，一种在生成目标文件时自动执行。 参考 【公开课】现代CMake高级教程（持续更新中） 非常推荐观看。 cmake：添加自定义操作 CMake应用：生成器表达式","link":"/2023/03/04/cmake-related/"},{"title":"导数","text":"标量求导和向量求导是矩阵求导的特殊情况。向量求导使用的频率较高。对向量求导时运用乘法和除法公式时会出现标量求导，这里记录一下各种求导的结果。 以向量为\\(\\vec x\\)（m维向量），\\(\\vec t\\)（n维向量），标量为\\(t\\)为例。 对标量求导 向量对标量求导 结果是向量的每一个元素对标量求导组成的列向量： \\[ \\frac{\\partial \\vec x}{\\partial t}= \\begin{bmatrix} \\frac{d x_1}{d t},\\\\...\\\\\\frac{d x_m}{d t} \\end{bmatrix}. \\] ## 对向量求导 ### 标量对向量求导 可以看做是多元函数对多变量的求导，因此结果是梯度，是标量对向量的每一个元素求导组成的行向量： \\[ \\frac{\\partial t}{\\partial \\vec x}=\\begin{bmatrix} \\frac{d t}{d x_1}&amp;...&amp;\\frac{d t}{d x_m} \\end{bmatrix}. \\] 向量对向量求导 结果是雅克比（Jacobi）矩阵。 \\[ \\frac{\\partial \\vec x}{\\partial \\vec t}= \\begin{bmatrix} \\frac{d x_1}{d t_1},&amp;\\frac{d x_2}{d t_1}&amp;...&amp;\\frac{d x_{n-1}}{d t_1}&amp;\\frac{d x_m}{d t_1}\\\\ \\frac{d x_1}{d t_2},&amp;\\frac{d x_2}{d t_2}&amp;...&amp;\\frac{d x_{n-1}}{d t_2}&amp;\\frac{d x_m}{d t_2}\\\\ &amp;&amp;...\\\\ &amp;&amp;...\\\\ \\frac{d x_1}{d t_{n}},&amp;\\frac{d x_2}{d t_{n}}&amp;...&amp;\\frac{d x_{n-1}}{d t_{n}}&amp;\\frac{d x_m}{d t_{n}}\\\\ \\end{bmatrix}. \\] 大部分常见公式可以查matrixcookbook。 还有一种使用爱因斯坦求和约定来计算的方法里奇微积分。使用这个网站可以计算：matrixcalculus。 本文参考链接：蓦风星吟​的回答:向量函数的求导问题？","link":"/2023/02/04/derivative/"},{"title":"浮点数","text":"浮点数的表示方法为 \\[ v=(-l)^s M 2^E \\] 对于规范化编码(Normalized Encoding)，M是frac的部分加1得到的，所以能表示的最小的正数是\\(2^{E_{min}}\\)。因此需要引入非规范化编码(Denormalized Encoding)，它的M不需要加1，E=1-Bias。最小的数是0。","link":"/2023/01/05/float-number/"},{"title":"Hello World","text":"是时候建个blog了，参考的2021最全hexo搭建博客+matery美化+使用，写的比较详细。踩的坑都在main和master分支上。以后大概主要写学习相关的文章，记录一下免得忘记。 顺便记录一下config的设置 https://hexo.io/zh-cn/docs/configuration.html 显示latex公式需要修改一些包，参考：成功解决在hexo中无法显示数学公式的问题","link":"/2021/12/12/hello-world/"},{"title":"Projective Dynamics 的local step实现推导","text":"介绍 Projective Dynamics是一种用于软体模拟的方法，算法分为local step和global step两个部分。其中local step可以对于每个四面体约束并行计算，global step只需要求解一个线性方程组，而他的矩阵非常特殊，是一个Gram矩阵，因此可以预先用Cholesky分解。本文主要介绍local step的实现的推导过程。用Corotated strain model的CUDA的实现作为例子。 背景 这部分内容参考SIGGRAPH 2012 Course FEM Simulation of 3D Deformable Solids。 能量密度函数(Energy Density Function) \\(\\Psi[\\Phi;\\vec X]\\)是一个能量密度函数，用来衡量在\\(\\vec X\\)附近的一个无穷小体积\\(dV\\)的应变能。 Corotated linear elasticity Corotated linear elasticity is a constitutive model that attempts to combine the simplicity of the stress-deformation relationship in a linear material with just enough nonlinear characteristics to secure rotational invariance. 对于corotated linear elasticity，\\(\\Psi\\)可以写成： \\[ \\Psi[F]=\\mu||F-R||^2_F+\\frac{\\lambda}{2}(\\text{tr}^2(R^TF)-I) \\] 对于四面体的四个顶点\\(\\vec q_1,\\vec q_2,\\vec q_3,\\vec q_4\\)，在任意时刻， \\[ F=\\begin{bmatrix}\\vec q_1-\\vec q_4 &amp; \\vec q_2-\\vec q_4 &amp; \\vec q_3-\\vec q_4\\end{bmatrix}D_m^{-1} \\] 其中 \\[ D_m = \\begin{bmatrix}\\vec q_{10}-\\vec q_{40} &amp; \\vec q_{20}-\\vec q_{40} &amp; \\vec q_{30}-\\vec q_{40}\\end{bmatrix} \\] 是rest shape时计算得到的。一般这个会预先算好。 Projective Dynamics Local step \\[ \\min_{p_i} \\frac{w_i}{2}||A_i S_i q -B_ip_i||^2_F+\\delta_{C_i}(p_i) \\] Global step \\[ (\\frac{M}{h^2}+\\sum_{i=1}^n w_i S_i^T A_i^T A_i S_i)q=\\frac{M}{h^2}s_n+\\sum_{i=1}^n w_i S_i^T A_i^T B_ip_i \\] 推导 根据论文，local step只需要选择想要最小化的约束，将投影结果算出后累加到每个顶点上即可。这里选择最小化能量密度函数，即： \\[ \\min_{p_i} \\frac{w_i}{2}||F_i^T-R_i^T||^2_F+\\delta_{C_i}(p_i) \\] 从上面的公式可以看出，\\(B_i\\)是一个单位矩阵。 转置是为了方便凑出\\(S_i^TA_i^Tp_i\\)，这样就能将\\(A_iS_i q\\)对应到\\(F^T\\)，\\(p_i\\)对应到\\(R^T\\)。 接下来只需要凑出\\(S_i^TA_i^Tp_i\\)即可。在我们的实现里，\\(q\\)是\\(\\mathbb{R}^{n\\times 3}\\)的矩阵（但是存储方式是flatten成\\(\\mathbb{R}^{3n}\\)的）。现在只考虑局部的一个四面体的四个顶点，因此我们把\\(q\\)当做一个\\(\\mathbb{R}^{4\\times 3}\\)的矩阵： \\[ q=\\begin{bmatrix}\\vec q_1^T\\\\\\vec q_2^T\\\\\\vec q_3^T\\\\\\vec q_4^T\\end{bmatrix} \\] 首先凑出\\(F\\)，根据背景部分的公式： \\[ F= (\\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 \\end{bmatrix} q)^T D_m^{-1} \\] \\[ F^T= D_m^{-T}\\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 \\end{bmatrix} q \\] 这样就得到： \\[ A_i= D_m^{-T},\\\\ S_i= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 \\end{bmatrix} \\] 然后计算\\(R^T\\)，先对\\(F\\)做SVD分解，得到\\(F=U\\Sigma V^T\\)，然后\\(R=UV^T\\)。于是 \\[ S_i^TA_i^Tp_i =(D_m^{-T}\\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 \\end{bmatrix} )^T(UV^T)^T \\] 因为这一项是一个\\(\\mathbb{R}^{4\\times 3}\\)的矩阵，所以我们需要将其flatten成\\(\\mathbb{R}^{12}\\)的向量，然后累加到每个顶点上。实际操作中就计算\\(S_i^TA_i^Tp_i\\)的转置\\(\\mathbb{R}^{3\\times4}\\)，然后将每个列向量累加到对应的顶点上即可，也就是说最后得到的是一个\\(\\mathbb{R}^{3\\times4}\\)的矩阵： \\[ UV^TD_m^{-T}\\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 \\end{bmatrix} \\] 实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051__global__ void solveLocal(const float* V0, const float wi, float* xProj, const glm::mat3* DmInvs, const float* qn, const indexType* tets, int numTets){ int index = (blockIdx.x * blockDim.x) + threadIdx.x; if (index &lt; numTets) { const int v0Ind = tets[index * 4 + 0] * 3; const int v1Ind = tets[index * 4 + 1] * 3; const int v2Ind = tets[index * 4 + 2] * 3; const int v3Ind = tets[index * 4 + 3] * 3; const glm::vec3 v0 = glm::vec3(qn[v0Ind + 0], qn[v0Ind + 1], qn[v0Ind + 2]); const glm::vec3 v1 = glm::vec3(qn[v1Ind + 0], qn[v1Ind + 1], qn[v1Ind + 2]); const glm::vec3 v2 = glm::vec3(qn[v2Ind + 0], qn[v2Ind + 1], qn[v2Ind + 2]); const glm::vec3 v3 = glm::vec3(qn[v3Ind + 0], qn[v3Ind + 1], qn[v3Ind + 2]); const glm::mat3 DmInv = DmInvs[index]; glm::mat3 R = glm::mat3(v0 - v3, v1 - v3, v2 - v3) * DmInv glm::mat3 U; glm::mat3 S; svd(R[0][0], R[1][0], R[2][0], R[0][1], R[1][1], R[2][1], R[0][2], R[1][2], R[2][2], U[0][0], U[1][0], U[2][0], U[0][1], U[1][1], U[2][1], U[0][2], U[1][2], U[2][2], S[0][0], S[1][0], S[2][0], S[0][1], S[1][1], S[2][1], S[0][2], S[1][2], S[2][2], R[0][0], R[1][0], R[2][0], R[0][1], R[1][1], R[2][1], R[0][2], R[1][2], R[2][2]); R = U * glm::transpose(R); if (glm::determinant(R) &lt; 0) { R[2] = -R[2]; } const glm::mat4x3 piTAiSi = glm::abs(V0[index]) * wi * R * glm::transpose(DmInv) * glm::mat4x3{ 1, 0, 0, 0, 1, 0, 0, 0, 1, -1, -1, -1 }; atomicAdd(&amp;(xProj[v0Ind + 0]), piTAiSi[0][0]); atomicAdd(&amp;(xProj[v0Ind + 1]), piTAiSi[0][1]); atomicAdd(&amp;(xProj[v0Ind + 2]), piTAiSi[0][2]); atomicAdd(&amp;(xProj[v1Ind + 0]), piTAiSi[1][0]); atomicAdd(&amp;(xProj[v1Ind + 1]), piTAiSi[1][1]); atomicAdd(&amp;(xProj[v1Ind + 2]), piTAiSi[1][2]); atomicAdd(&amp;(xProj[v2Ind + 0]), piTAiSi[2][0]); atomicAdd(&amp;(xProj[v2Ind + 1]), piTAiSi[2][1]); atomicAdd(&amp;(xProj[v2Ind + 2]), piTAiSi[2][2]); atomicAdd(&amp;(xProj[v3Ind + 0]), piTAiSi[3][0]); atomicAdd(&amp;(xProj[v3Ind + 1]), piTAiSi[3][1]); atomicAdd(&amp;(xProj[v3Ind + 2]), piTAiSi[3][2]); }}","link":"/2023/12/20/local-step-inference/"},{"title":"对LLVM的Pass进行调试的vscode配置","text":"文件夹结构 安装LLVM的过程省去，假设当前目录下有: CMakeLists.txt文件；编译pass include文件夹：pass的头文件 src文件夹；pass的源码 test文件夹：用于静态分析pass的测试代码（.c文件） 以下是CMakeLists.txt的内容，需要设置模式为Debug： 1234567891011121314151617181920212223cmake_minimum_required(VERSION 3.10)find_package(LLVM 8 REQUIRED CONFIG)list(APPEND CMAKE_MODULE_PATH &quot;${LLVM_CMAKE_DIR}&quot;)include(HandleLLVMOptions)include(AddLLVM)message(STATUS &quot;Found LLVM ${LLVM_PACKAGE_VERSION}&quot;)add_definitions(${LLVM_DEFINITIONS})include_directories(${LLVM_INCLUDE_DIRS} include)link_directories(${LLVM_LIBRARY_DIRS} ${CMAKE_CURRENT_BINARY_DIR})set(CMAKE_BUILD_TYPE Debug)add_llvm_library(DivZeroPass MODULE src/PointerAnalysis.cpp src/DivZeroAnalysis.cpp src/Transfer.cpp src/ChaoticIteration.cpp src/Domain.cpp src/Utils.cpp) 安装必要软件 安装lldb 12sudo apt updatesudo apt install lldb 配置vscode 填写json文件 在.vscode文件夹下新建launch.json文件，填写如下内容： 12345678910111213141516171819202122{ &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;Debug LLVM Pass&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;/usr/bin/opt&quot;, &quot;args&quot;: [ &quot;-load&quot;, &quot;${workspaceFolder}/build/DivZeroPass.so&quot;, &quot;-DivZero&quot;, &quot;${fileDirname}/${fileBasenameNoExtension}.ll&quot;, &quot;-disable-output&quot; ], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${workspaceFolder}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: false, &quot;MIMode&quot;: &quot;lldb&quot;, &quot;preLaunchTask&quot;: &quot;compile-llvm&quot; } ]} 在.vscode文件夹下新建tasks.json文件，填写如下内容： 1234567891011121314151617{ &quot;version&quot;: &quot;2.0.0&quot;, &quot;tasks&quot;: [ { &quot;label&quot;: &quot;compile-llvm&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;clang&quot;, &quot;args&quot;: [ &quot;-g&quot;, &quot;-emit-llvm&quot;, &quot;-S&quot;, &quot;-fno-discard-value-names&quot;, &quot;-Xclang&quot;, &quot;-disable-O0-optnone&quot;, &quot;-c&quot;, &quot;-o&quot;, &quot;${fileDirname}/${fileBasenameNoExtension}.ll&quot;, &quot;${file}&quot; ], &quot;group&quot;: { &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true } } ]} 调试 因为有设置prelaunch task，在test目录下打开一个.c文件，按下F5即可开始调试当前活动的.c文件，调试前会自动编译。断点要设置在pass的源码中。","link":"/2023/11/20/llvm-vscode/"},{"title":"在latex项目里用plotneuralnet","text":"plotneuralnet是一个绘制神经网络的工具，使用该工具可以将python脚本转换为latex脚本，编译得到pdf。需要以下几个步骤： 下载plotneuralnet的github源码，https://github.com/HarisIqbal88/PlotNeuralNet 找一个latex环境，可以是win/linux/overleaf，前两种需要安装latex的环境，比如texlive，可以参考https://github.com/luanshiyinyang/PlotNeuralNet，texlive在这里可以找到https://mirrors.tuna.tsinghua.edu.cn/CTAN/systems/texlive/Images/。 写好脚本my_arch.py（名字可以随便起）放在源码的pyexamples文件夹下，编写的教程可以在其他地方搜到。 直接用python运行python my_arch.py，可以看到目录下的my_arch.tex 这样就得到了一个能够直接编译的tex文件。但是实际上很多情况下我们想把这个放进自己的项目里。我以一个比较大的tex项目为例，文件夹结构大致如下： 查看文件夹结构 . ├── Makefile ├── README.md ├── bibtex-style │ ├── gbt7714-2005.bst │ ├── thesis.bst │ └── thesis2.bst ├── code │ └── demo.cpp ├── ctex-fontset-adobe2.def ├── docs │ ├── abstract.tex │ ├── ack.tex │ ├── appendix1.tex │ ├── chap01.tex │ ├── chap02.tex │ ├── chap03.tex │ ├── chap04.tex │ ├── chap05.tex │ ├── chap06.tex │ ├── disclaim.tex │ ├── grading.tex │ ├── info.tex │ ├── progress.tex │ └── proposal.tex ├── fonts │ └── init_fonts.sh ├── gulpfile.js ├── image │ ├── appendix1 │ ├── chap03 │ │ ├── overleaf-config.jpg │ │ ├── overleaf-create-proj.jpg │ │ ├── overleaf-example.jpg │ │ ├── overleaf-upload-proj.jpg │ │ └── vscode-example.png │ ├── chap04 │ │ ├── example │ │ │ ├── 2007_000799.jpg │ │ └── result │ │ ├── compare │ │ │ └── zoom_dog.png │ │ └── error │ │ └── p_2008_001580.png │ └── template │ ├── readme.md │ └── logo.pdf ├── main.bib ├── main.pdf ├── main.tex ├── package.json ├── packages │ ├── algorithm2e.sty │ ├── algorithm2e.tex │ └── ctex-xecjk-adobefonts.def │ └── thubeamer.sty ├── code.sty └── thesis.cls 实际上只需要关心docs文件夹和main.tex，因为神经网络结构一般放在正文里。 以放入docs的chap03.tex为例，需要以下步骤： 将源码的layers文件夹放进自己的项目文件夹。 把my_arch.tex中这几行的内容（里面的配置可以调整）放入main.tex（最外层的文档）的序言（preamble）区，也就是放。 123\\documentclass[border=8pt, multi, tikz]{standalone} \\usepackage{import}\\subimport{../layers/}{init} 调整subimport里面的layers的路径，按照第一步来做，则应该改成\\subimport{layers/}{init}。 接下来将神经网络结构放入正文，把my_arch.tex里除上面这几行之外的内容完整地拷贝到chap03.tex里自己想放的位置。要去掉\\begin{document}和\\end{document}。 如果代码里有includegraphics，要把对应的图片放在对应的文件夹下。 编译就可以看到结果。 这样做会看到神经网络图片非常大，超出了页面。因为这个模块是tikzpicture的，所以可以用TikZ的方法来调整。 以整体缩放为例，参考在 LaTeX 中同步缩放 TikZ 与其中的 node，可以在\\begin{tikzpicture}上面加入下面这段代码： 12345\\tikzset{global scale/.style={ scale=#1, every node/.append style={scale=#1} }} 然后把\\begin{tikzpicture}改成\\begin{tikzpicture}[scale=0.5]，就能把整个大小缩放为原来的0.5倍。","link":"/2022/03/17/plotneuralnet-in-a-project/"},{"title":"Point triangle distance","text":"介绍 该文章中的c++代码参考了ipc-toolkit的实现。 点到三角形的距离可以用如下定义： \\[ \\begin{aligned} \\text{distance}(\\vec{\\mathbf{x_p}}, \\vec{\\mathbf{x_t}_1}, \\vec{\\mathbf{x_t}_2}, \\vec{\\mathbf{x_t}_3}) &amp;= \\min_{\\beta_1, \\beta_2} \\left\\| \\vec{\\mathbf{x_p}} - ( \\vec{\\mathbf{x_t}_1} + \\beta_1 (\\vec{\\mathbf{x_t}_2} - \\vec{\\mathbf{x_t}_1}) + \\beta_2 (\\vec{\\mathbf{x_t}_3} - \\vec{\\mathbf{x_t}_1}) ) \\right\\| \\\\&amp;s.t. \\beta_1 \\geq 0, \\beta_2 \\geq 0, \\beta_1 + \\beta_2 \\leq 1 \\end{aligned} \\] 这是一个分断连续的函数，实际计算时可以根据点和三角形的位置关系分以下几种情况讨论，首先要将点投影到三角形所在的平面上： 投影后，点在三角形内部，此时距离为点到三角形所在平面的距离。 投影后，点在三角形的某个边朝外的半平面且投影在边上的点在边上，此时距离为点到边的距离。 其他情况，此时距离为点到三角形的三个顶点的最小距离。 因此首先判断点到三角形所在平面的距离的类型。 点到平面的距离类型判断 一个可行的计算方式是把点\\(\\mathbf{\\vec{x_p}}\\)分别投影到三角形三条边各自张成的二维空间，得到6个投影后的参数，通过参数的数值判断。因为三角形三条边已知，可以先计算出三角形的法向量，对于任意一条边\\(t_1 t_2\\)，可以构建一个直角坐标系，三个坐标轴分别是边的方向向量\\(\\vec{e_0} = \\mathbf{\\vec{x_{t2}}} - \\mathbf{\\vec{x_{t1}}}\\)，法向量和边的方向向量的叉乘\\(e_1 = \\vec{e_0} \\times \\vec{n}\\)，法向量\\(\\vec{n}\\)。那么因为\\(\\vec{e_0}, \\vec{e_1}\\)张成一个二维空间，可以将点\\(\\mathbf{\\vec{x_p}}\\)投影到这个平面上，得到坐标\\((\\lambda_0^{1,2}, \\lambda_1^{1,2})\\)。对于边\\(t_2 t_3\\)和\\(t_3 t_1\\)，同样可以得到坐标\\((\\lambda_0^{2,3}, \\lambda_1^{2,3})\\)和\\((\\lambda_0^{3,1}, \\lambda_1^{3,1})\\)。 最小二乘法计算投影坐标 要求解坐标，可以构建一个\\(3\\times 2\\) 的矩阵\\(A\\)，其中两个列向量分别是是边的方向向量\\(\\vec{e_0}\\)和法向量\\(\\vec{e_1}\\)，计算投影坐标等价于找到该空间内的一个点使得它与\\(\\mathbf{\\vec{x_p}}\\)的距离最短。可以通过最小二乘法求解： \\[ \\begin{aligned} \\min_{\\vec{x}} &amp; \\left\\| A \\vec{x} - \\mathbf{\\vec{x_p}} \\right\\| ^2 \\\\ &amp;= \\min_{\\vec{x}} \\vec x^T A^T A \\vec x - 2 \\mathbf{\\vec{x_p}}^T A \\vec x + \\mathbf{\\vec{x_p}}^T \\mathbf{\\vec{x_p}} \\\\ s.t. &amp; A = \\begin{bmatrix} e_{01} &amp; e_{02} \\\\ \\end{bmatrix}\\\\ &amp; x = \\begin{bmatrix} \\lambda_0^{1,2} \\\\ \\lambda_1^{1,2} \\end{bmatrix} \\end{aligned} \\] 令梯度为0，可以得到： \\[ \\begin{aligned} \\frac{\\partial}{\\partial \\vec{x}} \\left( \\vec x^T A^T A \\vec x - 2 \\mathbf{\\vec{x_p}}^T A \\vec x + \\mathbf{\\vec{x_p}}^T \\mathbf{\\vec{x_p}} \\right) &amp;= 0 \\\\ 2 A^T A \\vec x - 2 A^T \\mathbf{\\vec{x_p}} &amp;= 0 \\\\ A^T A \\vec x &amp;= A^T \\mathbf{\\vec{x_p}} \\\\ \\vec x &amp;= (A^T A)^{-1} A^T \\mathbf{\\vec{x_p}} \\end{aligned} \\] c++代码如下： 1234567891011121314151617template&lt;typename Scalar&gt;__forceinline__ __device__ glm::tvec2&lt;Scalar&gt; computeProjectedCoordinate( const glm::tvec3&lt;Scalar&gt;&amp; p, const glm::tvec3&lt;Scalar&gt;&amp; t0, const glm::tvec3&lt;Scalar&gt;&amp; t1, const glm::tvec3&lt;Scalar&gt;&amp; normal) { glm::tmat2x3&lt;Scalar&gt; basis; basis[0] = t1 - t0; basis[1] = glm::cross(basis[0], normal); glm::tmat2x2&lt;Scalar&gt; basisT_basis = glm::tmat2x2&lt;Scalar&gt;( glm::dot(basis[0], basis[0]), glm::dot(basis[0], basis[1]), glm::dot(basis[1], basis[0]), glm::dot(basis[1], basis[1]) ); return glm::inverse(basisT_basis) * glm::tvec2&lt;Scalar&gt;( glm::dot(basis[0], p - t0), glm::dot(basis[1], p - t0) );} 点到平面的距离类型的三种情况 当\\(\\lambda_0^{1,2}\\)满足\\(0 \\lt \\lambda_0^{1,2} \\lt 1\\)，\\(\\lambda_1^{1,2}\\)满足\\(\\lambda_1^{1,2} \\geq 0\\)时，距离为点到边\\(t_1 t_2\\)所在直线的距离。其他边以此类推。 如果点对于三条边的坐标都不满足条件1，那么\\(\\lambda_0^{1,2} \\lt 0, \\lambda_0^{3,1} \\ge 1\\)，则距离为点到\\(\\mathbf{x_{t1}}\\)的距离；\\(\\lambda_0^{2,3} \\lt 0, \\lambda_0^{1,2} \\ge 1\\)，则距离为点到\\(\\mathbf{x_{t2}}\\)的距离；\\(\\lambda_0^{3,1} \\lt 0, \\lambda_0^{2,3} \\ge 1\\)，则距离为点到\\(\\mathbf{x_{t3}}\\)的距离。 如果点对于三条边的坐标都不满足条件1和2，那么距离为点到三个平面的最小距离。 c++代码如下： 12345678910111213141516171819202122232425262728293031323334353637template&lt;typename Scalar&gt;__device__ DistanceType point_triangle_distance_type( const glm::tvec3&lt;Scalar&gt;&amp; p, const glm::tvec3&lt;Scalar&gt;&amp; t0, const glm::tvec3&lt;Scalar&gt;&amp; t1, const glm::tvec3&lt;Scalar&gt;&amp; t2){ glm::tvec3&lt;Scalar&gt; normal = glm::cross(t1 - t0, t2 - t0); glm::tmat3x2&lt;Scalar&gt; param; param[0] = computeProjectedCoordinate(p, t0, t1, normal); if (param[0][0] &gt; 0.0 &amp;&amp; param[0][0] &lt; 1.0 &amp;&amp; param[0][1] &gt;= 0.0) { return DistanceType::P_E0; } param[1] = computeProjectedCoordinate(p, t1, t2, normal); if (param[1][0] &gt; 0.0 &amp;&amp; param[1][0] &lt; 1.0 &amp;&amp; param[1][1] &gt;= 0.0) { return DistanceType::P_E1; } param[2] = computeProjectedCoordinate(p, t2, t0, normal); if (param[2][0] &gt; 0.0 &amp;&amp; param[2][0] &lt; 1.0 &amp;&amp; param[2][1] &gt;= 0.0) { return DistanceType::P_E2; } if (param[0][0] &lt;= 0.0 &amp;&amp; param[2][0] &gt;= 1.0) { return DistanceType::P_T0; } else if (param[1][0] &lt;= 0.0 &amp;&amp; param[0][0] &gt;= 1.0) { return DistanceType::P_T1; } else if (param[2][0] &lt;= 0.0 &amp;&amp; param[1][0] &gt;= 1.0) { return DistanceType::P_T2; } return DistanceType::P_T;} 计算距离 下面给出的c++代码计算的都是距离的平方。 点到点的距离 点到点的距离是两点之间的欧氏距离。 \\[ \\text{distance}(\\vec{\\mathbf{x_p}}, \\vec{\\mathbf{x_q}}) = \\left\\| \\vec{\\mathbf{x_p}} - \\vec{\\mathbf{x_q}} \\right\\| \\] c++代码如下： 1234567template&lt;typename Scalar&gt;__device__ Scalar point_point_distance( const glm::tvec3&lt;Scalar&gt;&amp; p0, const glm::tvec3&lt;Scalar&gt;&amp; p1){ return glm::length2(p0 - p1);} 点到直线的距离 点到直线的距离可以通过计算点与直线上两点连线得的三角形的面积的2倍除以直线上两点的长度得到。 \\[ \\text{distance}(\\vec{\\mathbf{x_p}}, \\vec{\\mathbf{x_t}_1}, \\vec{\\mathbf{x_t}_2}) = \\frac{\\left\\| (\\vec{\\mathbf{x_p}} - \\vec{\\mathbf{x_t}_1}) \\times (\\vec{\\mathbf{x_p}} - \\vec{\\mathbf{x_t}_2}) \\right\\|}{\\left\\| \\vec{\\mathbf{x_t}_2} - \\vec{\\mathbf{x_t}_1} \\right\\|} \\] c++代码如下： 12345678template&lt;typename Scalar&gt;__device__ Scalar point_line_distance( const glm::tvec3&lt;Scalar&gt;&amp; p, const glm::tvec3&lt;Scalar&gt;&amp; e0, const glm::tvec3&lt;Scalar&gt;&amp; e1){ return glm::length2(glm::cross(e0 - p, e1 - p)) / glm::length2(e1 - e0);} 点到平面的距离 点到平面的距离是点到平面的法向量的投影。 \\[ \\text{distance}(\\vec{\\mathbf{x_p}}, \\vec{\\mathbf{x_t}_1}, \\vec{\\mathbf{x_t}_2}, \\vec{\\mathbf{x_t}_3}) = \\frac{\\left| (\\vec{\\mathbf{x_p}} - \\vec{\\mathbf{x_t}_1}) \\cdot \\vec{n} \\right|}{\\left\\| \\vec{n} \\right\\|} \\] c++代码如下： 123456789template&lt;typename Scalar&gt;__device__ Scalar point_plane_distance( const glm::tvec3&lt;Scalar&gt;&amp; p, const glm::tvec3&lt;Scalar&gt;&amp; origin, const glm::tvec3&lt;Scalar&gt;&amp; normal){ const Scalar point_to_plane = glm::dot(p - origin, normal); return point_to_plane * point_to_plane / glm::dot(normal, normal);}","link":"/2024/09/15/point-triangle-distance/"},{"title":"程序优化","text":"C代码中有许多比较通用的可以优化的地方。其中大部分优化方式在其他语言中也是有效的。 减弱强度 使用指令个数更少的代码或者需要总的时钟周期更少的代码代替原代码，比如位移动代替乘法除法等。 使用局部变量(代码移动) 表达式 如果某些表达式被重复计算了，可以提前保存。一般来说O1以上会自动开启此优化。 12345678// 1.cint t(int i){ int x=3+i*i; int y=4+i*i; int z=5+i*i; int xx=x/y+y/z+z; return xx;} 用-S生成汇编，-Ox指定优化等级： 1gcc 1.c -S 使用上面的命令结果为： 1234567891011121314151617181920212223242526272829303132333435363738t: pushq %rbp .seh_pushreg %rbp movq %rsp, %rbp .seh_setframe %rbp, 0 subq $16, %rsp .seh_stackalloc 16 .seh_endprologue movl %ecx, 16(%rbp) movl 16(%rbp), %eax imull 16(%rbp), %eax addl $3, %eax movl %eax, -4(%rbp) movl 16(%rbp), %eax imull 16(%rbp), %eax addl $4, %eax movl %eax, -8(%rbp) movl 16(%rbp), %eax imull 16(%rbp), %eax addl $5, %eax movl %eax, -12(%rbp) movl -4(%rbp), %eax cltd idivl -8(%rbp) movl %eax, %ecx movl -8(%rbp), %eax cltd idivl -12(%rbp) leal (%rcx,%rax), %edx movl -12(%rbp), %eax addl %edx, %eax movl %eax, -16(%rbp) movl -16(%rbp), %eax addq $16, %rsp popq %rbp ret .seh_endproc .ident &quot;GCC: (x86_64-posix-seh-rev0, Built by MinGW-W64 project) 8.1.0&quot; 1gcc 1.c -S -O1 使用上面的命令结果为： 123456789101112131415161718192021222324 .file &quot;1.c&quot; .text .globl t .def t; .scl 2; .type 32; .endef .seh_proc tt: .seh_endprologue imull %ecx, %ecx leal 4(%rcx), %r8d leal 5(%rcx), %r9d addl $3, %ecx movl %ecx, %eax cltd idivl %r8d movl %eax, %ecx movl %r8d, %eax cltd idivl %r9d addl %ecx, %eax addl %r9d, %eax ret .seh_endproc .ident &quot;GCC: (x86_64-posix-seh-rev0, Built by MinGW-W64 project) 8.1.0&quot; 可见gcc是会主动优化该类型的冗余代码的。 函数调用 循环等过程中重复调用函数会花非常多的不必要的时间。因为编译器对此函数一无所知，如果存在副作用则优化后程序的结果会与优化前不一样，所以会避免对其使用优化。应主动使用局部变量来保存此重复调用的函数。 内存别名 如果赋值时使用了内存中的值，可以换成用局部变量保存（比如循环中保存累加的值），这样可以避免编译器无法判断此段代码是否使用了内存别名（比如对数组求和但是保存在数组内部）。因为优化后会得到不一样的结果而不进行优化。下面是这个例子 12345678910void sum_rows1(double *a, double *b, long n){ long i, j; for (i = 0; i &lt; n; i++) { b[i] = 0; for (j = 0; j &lt; n; j++) b[i] += a[i*n + j]; }} 可以用局部变量代替b[i]。 1234567891011void sum_rows2(double *a, double *b, long n){ long i, j; for (i = 0; i &lt; n; i++) { double val = 0; for (j = 0; j &lt; n; j++) val += a[i*n + j]; b[i] = val; }} 针对机器对代码进行调整 计算重排 此部分针对于流水线CPU，对部分代码进行重排可以更高效利用ALU。 对于表达式来说，其中如果表达式右边包含多个乘法，则可以使用多个单次乘法及赋值代替一次全部乘起来。这样会避免内存的依赖，因此流水线处理器可以保证每个乘法同时处于执行的不同阶段，而不会需要等待上一条指令写回后再执行下一条。 下面的代码里，将一个数组中各个元素乘起来，用x保存。这里用一次乘两个元素的方式减少了一半赋值语句。 12345678910111213141516171819void unroll2a_combine(vec__ptr v, data_t *dest){ long length = vec_length(v); long limit = length - 1; data_t *d = get_vec_start(v); data_t x = IDENT; long i; /* Combine 2 elements at a time */ for (i = 0; i &lt; limit; i += 2) { x = (x OP d[i])OP d[i + l]; } /* Finish any remaining elements */ for (; i &lt; length; i++) { x = x OP d[i]; } *dest = x;} 但是仍然有优化空间，因为这里每次都乘到x上，需要等x更新完才能继续下一次计算，流水线上其他元件不能不闲着。将乘数全部换成d中的元素，则每次执行时都没有互相的内存依赖（d[0],d[1]一组，d[2],d[3]一组，互不依赖）。 12345678910111213141516171819void unroll2a_combine(vec__ptr v, data_t *dest){ long length = vec_length(v); long limit = length - 1; data_t *d = get_vec_start(v); data_t x = IDENT; long i; /* Combine 2 elements at a time */ for (i = 0; i &lt; limit; i += 2) { x = x OP (d[i] OP d[i + l]); } /* Finish any remaining elements */ for (; i &lt; length; i++) { x = x OP d[i]; } *dest = x;} 这样可能存在的问题是，浮点数乘法不一定满足结合律，但是对于目前我们常使用的位数（32，64）即使存在误差，大部分时间是可以接受的。这个优化方法叫做循环展开（loop unrolling）。 避免无法预测的分支 条件移动可以在流水线中进行，条件跳转则如果遇到不可预测的分支可能出现预测（此时会执行预测结果开始的指令，但是不修改寄存器和内存）失败而直接回到分支前重新执行。 条件移动的例子是： 123456long f(long x,long y){ long res; if(x&gt;y)res=x-y; else res=y-x; return res;} 汇编为： 12345678910111213141516 .file &quot;1.c&quot; .text .globl f .def f; .scl 2; .type 32; .endef .seh_proc ff: .seh_endprologue movl %ecx, %r8d subl %edx, %r8d movl %edx, %eax subl %ecx, %eax cmpl %edx, %ecx cmovg %r8d, %eax ret .seh_endproc .ident &quot;GCC: (x86_64-posix-seh-rev0, Built by MinGW-W64 project) 8.1.0&quot; 可以看到使用了cmovg来设置返回值%eax，否的情况则跳过。 如果使用下面的命令进行编译，汇编里就会使用条件分支。 1gcc 1.c -S -O1 -fno-if-conversion 123456789101112131415161718192021 .file &quot;1.c&quot; .text .globl f .def f; .scl 2; .type 32; .endef .seh_proc ff: .seh_endprologue cmpl %edx, %ecx jle .L2 movl %ecx, %eax subl %edx, %eax.L1: ret.L2: movl %edx, %eax subl %ecx, %eax jmp .L1 .seh_endproc .ident &quot;GCC: (x86_64-posix-seh-rev0, Built by MinGW-W64 project) 8.1.0&quot;} 这里是题外话，一般来说需要避免条件移动，有两个原因： 1. 他会将两个分支都进行计算，但是实际上两个分支的内容有多复杂是不知道的，有可能会额外执行非常多无意义的指令，而且有可能某分支的内容对于当前条件是非法的（比如对空指针进行解引用）。对GCC一般来说只有两个分支都是非常简单的语句才会使用。 2. 不必要执行的分支可能存在副作用。 这部分我的理解是，不要写太多奇怪的条件语句，尤其是循环内部，这样会使得预测结果错误比较频繁。比如如果只是最后退出循环的时候条件预测错了一次，开销相对整体的多次循环并不算很大。","link":"/2023/01/10/program-optimization/"},{"title":"光线投射（raycast）公式中的far-clip","text":"光线投射公式 从相机发出一条光线，穿过与相机距离为1的屏幕上的某点，已知相机参数和点在屏幕空间上的位置(sx,sy)，计算世界空间中投影到该点的对应点的坐标的公式为： \\[ P = ViewMat^{-1} * ProjMat^{-1} * ((sx, sy, 1, 1) * farClip). \\] 使用下面的公式得到的结果实际上是一样的。 \\[ P = ViewMat^{-1} * ProjMat^{-1} * (sx, sy, 1, 1), \\] 但是需要对向量进行规范化，即： \\[ P=[p.x,p.y,p.z,p.w]/p.w. \\] 证明 投影公式ProjMat为: \\[ P=\\begin{bmatrix} \\frac{1}{A\\tan(FOV/2)} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{\\tan(FOV/2)} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{f}{f-n} &amp; \\frac{-fn}{f-n}\\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix}. \\] 因为ViewMat是齐次矩阵，不修改w，所以只考虑投影公式。首先计算它的逆。 因为左下角与右上角为0，可以视为\\(2\\times 2\\)的分块矩阵，计算逆时只需要计算右下角的逆即可。左上角为对角矩阵，逆是对角元素的倒数。2维矩阵较小，可以用伴随矩阵计算逆。伴随矩阵公式为： \\[ adj({ {\\begin{bmatrix}{a}&amp;{b}\\\\{c}&amp;{d}\\end{bmatrix} }})={ {\\begin{bmatrix}\\,\\,\\,{d}&amp;\\!\\!{-b}\\\\{-c}&amp;{a}\\end{bmatrix} }}. \\] 关于n×n矩阵A，有 \\[ \\mathbf{A}\\, \\mathrm{adj}(\\mathbf{A}) = \\mathrm{adj}(\\mathbf{A})\\, \\mathbf{A} = \\det(\\mathbf{A})\\, \\mathbf{I}. \\] 即 \\[ \\mathbf{A}^{-1} = \\det(\\mathbf{A})^{-1}\\, \\mathrm{adj}(\\mathbf{A}) \\] 令 \\[ A=\\begin{bmatrix} \\frac{f}{f-n} &amp; \\frac{-fn}{f-n}\\\\ 1 &amp; 0 \\end{bmatrix} \\] 则 \\[ det(A)=\\frac{fn}{f-n}.\\\\ A^{-1}=\\frac{1}{\\frac{fn}{f-n} }\\begin{bmatrix} 0 &amp; \\frac{fn}{f-n}\\\\ -1 &amp; \\frac{f}{f-n} \\end{bmatrix}=\\begin{bmatrix} 0 &amp; 1\\\\ -\\frac{f-n}{fn} &amp; \\frac{1}{n} \\end{bmatrix} \\] 因此投影矩阵的逆为： \\[ P^{-1}=\\begin{bmatrix} {A\\tan(FOV/2)} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; {\\tan(FOV/2)} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; -\\frac{f-n}{fn} &amp; \\frac{1}{n} \\end{bmatrix} \\] 根据公式： \\[ \\begin{bmatrix} sx\\\\ sy\\\\ 1\\\\ 1 \\end{bmatrix} \\times farClip= \\begin{bmatrix} sx\\times farClip\\\\ sy\\times farClip\\\\ farClip\\\\ farClip \\end{bmatrix}, \\] \\[ P^{-1}\\begin{bmatrix} sx\\times farClip\\\\ sy\\times farClip\\\\ farClip\\\\ farClip \\end{bmatrix}= \\begin{bmatrix} {A\\tan(FOV/2)}sx\\times farClip\\\\ {\\tan(FOV/2)}sy\\times farClip\\\\ farClip\\\\ 1 \\end{bmatrix} \\] 对比直接相乘的结果： \\[ P^{-1}\\begin{bmatrix} sx\\\\ sy\\\\ 1\\\\ 1 \\end{bmatrix}= \\begin{bmatrix} {A\\tan(FOV/2)}sx\\\\ {\\tan(FOV/2)}sy\\\\ 1\\\\ 1 \\end{bmatrix}= \\begin{bmatrix} {A\\tan(FOV/2)}sx\\\\ {\\tan(FOV/2)}sy\\\\ 1\\\\ \\frac{1}{f} \\end{bmatrix} \\] 可以看出，无论乘不乘farClip规范化后的结果确实是一样的。但是因为glm并没有提供我们需要的规范化的函数，所以通过乘farClip来省去手动规范化的步骤是可以提高效率的。","link":"/2023/01/14/raycast-formula/"},{"title":"渲染方程背景知识","text":"本来想写BSDF的推导，但是发现基础知识很多，正好整理一下作为复习。部分内容参考Physically Based Rendering, Third Edition，非常推荐阅读。 概率论入门 概率密度函数(PDF: probability distribution function) 以一维为例，如果随机变量\\(X\\)是连续的，就需要使用概率密度函数\\(p(x)\\)来描述它。\\(P(x)\\)是累积分布函数。 \\[ P(x)=\\int_{-\\infty}^{x} p(x)dx. \\] 因为所有事件发生概率之和为1，所以如果\\(x\\in[a,b]\\)， \\[ P(b)=1. \\] 期望与方差(Expected Values and Variance) 如果有一个\\(x\\)的分布\\(p(x)\\)和一个函数\\(f\\)，对应的期望\\(E_p[f(x)]\\)的计算公式为： \\[ E_p[f(x)]=\\int_{D} f(x)p(x)dx. \\] 举个二维的例子，向一个半径为\\(r_0\\)的靶子投飞镖，得分计算方式如下： \\[ f(x,y)=\\sqrt{x^2+y^2} \\] 如果投手投出的飞镖是均匀分布在该靶子上，那么 \\[ p(x,y)=\\frac{1}{\\pi r_0^2} \\] 期望则是： \\[ E_p[f(x)]=\\int_{D} \\frac{\\sqrt{x^2+y^2}}{\\pi r_0^2}dxdy, \\] \\(D\\)为半径为\\(r\\)的圆内部。 该例子用极坐标变换计算： \\[ x=r\\cos\\theta, y=r\\sin\\theta.\\\\ \\frac{d(x,y)}{d(r,\\theta)}=\\begin{vmatrix}\\frac{\\partial x}{\\partial r}&amp;\\frac{\\partial x}{\\partial \\theta}\\\\\\frac{\\partial y}{\\partial r}&amp;\\frac{\\partial y}{\\partial \\theta}\\end{vmatrix}=\\begin{vmatrix}\\cos\\theta&amp; -r\\sin\\theta\\\\\\sin \\theta &amp; r \\cos \\theta\\end{vmatrix}=r\\\\ \\begin{align*} \\int_{D} \\frac{\\sqrt{x^2+y^2}}{\\pi r^2}dxdy &amp;=\\int_{D} \\frac{r}{\\pi r_0^2}|\\frac{d(x,y)}{d(r,\\theta)}|drd\\theta\\\\ &amp;=\\int_{0}^{2\\pi}\\int_{0}^{r_0} \\frac{r^2}{\\pi r_0^2}drd\\theta\\\\ &amp;=\\frac{2r_0}{3} \\end{align*} \\] 也就是说，按照上述规则投手最终得分为\\(\\frac{2r_0}{3}\\)分。 方差描述了随机变量的值与期望之间差别的偏离程度，方差越大，说明该随机变量的值与期望有较大差距。比如某地降雨量的期望是100mm，但是产生的值只为0mm或者200mm且等概率出现，那么此随机变量的方差与每次降雨量产生的值都为100mm的方差会大很多，后者为0。 计算公式为: \\[ \\begin{align*} V[f(x)]&amp;=E[(f(x)-E[f(x)])^2]\\\\ &amp;= E[f(x)^2-2f(x)E[f(x)]+{(E[f(x)])}^2]\\\\ &amp;= E[f(x)^2]-2E[f(x)E[f(x)]]+E[{(E[f(x)])}^2]\\\\ &amp;= E[f(x)^2]-2E[f(x)]E[f(x)]+{(E[f(x)])}^2\\\\ &amp;= E[f(x)^2]-{E[f(x)]}^2 \\end{align*} \\] 因为\\(E[f(x)]\\)是一个常数，因此他的期望就是自己，可以由此化简。 蒙特卡洛估计(The Monte Carlo Estimator) 想要计算 \\[ F(x)=\\int_a^bf(x)dx， \\] 给定一个均匀分布的随机变量\\(X_i\\in[a,b]\\)，可以通过下面的公式估计\\(F(x)\\)： \\[ F_N=\\frac{b-a}{N}\\sum_{i=1}^Nf(X_i)， \\] 可以证明\\(E[F_N]\\)=F(x)： 首先，由均匀分布可知： \\[ p(x)=\\frac{1}{b-a} \\] 因此， $$ \\[\\begin{align*} E[F_N]&amp;=E[\\frac{b-a}{N}\\sum_{i=1}^Nf(X_i)]\\\\ &amp;=\\frac{b-a}{N}\\sum_{i=1}^NE[f(X_i)]\\\\ &amp;=\\frac{b-a}{N}\\sum_{i=1}^N\\int_a^bf(x)p(x)dx\\\\ &amp;=\\frac{1}{N}\\sum_{i=1}^N\\int_a^bf(x)dx\\\\ &amp;=\\int_a^bf(x)dx \\end{align*}\\] $$ 如果随机变量服从分布\\(p(x)\\)，依然可以通过下面的公式估计\\(F(x)\\)： \\[ F_N=\\frac{1}{N}\\sum_{i=1}^N\\frac{f(X_i)}{p(X_i)}， \\] 证明如下： $$ \\[\\begin{align*} E[F_N]&amp;=E[\\frac{1}{N}\\sum_{i=1}^N\\frac{f(X_i)}{p(X_i)}]\\\\ &amp;=\\frac{1}{N}\\sum_{i=1}^NE[\\frac{f(X_i)}{p(X_i)}]\\\\ &amp;=\\frac{1}{N}\\sum_{i=1}^N\\int_a^b\\frac{f(x)}{p(x)}p(x)dx\\\\ &amp;=\\frac{1}{N}\\sum_{i=1}^N\\int_a^bf(x)dx\\\\ &amp;=\\int_a^bf(x)dx \\end{align*}\\] $$ 该估计方式可以扩展到高维，在高维的情况下，使用该方法是能大幅度提高计算效率的。要注意的是，该方法的误差为\\(O(\\sqrt{N})\\)，也就是说采样数为10000的估计结果只比采样数为100精确一个小数点。 渲染方程实际上无法直接解，要通过离散化计算结果。最常用的方法是蒙特卡洛积分(Monte Carlo Integration)。 采样随机变换(Sampling Random Variables)：逆变换采样(The Inversion Method) 动机 实际使用随机变量时，我们希望能够在给定的分布中采样随机变量，这样就可以用更快收敛的方式用蒙特卡洛积分来估计积分。 步骤 计算累积分布函数CDF:\\(P(x)=\\int_0^xp(x')dx'\\)。 求CDF的反函数(inverse) \\(P^{-1}(x)\\)。 使用均匀分布的随机变量采样得到\\(\\xi\\)。 \\(X_i=P^{-1}(\\xi)\\)。 以cosine-weighted采样为例，已知，在球坐标系中任意球面的极小面积为： \\[ dA = (r\\sin\\theta\\,d\\varphi)(r d\\theta)=r^2(\\sin\\theta\\,d\\theta\\,d\\varphi) \\] 立体角是投影面积与球半径平方值的比，立体角与球面坐标的微分有如下转换关系: \\[ d\\omega=\\sin\\theta d\\theta d\\phi. \\] 易得半个球面的曲面积分为： \\[ \\begin{align*} \\int_H d\\omega&amp;= \\oiint_H \\sin\\theta d\\theta d\\phi\\\\ &amp;=\\int_0^{2\\pi}d\\phi\\int_0^{\\frac{\\pi}{2}}\\sin\\theta d\\theta \\\\ &amp;=2\\pi. \\end{align*} \\] 我们希望采样的点满足\\(p(\\omega)=C cos\\theta\\)。代入积分得 \\[ \\begin{align*} \\int_H p(\\omega)d\\omega&amp;= \\oiint_H C cos\\theta\\sin\\theta d\\theta d\\phi\\\\ &amp;=C\\int_0^{2\\pi}d\\phi\\int_0^{\\frac{\\pi}{2}}cos\\theta\\sin\\theta d\\theta \\\\ &amp;=C\\pi \\end{align*} \\] 由此可知 $$ \\[\\begin{align*} p(\\omega)&amp;= \\frac{cos\\theta}{\\pi}\\\\ p(\\theta,\\phi)&amp;=\\frac{sin\\theta cos\\theta}{\\pi} \\end{align*}\\] $$ 对两个变量分别积分可得： \\[ \\begin{align*} p(\\theta)&amp;=\\int_{0}^{2\\pi}\\frac{sin\\theta cos\\theta}{\\pi}d\\phi=\\sin2\\theta\\\\ P(\\theta)&amp;=\\int_{0}^{\\theta}p(\\theta)d\\theta\\\\&amp;=\\frac{1-\\cos2\\theta}{2}\\\\ &amp;=\\sin^2\\theta\\\\ p(\\phi)&amp;=\\int_{0}^{\\frac{\\pi}{2}}\\frac{sin\\theta cos\\theta}{\\pi}d\\theta=\\frac{1}{2\\pi}\\\\ P(\\phi)&amp;=\\int_{0}^{\\phi}p(\\phi)d\\phi=\\frac{\\phi}{2\\pi}\\\\ \\end{align*} \\] 计算反函数： \\[ \\begin{align*} P^{-1}(\\theta)&amp;=\\arcsin{\\sqrt{\\theta}}\\\\ P^{-1}(\\phi)&amp;=2\\pi \\phi\\\\ \\end{align*} \\] 采样步骤为： 在\\([-1,1]\\)上均匀采样\\(x\\)，计算\\(\\arcsin{\\sqrt{x}}\\)得到\\(\\theta\\) 在\\([-1,1]\\)上均匀采样\\(y\\)，计算\\(2\\pi y\\)得到\\(\\phi\\) The Rejection Method 对于无法求反函数的\\(f(x)\\)的CDF，可以用The Rejection Method： 循环： 从已知的\\(p(x)\\)采样X 如果\\(xi&lt;f(X)/c\\cdot p(X)\\) 返回X","link":"/2023/02/09/rendering-equation-bg/"},{"title":"渲染方程及BSDFs","text":"光传播方程(The Light Transport Equation, LTE) \\[ L_o(p,\\omega_o)=L_e(p,\\omega_o)+\\int_{s^2}f(p,\\omega_o,\\omega_i)|\\cos\\theta_i|d\\omega_i \\] 漫反射 上一篇文章提到，立体角与球面坐标的微分有如下转换关系： \\[ d\\omega=\\sin\\theta d\\theta d\\phi. \\] 漫反射的BSDF应该是一个常数函数，假设为\\(f_{lambert}\\)，因为反照率的定义为： \\[ albedo(p)=\\frac{L_o(p,w_o)}{L_i(p,w_i)} \\] 根据只考虑漫反射时的渲染方程： \\[ \\begin{align*} L_o(p,w_o)&amp;=\\int_H f \\cos\\theta L_i(p,w_i) d\\omega\\\\ &amp;=f_{lambert}\\int_H \\cos\\theta L_i(p,w_i) d\\omega\\\\ &amp;=f_{lambert} L_i(p,w_i)\\int_H \\cos\\theta d\\omega \\end{align*} \\] 所以： \\[ f_{lambert}=\\frac{albedo(p)}{\\int_H \\cos\\theta d\\omega} \\] 上一篇文章的最后一节计算过: \\[ \\begin{align*} \\int_H \\cos\\theta d\\omega&amp;= \\oiint_H cos\\theta\\sin\\theta d\\theta d\\phi\\\\ &amp;=\\int_0^{2\\pi}d\\phi\\int_0^{\\frac{\\pi}{2}}cos\\theta\\sin\\theta d\\theta \\\\ &amp;=\\pi. \\end{align*} \\] 因此漫反射的BSDF为: \\[ f_{lambert}=\\frac{albedo(p)}{\\pi}. \\]","link":"/2023/02/10/rendering-equation/"},{"title":"GAMES103:刚体模拟笔记","text":"2 Math Tetrahedral Volume 1681077071804 1681077089478 Barycentric Weights 1681077102213 1681077142508 Particle-triangle Intersection First, we find 𝑡 when the particle hits the plane: 1681077167578 We then check if 𝐩(𝑡) is inside or not. Singular Value Decomposition \\[ A=UDV^T \\] Any linear deformation can be decomposed into three steps: rotation, scaling and rotation Symmetric Positive Definiteness (s.p.d.) A is s.p.d. if only if: \\(v^T A𝐯&gt;0\\), for any 𝐯≠0. A is symmetric semi-definite if only if: \\(v^T A𝐯≥0\\), for any 𝐯≠0. \\[ 𝑎_{𝑖𝑖}&gt;∑_{𝑖≠𝑗}|𝑎_{𝑖𝑗} | \\forall 𝑖 \\] A diagonally dominant matrix is p.d. Finally, a s.p.d. matrix must be invertible: \\[ A^{−1}=(U^T )^{−1} D^{−1} U^{−1}=𝐔D^{−1} U^T. \\] 1681077471574 Linear Solver A direct solver is typically based LU factorization, or its variant: Cholesky, LDLT, etc… When A is sparse, L and U are not so sparse. Their sparsity depends on the permutation. (See matlab) It contains two steps: factorization and solving. If we must solve many linear systems with the same A, we can factorize it only once. Cannot be easily parallelized: Intel MKL PARDISO Iterative Linear Solver 1681077521644 1681077543465 Tensor Calculus 1681077579832 1681077587043 1681077593875 3 rigid Torque and Inertia 1681077744681 Translational and Rotational Motion 1681077765559 Rigid Body Simulation 1681077796806 Some More Implementation Issues Translational motion is much easier to implement than rotational motion. You can implement the update of 𝐪 first using a constant 𝛚. In that case, the object should spin constantly. Gravity doesn’t cause any torque! If your simulator does not contain any other force, there is no need to update 𝛚. 4 Rigid Contact Particle Collision Detection and Response Signed Distance Function A signed distance function 𝜙(𝐱) defines the distance from 𝐱 to a surface with a sign. The sign indicates on which side 𝐱 is located. 1681077902937 Penalty methods Quadratic Penalty Method \\[ f\\leftarrow −𝑘 \\phi(x) N \\] #### Quadratic Penalty Method with a Buffer \\[ f\\leftarrow 𝑘(\\epsilon − \\phi(x))N \\] #### Log-Barrier Penalty Method \\[ f\\leftarrow \\rho\\frac{1}{\\phi(x)}N \\] #### A Short Summary of Penalty Methods * The use of step size adjustment is a must. To avoid overshooting. To avoid penetration in log-barrier methods. Log-barrier method can be limited within a buffer as well. Li et al. 2020. Incremental Potential Contact: Intersection- and Inversion-free Large Deformation Dynamics. TOG. Wu et al. 2020. A Safe and Fast Repulsion Method for GPU-based Cloth Self Collisions. TOG. Frictional contacts are difficult to handle. Impulse methods collision: \\[ x^{new}\\leftarrow x+|\\phi(x)|N=x−\\phi(x)∇\\phi(x) \\] 1681078352423 Rigid Collision Detection and Response by Impulse 1681078374348 Shape Matching Basic Idea 1681078420872 Mathematical Formulation 1681078437745 1681078456687 pros and cons Easy to implement and compatible with other nodal systems, i.e., cloth, soft bodies and even particle fluids. Difficult to strictly enforce friction and other goals. The rigidification process will destroy them. More suitable when the friction accuracy is unimportant, i.e., buttons on clothes.","link":"/2023/04/09/rigidBody-note/"},{"title":"Stress hessian computation in FEM","text":"介绍 在进行软体模拟时，如果使用牛顿法计算最优的下降方向，需要计算能量密度函数\\(\\Psi\\)关于位置\\(\\vec {\\mathbf{x}}\\) 的Hessian矩阵，即\\(\\frac{\\partial^2 \\Psi}{\\partial \\vec {\\mathbf{x}}^2}\\)。其中\\(\\vec {\\mathbf{x}}\\)是一个四面体的四个顶点的位置。因为： \\[ \\begin{aligned} \\frac{\\partial^2 \\Psi}{\\partial \\vec {\\mathbf{x}}^2} &amp;= \\frac{\\partial}{\\partial \\vec {\\mathbf{x}}} \\left(\\frac{\\partial \\Psi}{\\partial \\vec {\\mathbf{x}}}\\right) \\\\ &amp;= \\frac{\\partial}{\\partial \\vec {\\mathbf{x}}} \\left(\\frac{\\partial \\Psi}{\\partial F}:\\frac{\\partial F}{\\partial \\vec {\\mathbf{x}}}\\right) \\\\ &amp;= \\frac{\\partial}{\\partial \\vec {\\mathbf{x}}} \\left(\\frac{\\partial \\Psi}{\\partial F}\\right):\\frac{\\partial F}{\\partial \\vec {\\mathbf{x}}} + \\frac{\\partial \\Psi}{\\partial F}:\\frac{\\partial^2 F}{\\partial \\vec {\\mathbf{x}}^2}\\\\ &amp;=\\frac{\\partial}{\\partial \\vec {\\mathbf{x}}} \\left(P\\right):\\frac{\\partial F}{\\partial \\vec {\\mathbf{x}}}\\\\ &amp;=(\\frac{\\partial F}{\\partial \\vec {\\mathbf{x}}}: \\frac{\\partial P}{\\partial F}):\\frac{\\partial F}{\\partial \\vec {\\mathbf{x}}} \\end{aligned} \\] 涉及到的几个张量的维度如下： \\(\\vec {\\mathbf{x}} = \\begin{bmatrix} \\vec x_1 &amp; \\vec x_2 &amp; \\vec x_3 &amp; \\vec x_4 \\end{bmatrix} \\in \\mathbb{R}^{12}\\) \\(F = D_s D_m^{-1} \\in \\mathbb{R}^{3\\times 3}\\) \\(P = \\frac{\\partial \\Psi}{\\partial F} \\in \\mathbb{R}^{3\\times 3}\\) \\(\\frac{\\partial P}{\\partial F} \\in \\mathbb{R}^{3\\times 3\\times 3\\times 3}\\) \\(\\frac{\\partial F}{\\partial \\vec {\\mathbf{x}}} \\in \\mathbb{R}^{3\\times 3\\times 12}\\)是一个常数张量 一些不变量： \\(I_C = ||F||_F^2 = \\text{tr}(F^TF)\\) \\(II_C = ||F^TF||_F^2\\) \\(III_C = \\det(F^TF) = J\\) 本文具体说明如何进行\\(\\frac{\\partial P}{\\partial F}\\)的计算，参考论文Nonlinear Material Design Using Principal Stretches的3.2节。 推导 Neo-Hookean模型 以Bonet and Wood (2008)-style Neo-Hookean为例： \\[ \\Psi(F)_{\\text{BW08}} = \\frac{\\mu}{2}(I_C-3) - \\mu \\ln J + \\frac{\\lambda}{2}(\\ln J)^2 \\] 可以对\\(F\\)进行Polar SVD分解(翻转\\(\\Sigma\\)符号使得\\(U,V\\)行列式为正)： \\[ F = U\\Sigma V^T \\] 则\\(\\Psi\\)可以有以下形式： \\[ \\begin{aligned} \\Psi(F)_{\\text{BW08}} &amp;=\\hat\\Psi(\\Sigma)_{\\text{BW08}} \\\\ &amp;=\\frac{\\mu}{2}(\\sum_{i=1}^3 \\sigma_i^2 - 3) - \\mu \\ln(\\sigma_1\\sigma_2\\sigma_3) + \\frac{\\lambda}{2}(\\ln(\\sigma_1\\sigma_2\\sigma_3))^2 \\end{aligned} \\] 那么： \\[ P = U \\hat P V^T \\] 其中： \\[ \\hat P = \\begin{bmatrix} \\frac{\\partial \\hat\\Psi}{\\partial \\sigma_{1}} &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{\\partial \\hat\\Psi}{\\partial \\sigma_{2}} &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{\\partial \\hat\\Psi}{\\partial \\sigma_{3}} \\end{bmatrix} \\] 对于Neo-Hookean模型： \\[ \\hat P_{ii} = \\mu(\\sigma_i-\\frac{1}{\\sigma_i}) + \\lambda \\ln(\\sigma_1\\sigma_2\\sigma_3)\\frac{1}{\\sigma_i} \\] 接下来计算\\(\\frac{\\partial P}{\\partial F}\\)。 \\[ \\begin{aligned} \\frac{\\partial P}{\\partial F_{ij}} &amp;= \\frac{\\partial U}{\\partial F_{ij}} \\hat P V^T + U \\frac{\\partial \\hat P}{\\partial \\sigma_{k}} \\frac{\\partial \\sigma_{k}}{\\partial F_{ij}} V^T + U \\hat P \\frac{\\partial V^T}{\\partial F_{ij}}\\\\ \\end{aligned} \\] 其中有三个未知量，分别是\\(\\frac{\\partial U}{\\partial F_{ij}}\\)，\\(\\frac{\\partial V^T}{\\partial F_{ij}}\\)和\\(\\frac{\\partial \\sigma_{k}}{\\partial F_{ij}}\\)。根据论文Nonlinear Material Design Using Principal Stretches的3.2节，可以使用\\(\\frac{\\partial F}{\\partial F_{ij}}\\)来计算这三个未知量。 \\[ \\begin{aligned} \\frac{\\partial F}{\\partial F_{ij}} &amp;= \\frac{\\partial U}{\\partial F_{ij}} \\Sigma V^T + U \\frac{\\partial \\Sigma}{\\partial F_{ij}} V^T + U \\Sigma \\frac{\\partial V^T}{\\partial F_{ij}}\\\\ U^T \\frac{\\partial F}{\\partial F_{ij}} V &amp;= U^T \\frac{\\partial U}{\\partial F_{ij}} \\Sigma + \\frac{\\partial \\Sigma}{\\partial F_{ij}} + \\Sigma \\frac{\\partial V^T}{\\partial F_{ij}} V\\\\ \\end{aligned} \\] 已知\\(U^T U = I\\)，\\(V^T V = I\\)，有： \\[ \\begin{aligned} \\frac{\\partial U^T U}{\\partial F_{ij}} &amp; = \\frac{\\partial U^T}{\\partial F_{ij}} U + U^T \\frac{\\partial U}{\\partial F_{ij}} \\\\ &amp; = (U^T \\frac{\\partial U}{\\partial F_{ij}} )^T + U^T \\frac{\\partial U}{\\partial F_{ij}} = 0\\\\ \\frac{\\partial V^T V}{\\partial F_{ij}} &amp; = \\frac{\\partial V^T}{\\partial F_{ij}} V + V^T \\frac{\\partial V}{\\partial F_{ij}}\\\\ &amp; = \\frac{\\partial V^T}{\\partial F_{ij}} V + (V^T \\frac{\\partial V}{\\partial F_{ij}})^T = 0\\\\ \\end{aligned} \\] 所以，\\(\\frac{\\partial U}{\\partial F_{ij}}\\)，\\(\\frac{\\partial V^T}{\\partial F_{ij}}\\)都是反对称矩阵。对角线元素都为0。假设 \\[ \\begin{aligned} \\Sigma &amp;= \\begin{bmatrix} \\sigma_1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_3 \\end{bmatrix}\\\\ U^T\\frac{\\partial F}{\\partial F_{ij}} V&amp;= \\begin{bmatrix} f_{11} &amp; f_{12} &amp; f_{13} \\\\ f_{21} &amp; f_{22} &amp; f_{23} \\\\ f_{31} &amp; f_{32} &amp; f_{33} \\end{bmatrix},\\\\ U^T\\frac{\\partial U}{\\partial F_{ij}} &amp;= \\begin{bmatrix} 0 &amp; u_{12} &amp; u_{13} \\\\ -u_{12} &amp; 0 &amp; u_{23} \\\\ -u_{13} &amp; -u_{23} &amp; 0 \\end{bmatrix},\\\\ \\frac{\\partial V^T}{\\partial F_{ij}}V &amp;= \\begin{bmatrix} 0 &amp; v_{12} &amp; v_{13} \\\\ -v_{12} &amp; 0 &amp; v_{23} \\\\ -v_{13} &amp; -v_{23} &amp; 0 \\end{bmatrix} \\end{aligned} \\] 则： \\[ \\begin{aligned} U^T \\frac{\\partial U}{\\partial F_{ij}} \\Sigma + \\Sigma \\frac{\\partial V^T}{\\partial F_{ij}} V &amp;= \\begin{bmatrix} 0 &amp; \\sigma_2 u_{12} + \\sigma_1 v_{12} &amp; \\sigma_3 u_{13} + \\sigma_1 v_{13} \\\\ -\\sigma_1 u_{12} - \\sigma_2 v_{12} &amp; 0 &amp; \\sigma_3 u_{23} + \\sigma_2 v_{23} \\\\ -\\sigma_1 u_{13} - \\sigma_3 v_{13} &amp; -\\sigma_2 u_{23} - \\sigma_3 v_{23} &amp; 0 \\end{bmatrix} \\end{aligned} \\] 可以看出，\\(U^T \\frac{\\partial U}{\\partial F_{ij}} \\Sigma + \\Sigma \\frac{\\partial V^T}{\\partial F_{ij}} V\\)对\\(\\frac{\\partial F}{\\partial F_{ij}}\\)的对角元没有贡献，所以： \\[ \\frac{\\partial \\Sigma}{\\partial F_{ij}} = \\begin{bmatrix} f_{11} &amp; 0 &amp; 0 \\\\ 0 &amp; f_{22} &amp; 0 \\\\ 0 &amp; 0 &amp; f_{33} \\end{bmatrix} \\] 即\\(\\frac{\\partial \\sigma_{k}}{\\partial F_{ij}} = f_{kk}\\)，\\(k=1,2,3\\)。以及 \\[ \\begin{bmatrix} 0 &amp; \\sigma_2 u_{12} + \\sigma_1 v_{12} &amp; \\sigma_3 u_{13} + \\sigma_1 v_{13} \\\\ -\\sigma_1 u_{12} - \\sigma_2 v_{12} &amp; 0 &amp; \\sigma_3 u_{23} + \\sigma_2 v_{23} \\\\ -\\sigma_1 u_{13} - \\sigma_3 v_{13} &amp; -\\sigma_2 u_{23} - \\sigma_3 v_{23} &amp; 0 \\end{bmatrix} = \\begin{bmatrix} 0 &amp; f_{12} &amp; f_{13} \\\\ f_{21} &amp; 0 &amp; f_{23} \\\\ f_{31} &amp; f_{32} &amp; 0 \\end{bmatrix} \\] 三组对称位置的元素分别可以构成一个二元一次方程组。 \\[ \\begin{aligned} \\begin{bmatrix} \\sigma_2 &amp; \\sigma_1 \\\\ \\sigma_1 &amp; \\sigma_2 \\end{bmatrix} \\begin{bmatrix} u_{12} \\\\ v_{12} \\end{bmatrix} &amp;= \\begin{bmatrix} f_{12} \\\\ -f_{21} \\end{bmatrix},\\\\ \\begin{bmatrix} \\sigma_3 &amp; \\sigma_1 \\\\ \\sigma_1 &amp; \\sigma_3 \\end{bmatrix} \\begin{bmatrix} u_{13} \\\\ v_{13} \\end{bmatrix} &amp;= \\begin{bmatrix} f_{13} \\\\ -f_{31} \\end{bmatrix},\\\\ \\begin{bmatrix} \\sigma_3 &amp; \\sigma_2 \\\\ \\sigma_2 &amp; \\sigma_3 \\end{bmatrix} \\begin{bmatrix} u_{23} \\\\ v_{23} \\end{bmatrix} &amp;= \\begin{bmatrix} f_{23} \\\\ -f_{32} \\end{bmatrix} \\end{aligned} \\] 总结一下计算\\(\\frac{\\partial^2 \\Psi}{\\partial \\vec{x}^2}\\)的步骤： 计算\\(F = U\\Sigma V^T\\)，得到\\(\\sigma_1,\\sigma_2,\\sigma_3,U,V\\). 计算\\(\\hat P\\). 遍历\\(i,j\\)， 计算\\(\\frac{\\partial F}{\\partial F_{ij}}\\)，取\\(\\frac{\\partial F}{\\partial F_{ij}}\\)的对角元素作为\\(\\frac{\\partial \\sigma_{k}}{\\partial F_{ij}}\\)，\\(k=1,2,3\\). 用其他元素代入上面的三个方程组，解出\\(u_{12},v_{12},u_{13},v_{13},u_{23},v_{23}\\)，即得到\\(\\frac{\\partial U}{\\partial F_{ij}}\\)和\\(\\frac{\\partial V^T}{\\partial F_{ij}}\\). 使用公式\\(\\frac{\\partial P}{\\partial F_{ij}} = \\frac{\\partial U}{\\partial F_{ij}} \\hat P V^T + U \\frac{\\partial \\hat P}{\\partial \\sigma_{k}} \\frac{\\partial \\sigma_{k}}{\\partial F_{ij}} V^T + U \\hat P \\frac{\\partial V^T}{\\partial F_{ij}}, k=1,2,3\\). \\(\\frac{\\partial^2 \\Psi}{\\partial \\vec{x}^2} = \\text{vec}^T(\\frac{\\partial F}{\\partial \\vec{x}}) \\text{vec}(\\frac{\\partial P}{\\partial F}) \\text{vec}(\\frac{\\partial F}{\\partial \\vec{x}})\\) Corotated linear elasticity \\[ \\begin{aligned} F &amp;= U\\Sigma V^T\\\\ R &amp;= UV^T, S = V\\Sigma V^T\\\\ \\Psi(F) &amp;= \\mu ||F-R||_F^2 + \\frac{\\lambda}{2}(\\text{tr}^2(R^TF-I))\\\\ P &amp;= \\frac{\\partial \\Psi}{\\partial F} = 2\\mu(F-R) + \\lambda \\text{tr}(R^TF-I)R\\\\ \\frac{\\partial P}{\\partial F} &amp;= 2\\mu (I - \\frac{\\partial R}{\\partial F}) + \\lambda (R \\otimes R + \\text{tr}(R^TF-I)\\frac{\\partial R}{\\partial F})\\\\ &amp;=2\\mu (I - \\frac{\\partial R}{\\partial F}) + \\lambda [F \\otimes R + \\text{tr}(R^TF-I)\\frac{\\partial R}{\\partial R}] : \\frac{\\partial R}{\\partial F}\\\\ \\end{aligned} \\] 其中用到了\\(\\frac{\\partial R}{\\partial F}\\)，因为 \\[ \\frac{\\partial R}{\\partial F_{ij}} = \\frac{\\partial U}{\\partial F_{ij}} V^T + U \\frac{\\partial V^T}{\\partial F_{ij}} \\] 上一节的推导中可以通过\\(\\frac{\\partial F}{\\partial F_{ij}}\\)计算\\(\\frac{\\partial U}{\\partial F_{ij}}\\)和\\(\\frac{\\partial V^T}{\\partial F_{ij}}\\)，所以可以直接计算\\(\\frac{\\partial R}{\\partial F}\\)。 使用上面公式计算\\(\\frac{\\partial P}{\\partial F}\\)之后，计算Hessian矩阵的方法就和上一节一样了。","link":"/2024/08/08/stress-hessian/"},{"title":"张量","text":"对于三维空间中的各向同性材料，弹性模量张量（Elastic Modulus Tensor）可以视为一个\\(3 \\times 3\\)矩阵，其中每个元素本身又是一个 \\(3 \\times 3\\) 的矩阵。这样的表示反映了张量的四阶本质，即它有四个索引。 在这种表示下，张量的每个元素 \\(C_{ijkl}\\)表示当在 kl 方向上施加应力时，材料在 ij 方向上的应变响应。对于各向同性材料，这个张量可以通过两个Lamé常数 \\(\\lambda\\) 和 \\(\\mu\\) 来定义，其定义如下： \\[ C_{ijkl} = \\lambda \\delta_{ij} \\delta_{kl} + \\mu (\\delta_{ik} \\delta_{jl} + \\delta_{il} \\delta_{jk}) \\] 其中 \\(\\delta\\) 是Kronecker delta函数。 下面是弹性模量张量的具体展开形式，其中每个元素 \\(C_{ijkl}\\)根据上述公式计算： \\[\\begin{bmatrix} \\begin{bmatrix} C_{1111} &amp; C_{1122} &amp; C_{1133} \\\\ C_{1211} &amp; C_{1212} &amp; C_{1213} \\\\ C_{1311} &amp; C_{1312} &amp; C_{1313} \\\\ \\end{bmatrix} \\begin{bmatrix} C_{1121} &amp; C_{1122} &amp; C_{1123} \\\\ C_{1221} &amp; C_{1222} &amp; C_{1223} \\\\ C_{1321} &amp; C_{1322} &amp; C_{1323} \\\\ \\end{bmatrix} \\begin{bmatrix} C_{1131} &amp; C_{1132} &amp; C_{1133} \\\\ C_{1231} &amp; C_{1232} &amp; C_{1233} \\\\ C_{1331} &amp; C_{1332} &amp; C_{1333} \\\\ \\end{bmatrix} \\\\ \\begin{bmatrix} C_{2111} &amp; C_{2112} &amp; C_{2113} \\\\ C_{2211} &amp; C_{2212} &amp; C_{2213} \\\\ C_{2311} &amp; C_{2312} &amp; C_{2313} \\\\ \\end{bmatrix} \\begin{bmatrix} C_{2121} &amp; C_{2122} &amp; C_{2123} \\\\ C_{2221} &amp; C_{2222} &amp; C_{2223} \\\\ C_{2321} &amp; C_{2322} &amp; C_{2323} \\\\ \\end{bmatrix} \\begin{bmatrix} C_{2131} &amp; C_{2132} &amp; C_{2133} \\\\ C_{2231} &amp; C_{2232} &amp; C_{2233} \\\\ C_{2331} &amp; C_{2332} &amp; C_{2333} \\\\ \\end{bmatrix} \\\\ \\begin{bmatrix} C_{3111} &amp; C_{3112} &amp; C_{3113} \\\\ C_{3211} &amp; C_{3212} &amp; C_{3213} \\\\ C_{3311} &amp; C_{3312} &amp; C_{3313} \\\\ \\end{bmatrix} \\begin{bmatrix} C_{3121} &amp; C_{3122} &amp; C_{3123} \\\\ C_{3221} &amp; C_{3222} &amp; C_{3223} \\\\ C_{3321} &amp; C_{3322} &amp; C_{3323} \\\\ \\end{bmatrix} \\begin{bmatrix} C_{3131} &amp; C_{3132} &amp; C_{3133} \\\\ C_{3231} &amp; C_{3232} &amp; C_{3233} \\\\ C_{3331} &amp; C_{3332} &amp; C_{3333} \\\\ \\end{bmatrix} \\end{bmatrix}\\] 这里的ijkl索引里，i可以看做是大的3x3矩阵的行，j可以看做是小的3x3矩阵的行，k可以看做是大的3x3矩阵的列，l可以看做是小的3x3矩阵的列。","link":"/2023/11/19/tensor/"},{"title":"跨页表格的题注","text":"使用latex写文章，尤其是和图像相关的文章时，常常会遇到需要用图片表格的情况。为了能够跨页，我们可以使用longtable。因为图片较大，如果没有刻意缩小，那么表格就会跨页。有的文章接收方会要求跨页表格在第二页顶部增加额外的题注来接上上一页的表格，以及第一个题注在表格顶部。 跨页题注 https://tex.stackexchange.com/questions/115195/table-captions-continued 给出了一个解决方法。 The package longtable allows you to define a header for the first page by \\endfirsthead a header for all next pages by \\endhead a footer fo all pages expect the last one by \\endfoot a footer for the last page by \\endlastfoot. 这段里的这几个command可以用于控制跨页表格的题注的内容。举例来说，如果我需要在长表格前有个题注，跨页时第二面的表格顶部也有题注，可以这么写： 123456789\\begin{longtable}{ccc}\\caption {Some Members of the Suggested Estimator}\\label{Tab:1}\\\\\\endfirsthead\\caption* {\\textbf{Table \\ref{Tab:1} Continued:} Blablabla}\\\\\\endhead\\endfoot\\endlastfoot%任意内容\\end{longtable} 完整代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374\\begin{longtable}{ccccc}\\caption {本文的算法在VoxCeleb1数据集上得到的结果}\\label{fig:result3}\\\\\\endfirsthead\\caption* {\\textbf{表 \\ref{fig:result3} 接上图:} 本文的算法在VoxCeleb1数据集上得到的结果}\\\\\\endhead\\endfoot\\endlastfoot\\diagbox{原图}{驱动帧} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-0-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-0-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-0-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-0-4.png} \\\\\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-1-0.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-1-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-1-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-1-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-1-4.png} \\\\\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-2-0.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-2-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-2-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-2-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-2-4.png} \\\\\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-3-0.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-3-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-3-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-3-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-3-4.png} \\\\\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-4-0.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-4-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-4-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-4-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-4-4.png} \\\\\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-5-0.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-5-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-5-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-5-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/1/gaussian-5-4.png} \\\\\\midrule\\diagbox{原图}{驱动帧} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-0-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-0-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-0-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-0-4.png} \\\\\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-1-0.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-1-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-1-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-1-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-1-4.png} \\\\\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-2-0.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-2-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-2-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-2-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-2-4.png} \\\\\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-3-0.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-3-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-3-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-3-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-3-4.png} \\\\\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-4-0.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-4-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-4-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-4-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-4-4.png} \\\\\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-5-0.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-5-1.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-5-2.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-5-3.png} &amp;\\includegraphics[width=20mm]{image/appendix1/table/vox/2/gaussian-5-4.png} \\\\\\end{longtable} 也就是说，把想放的内容放在end开头的这几个command之前就可以。上面的代码可以得到下面这个效果： 效果 如果要增加跨页每个表格顶部的横杠，可在\\endfirsthead前面加，比如： 12345678910\\begin{longtable}{ccc}\\caption {Some Members of the Suggested Estimator}\\label{Tab:1}\\\\\\toprule\\endfirsthead\\caption* {\\textbf{Table \\ref{Tab:1} Continued:} Blablabla}\\\\\\endhead\\endfoot\\bottomrule\\endlastfoot%任意内容\\end{longtable}","link":"/2022/05/06/table-caption-continued/"},{"title":"Applications of eigenvectors","text":"很多时候需要把已有问题转化为特征值问题。 有一个对称矩阵（有时候是对称半正定矩阵），我们想要找一个方便的基。 具体的优化问题。 Setup Given: Collection of data points \\(x_i\\) Age Weight Blood pressure Heart rate Find: Correlations between different dimensions Simplest Model One-dimensional subspace: \\[ \\mathbf{x_i} \\approx c_i \\mathbf{v}, \\quad \\mathbf{v} \\text{ unknown} \\] Equivalently: \\[ \\mathbf{x_i} \\approx c_i \\hat{\\mathbf{v}}, \\quad \\hat{\\mathbf{v}} \\text{ unknown with } ||\\hat{\\mathbf{v}}||_2 = 1 \\] Variational Idea \\[ \\text{minimize } \\sum_{i=1}^n ||\\mathbf{x_i} - \\text{proj}_{\\hat{\\mathbf{v}}} \\mathbf{x_i}||_2^2\\\\ \\text{subject to } ||\\hat{\\mathbf{v}}||_2 = 1 \\] What does the constraint do? Does not affect optimal \\(\\hat{\\mathbf{v}}\\). Removes scaling ambiguity. Geometric Interpretation Review from Last Lecture \\[ \\begin{aligned} &amp;\\min_{c_i} \\sum_i ||\\mathbf{x_i} - c_i \\hat{\\mathbf{v}}||_2^2 \\\\&amp;\\xRightarrow {c_i = \\mathbf{x_i}\\cdot \\hat{\\mathbf{v}}} \\sum_i ||\\mathbf{x_i} - (\\mathbf{x_i}\\cdot \\hat{\\mathbf{v}}) \\hat{\\mathbf{v}}||_2^2\\\\ &amp;= \\sum_i ||\\mathbf{x_i}||_2^2 - 2 \\sum_i (\\mathbf{x_i}\\cdot \\hat{\\mathbf{v}})^2 + \\sum_i (\\mathbf{x_i}\\cdot \\hat{\\mathbf{v}})^2\\\\ &amp;= \\sum_i ||\\mathbf{x_i}||_2^2 - \\sum_i (\\mathbf{x_i}\\cdot \\hat{\\mathbf{v}})^2\\\\ &amp;= \\text{const.} - \\sum_i (\\mathbf{x_i}\\cdot \\hat{\\mathbf{v}})^2\\\\ &amp;= \\text{const.} - ||X^T \\hat{\\mathbf{v}}||_2^2 \\\\ &amp;= \\text{const.} - \\hat{\\mathbf{v}}^T X X^T \\hat{\\mathbf{v}}, \\end{aligned} \\] \\[ X=\\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ \\mathbf{x_1} &amp; \\mathbf{x_2} &amp; \\cdots &amp; \\mathbf{x_n} \\\\ | &amp; | &amp; &amp; | \\end{bmatrix}\\\\ \\] \\[ X^T \\hat{\\mathbf{v}} = \\begin{bmatrix} \\mathbf{x_1}\\cdot \\hat{\\mathbf{v}} \\\\ \\mathbf{x_2}\\cdot \\hat{\\mathbf{v}} \\\\ \\vdots \\\\ \\mathbf{x_n}\\cdot \\hat{\\mathbf{v}} \\end{bmatrix} \\] Equivalent Optimization \\[ \\textrm{maximize } ||X^T \\hat{\\mathbf{v}}||_2^2\\\\ \\textrm{subject to } ||\\hat{\\mathbf{v}}||_2 = 1 \\] homogeneous quadratic function with quadratic constraint，有二次约束（尤其是norm为1）的齐次二次优化问题，可以转化为特征值问题。 \\[ \\Lambda(\\hat{\\mathbf{v}};\\lambda) = \\frac{1}{2} \\hat{\\mathbf{v}}^T X X^T \\hat{\\mathbf{v}} + \\lambda (\\frac{1}{2} - \\frac{1}{2} ||\\hat{\\mathbf{v}}||_2^2)\\\\ \\mathbf{0} = \\nabla_{\\hat{\\mathbf{v}}} \\Lambda(\\hat{\\mathbf{v}};\\lambda) = X X^T \\hat{\\mathbf{v}} - \\lambda \\hat{\\mathbf{v}}\\\\ \\Rightarrow X X^T \\hat{\\mathbf{v}} = \\lambda \\hat{\\mathbf{v}}\\\\ \\] \\[ \\mathbf{v} X X^T \\hat{\\mathbf{v}} = \\lambda \\mathbf{v} \\hat{\\mathbf{v}} = \\lambda\\\\ \\] 因此最优的\\(\\hat{\\mathbf{v}}\\)是\\(X X^T\\)的最大的特征值对应的特征向量。 End Goal Eigenvector of \\(X X^T\\) with largest eigenvalue. \"Firs principal component\" More after SVD. 问题转化 \\[ \\min \\frac{1}{2} x^T A x - x^T b \\rightarrow Ax = b\\\\ \\min \\frac{1}{2} x^T A x \\text{ s.t. } ||x||_2^2 = 1 \\rightarrow Ax = \\lambda x, A \\text{ positive semidefinite} \\] Physics Newton: \\[ \\mathbf{F} = m \\frac{d^2 \\mathbf{x}}{dt^2} \\] Hooke: \\[ \\mathbf{F} = -k (\\mathbf{x}-\\mathbf{y}) \\] \\[ \\mathbf{F}\\in \\mathbb{R}^{3n}, \\mathbf{X} \\in \\mathbb{R}^{3n}\\\\ \\mathbf{F}=M\\mathbf{X}''=A\\mathbf{x}\\\\ v:=x' \\] \\[ \\frac{d}{dt} \\begin{bmatrix} \\mathbf{x} \\\\ \\mathbf{v} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{0} &amp; I_{3n \\times 3n}\\\\ M^{-1}A &amp; \\mathbf{0} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ \\mathbf{v} \\end{bmatrix} \\] General Linear ODE \\(\\mathbf{y}\\)是关于时间的向量值函数，\\(\\mathbf{y}_i\\)是\\(B\\)的特征向量，\\(\\lambda_i\\)是特征值。 \\[ \\mathbf{y}' = B \\mathbf{y} \\\\ B \\mathbf{y}_i = \\lambda_i \\mathbf{y}_i\\\\ \\] 可以假设： \\[ \\mathbf{y}(0)= c_1 \\mathbf{y}_1 + c_2 \\mathbf{y}_2 + \\cdots + c_k \\mathbf{y}_k\\\\ \\] 此外，假设\\(\\mathbf{y}_i\\) span \\(\\mathbb{R}^n\\)，则： \\[ \\mathbf{y}(t) = \\sum_{i=1}^n c_i(t) \\mathbf{y}_i\\\\ B\\mathbf{y} = \\sum_{i=1}^n c_i(t) B \\mathbf{y}_i = \\sum_{i=1}^n c_i(t) \\lambda_i \\mathbf{y}_i\\\\ \\mathbf{y}'(t) = \\sum_{i=1}^n c_i'(t) \\mathbf{y}_i\\\\ \\] 因为\\(B\\mathbf{y} = \\mathbf{y}'\\): \\[ c_i'(t) = \\lambda_i c_i(t)\\\\ \\rightarrow c_i(t) = c_i(0) e^{\\lambda_i t}\\\\ \\mathbf{y}(t) = c_1 e^{\\lambda_1 t} \\mathbf{y}_1 + c_2 e^{\\lambda_2 t} \\mathbf{y}_2 + \\cdots + c_k e^{\\lambda_k t} \\mathbf{y}_k\\\\ \\] Linear Systems \\[ \\mathbf{b} = c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2 + \\cdots + c_k \\mathbf{x}_k\\\\ A \\mathbf{x} = \\mathbf{b}\\\\ \\Rightarrow \\mathbf{x} = \\frac{c_1}{\\lambda_1} \\mathbf{x}_1 + \\frac{c_2}{\\lambda_2} \\mathbf{x}_2 + \\cdots + \\frac{c_k}{\\lambda_k} \\mathbf{x}_k\\\\ \\] Verify: \\[ A \\mathbf{x} = \\sum_{i=1}^k \\frac{c_i}{\\lambda_i} A \\mathbf{x}_i = \\sum_{i=1}^k \\frac{c_i}{\\lambda_i} \\lambda_i \\mathbf{x}_i = \\sum_{i=1}^k c_i \\mathbf{x}_i = \\mathbf{b} \\] Organizing a Collection Setup Have: n items in a dataset \\(w_{ij} \\geq 0\\) similarity of items \\(i\\) and \\(j\\) \\(w_{ij} = w_{ji}\\) Want: \\(x_i\\) embedding on \\(\\mathbb{R}\\) 我们希望\\(w_{ij}\\)越大，\\(x_i\\)和\\(x_j\\)越接近: \\[ \\min_{\\{x_i\\}} E(\\mathbf{x}),\\\\ E(\\mathbf{x}) = \\sum_{i,j} w_{ij} (x_i - x_j)^2,\\\\ \\text{s.t. } ||\\mathbf{x}||_2^2 = 1 \\leftrightarrow \\sum_i x_i^2 = 1\\\\ \\mathbf{1} \\cdot \\mathbf{x} = 0 \\leftrightarrow \\sum_i x_i = 0 \\] Let \\(a:=w \\cdot \\mathbf{1}\\), \\[ \\begin{aligned} E(\\mathbf{x}) &amp;= \\sum_{i,j} w_{ij} (x_i^2 - 2x_i x_j + x_j^2) \\\\ &amp;= \\sum_{i,j} w_{ij} x_i^2 - 2 \\sum_{i,j} w_{ij} x_i x_j + \\sum_{i,j} w_{ij} x_j^2\\\\ &amp;= 2\\sum_{i} a_i x_i^2 - 2 \\sum_{i,j} w_{ij} x_i x_j\\\\ &amp;= 2x^T A x - 2x^T W x\\\\ &amp;= 2x^T (A - W) x, \\end{aligned}\\\\ A=diag(a)\\\\ \\] Fact: \\[ (A-W) \\mathbf{1} = diag(a) \\mathbf{1} - W \\mathbf{1} = a - a = 0 \\] Lagrange multiplier: \\[ \\Lambda = 2x^T (A-W) x - \\lambda (x^T x - 1) - \\mu (\\mathbf{1}^T x)\\\\ \\Rightarrow \\nabla_x \\Lambda = 4(A-W)x - 2 \\lambda x - \\mu \\mathbf{1} = 0\\\\ \\] Premultiply by \\(\\mathbf{1}^T\\) on both sides: \\[ \\begin{aligned} &amp;\\mathbf{1}^T 4(A-W)x - \\mathbf{1}^T 2 \\lambda x - \\mathbf{1}^T \\mu \\mathbf{1} \\\\ &amp;= 4 \\mathbf{1}^T (A-W)x - 2 \\lambda \\mathbf{1}^T x - \\mu n \\\\ &amp;= 0 - 0 - \\mu n = 0\\\\ &amp;\\Rightarrow \\mu = 0 \\end{aligned}\\\\ \\therefore \\Lambda = 0 \\Rightarrow 2(A-W)x = \\lambda x\\\\ E(\\mathbf{x}) = x^T (A-W) x = x^T \\lambda x = \\lambda x^T x = \\lambda \\] \\(\\mathbf{x}\\) is eigenvector of \\(A-W\\) with smallest \\(\\lambda\\not=0\\). \\(\\lambda\\) 不为0的原因是\\(E(\\mathbf{x})\\)不能为0，否则所有的\\(x_i\\)相等，与约束条件矛盾。 Definition Eigenvalue and eigenvector An eigenvector \\(\\mathbf{x}\\not=0\\) of \\(A \\in \\mathbb{R}^{n \\times n}\\) satisfies \\(A \\mathbf{x} = \\lambda \\mathbf{x}\\) for some \\(\\lambda \\in \\mathbb{R}\\); \\(\\lambda\\) is an eigenvalue. Complex eigenvalues and eigenvectors instead have \\(\\lambda \\in \\mathbb{C}\\) and \\(x \\in \\mathbb{C}^n\\). Scale doesn't matter! \\(\\rightarrow\\) can constrain \\(||\\mathbf{x}||_2 = 1\\). Spectrum and spectral radius The spectrum of \\(A\\) is the set of all eigenvalues of \\(A\\). The spectral radius \\(\\rho(A)\\) is the maximum value \\(|\\lambda|\\) over all eigenvalues \\(\\lambda\\) of \\(A\\). Eigenproblems in the Wild Optimize \\(A\\mathbf{x}\\) subject to \\(||\\mathbf{x}||_2 = 1\\).(important!) ODE/PDE problem: Closed solutions and approximations for \\(\\mathbf{y}' = B \\mathbf{y}\\). Critical points of Rayleigh quotient: \\(\\frac{\\mathbf{x}^T A \\mathbf{x}}{||\\mathbf{x}||_2^2}\\).","link":"/2023/12/22/Applied%20Numerical%20Algorithms/Applications-of-eigenvectors/"},{"title":"Condition number for linear systems","text":"Sensitivity and Conditioning Gaussian elimination works in theory, but what about floating point precision? How much can we trust \\(x_0\\) if \\(0&lt;||Ax_0 - b||_2 \\ll 1\\)(backwards error)? Perturbation Analysis How does \\(x\\) change if we solve \\((A + \\delta A)x = b + \\delta b\\)? Two viewpoints: Thanks to floating point precision, \\(A\\) and \\(b\\) are approximate. If \\(x_0\\) isn't the exact solution, what is the backward error? What is \"Small\" Vector norm A function \\(||\\cdot||: \\mathbb{R}^n \\rightarrow [0, \\infty)\\) satisfying: 1. \\(||x|| = 0 \\text{ iff } x = 0\\) 2. \\(||cx|| = |c| \\cdot ||x|| \\forall c \\in R, x \\in \\mathbb{R}^n\\) 3. \\(||x+y|| \\leq ||x|| + ||y|| \\forall x, y \\in \\mathbb{R}^n\\) Most Common Norm \\[ ||x||_2 = \\sqrt{x_1^2 + \\cdots + x_n^2} \\] p-Norms For \\(p \\geq 1\\), \\[ ||x||_p = (|x_1|^p + \\cdots + |x_n|^p)^{\\frac{1}{p}} \\] Taxicab/Manhattan norm: \\(||x||_1\\) 在图里的这个例子里，使用不同的norm得到的\\(x\\)的元素分布不一样。如果使用1-norm，得到的会是有很多0的稀疏向量。如果使用2-norm，结果就是更有可能是稠密的。 How are Norms the Same Equivalent norms Two norm \\(||\\cdot|| \\text{ and } ||\\cdot||'\\) are equivalent if there exist constants \\(c_{\\text{low}}, c_{\\text{high}} &gt; 0\\) such that \\(c_{\\text{low}}||x|| \\leq ||x||' \\leq c_{\\text{high}}||x|| \\text{ for all } x\\in \\mathbb{R}^n\\). Theorem: All norms on \\(\\mathbb{R}^n\\) are equivalent. Matrix Norms: \"Unrolled\" Construction \\[ A\\in \\mathbb{R}^{m\\times n} \\leftrightarrow a(:) \\in \\mathbb{R}^{mn}\\\\ ||A||_{\\text{Fro}} = \\sqrt{\\sum_{ij}a_{ij}^2} \\] Matrix Norms: \"Induced\" Construction \\[ ||A||:= \\max{\\{||Ax||: ||x|| = 1\\}} \\] 可以写成优化问题： \\[ \\max ||Ax||_2 \\text{ s.t. } \\frac{1}{2}||x||_2 = \\frac{1}{2} \\] 使用拉格朗日乘子法： \\[ n(x, \\lambda) = \\frac{1}{2}x^TA^TAx - \\lambda x^Tx\\\\ \\nabla n(x, \\lambda) = A^TAx - \\lambda x = 0\\\\ \\Rrightarrow A^TAx = \\lambda x\\\\ \\text{So } \\lambda = \\text{ largest eigenvalue of } A^TA\\\\ \\begin{aligned} ||A||_2 &amp;= \\sqrt{x^TA^TAx}\\\\ &amp;= \\sqrt{\\lambda} \\sqrt{x^Tx}\\\\ &amp;= \\sqrt{\\lambda}\\\\ &amp;= \\sqrt{\\text{max eigenvalue of } A^TA} \\\\ &amp;= \\sqrt{\\text{biggest singular value of } A} \\end{aligned} \\] Other Induced Norms: \\[ ||A||_1 = \\max_{j} \\sum_{i}|a_{ij}|\\\\ ||A||_{\\infty} = \\max_{i} \\sum_{j}|a_{ij}|\\\\ \\] \\[ \\begin{aligned} ||A||_\\infty &amp;= (\\max_x ||Ax||_\\infty \\text{ s.t. } ||x||_\\infty = 1) \\\\ &amp;= (\\max_{x, i} |(Ax)_i| \\text{ s.t. } |x_j| \\leq 1 \\forall j)\\\\ &amp;= \\max_{x,i} |\\sum_{j} A_{ij}x_j| \\text{ s.t. } |x_j| \\leq 1 \\forall j\\\\ &amp;\\le \\max_{x,i} \\sum_{j} |A_{ij}||x_j| \\text{ s.t. } |x_j| \\leq 1 \\forall j\\\\ &amp;\\le \\max_{x,i} \\sum_{j} |A_{ij}| \\text{ s.t. } |x_j| \\leq 1 \\forall j\\\\ \\end{aligned} \\] Take \\(x_j = \\text{sign}(A_{ij})\\), then the less-than-equal-to becomes equal-to. All matrix norms are equivalent. These are the special cases of vector norms. \\[ \\mathbb{R}^{m\\times n} \\cong \\mathbb{R}^{mn}\\\\ \\] Lemma Suppose \\(||A||\\) is induced. \\[ \\forall v\\neq 0, ||Av|| = ||v|| \\cdot ||A\\frac{v}{||v||}|| \\leq ||A|| \\cdot ||v|| \\] \\(||A\\frac{v}{||v||}|| \\leq ||A||\\) is the definition of induced norm. \\[ \\begin{aligned} ||AB|| &amp;= \\max_{||x|| = 1} ||A(Bx)|| \\\\ &amp; \\leq \\max_{||x|| = 1} ||A|| \\cdot ||Bx|| \\\\ &amp; = ||A|| ||B|| \\end{aligned} \\] Model Problem \\[ \\begin{aligned} (A+\\epsilon \\delta A)\\mathbf{x}(\\epsilon) &amp;= b + \\epsilon \\delta b\\\\ &amp;\\downarrow{\\frac{d}{d\\epsilon}} &amp;\\\\ \\delta A \\mathbf{x}(\\epsilon) + (A + \\epsilon \\delta A) \\mathbf{x}'(\\epsilon) &amp;= \\delta b\\\\ &amp;\\downarrow{\\epsilon = 0} &amp;\\\\ \\delta A \\mathbf{x}(0) + A \\mathbf{x}'(0) &amp;= \\delta b\\\\ &amp;\\Rightarrow \\mathbf{x}'(0) = A^{-1}\\delta b - A^{-1}\\delta A \\mathbf{x}(0)\\\\ \\end{aligned} \\] According to Taylor's formula: \\[ x(\\epsilon) = x(0) + \\epsilon x'(0) + O(\\epsilon^2) \\] Relative error: \\[ \\begin{aligned} \\frac{||x(\\epsilon) - x(0)|| }{||x(0)||} &amp;= \\frac{||\\epsilon x'(0) + O(\\epsilon^2)||}{||x(0)||} \\\\ &amp;= \\frac{||\\epsilon (A^{-1}\\delta b - A^{-1}\\delta A \\mathbf{x}(0)) + O(\\epsilon^2)||}{||x(0)||} \\\\ &amp; \\leq \\frac{|\\epsilon|}{||x(0)||} (||A^{-1}\\delta b|| + ||A^{-1}\\delta A \\mathbf{x}(0)||) + O(\\epsilon^2) \\\\ &amp; \\leq \\frac{|\\epsilon|}{||x(0)||} (||A^{-1}|| \\cdot ||\\delta b|| + ||A^{-1}|| \\cdot ||\\delta A|| \\cdot ||x(0)||) + O(\\epsilon^2) \\\\ &amp; \\leq |\\epsilon| \\cdot ||A^{-1}|| \\cdot ||A|| \\cdot (\\frac{||\\delta b||}{||A|| \\cdot ||x(0)||} + \\frac{||\\delta A||}{||A||}) + O(\\epsilon^2) \\\\ \\end{aligned} \\] Since \\(||A|| \\cdot ||x(0)|| \\geq ||Ax(0)|| = ||b||\\), \\[ \\begin{aligned} &amp;\\leq |\\epsilon| \\cdot ||A^{-1}|| \\cdot ||A|| \\cdot (\\frac{||\\delta b||}{||b||} + \\frac{||\\delta A||}{||A||}) + O(\\epsilon^2)\\\\ &amp; = |\\epsilon| k D + O(\\epsilon^2) \\end{aligned} \\] \\((\\frac{||\\delta b||}{||b||} + \\frac{||\\delta A||}{||A||})\\) is the relative error of input. \\(||A^{-1}|| \\cdot ||A||\\) is the condition number. \\[ ||A^{-1}|| \\neq \\frac{1}{||A||} \\] Small \\(k \\Rightarrow x\\) stable to changes in \\(b, A\\). Condition Number The condition number of \\(A\\in \\mathbb{R}^{n\\times n}\\) for a given matrix norm \\(||\\cdot||\\) is \\(cond(A) = ||A^{-1}|| \\cdot ||A||\\). Relative change: \\[ D :=\\frac{||\\delta b||}{||b||} + \\frac{||\\delta A||}{||A||}\\\\ \\frac{||x(\\epsilon) - x(0)|| }{||x(0)||} \\leq \\epsilon \\cdot D \\cdot k + O(\\epsilon^2) \\] Invariant to scaling (unlike determinant!); one for the identity. Condition Number of Induced Norms Because \\[ \\max_{||x||_2 = 1} ||A^{-1}x||_2 = \\frac{1}{\\min_{||x||_2 = 1} ||Ax||_2} \\] \\[ cond(A) = (\\max_{\\mathbf{x}\\neq 0} \\frac{||A\\mathbf{x}||}{||\\mathbf{x}||} )(\\min_{\\mathbf{y}\\neq 0} \\frac{||A\\mathbf{y}||}{||\\mathbf{y}||} )^{-1} \\] Chicken \\(\\Leftrightarrow\\) Egg Computing \\(||A^{-1}||\\) typically requires solving \\(Ax = b\\), but how do we know the reliability of \\(\\mathbf{x}\\)? What is the condition number of computing the condition number of \\(A\\)? What is the condition number of computing what the condition number is of computing the condition number of \\(A\\)? Instead Bound the condition number. Below: Problem is at least this hard Above: Problem is at most this hard Potential for Approximation \\[ ||A^{-1}x|| \\leq ||A^{-1}|| \\cdot ||x|| \\\\ \\Downarrow \\\\ cond(A) = ||A||||A^{-1}|| \\geq \\frac{||A||||A^{-1}x||}{||x||} \\]","link":"/2023/12/20/Applied%20Numerical%20Algorithms/Condition-number-for-linear-systems/"},{"title":"Cholesky factorization, sparse matrices","text":"Structured Linear Systems Gaussian elimination and/or LU can solve all the example problems above. But these systems can have special properties that make them easier or stabler to solve. Today's example: Positive definite, sparsity. min square problem: \\[ \\min_{x} ||Ax-b||^2_2 \\rightarrow A^TAx = A^Tb \\] \\[ M:=A^TA, \\] 1. Symmetric \\[ M^T = (A^TA)^T = A^TA = M \\] 2. Positive (Semi-)Definite B is positive semidefinite if for all \\(x \\in \\mathbb{R}^n, x^TMx \\geq 0\\). B is positive definite if \\(x^TMx &gt; 0\\) whenever \\(x \\neq 0\\). \\[ \\text{Take } v \\in \\mathbb{R}^n, v^TMv = v^TA^TAv = ||Av||^2_2 \\geq 0 \\] A ridiculously important Matrix \\[ A^TA \\] \"Gram matrix\" Pivoting for SPD \\(C\\) Solve \\(Cx = d\\) for \\(C\\) SPD. \\[ C = \\begin{bmatrix} c_{11} &amp; v^T \\\\ v &amp; \\tilde{C} \\end{bmatrix}, v \\in \\mathbb{R}^{n-1} \\] Forward substitution \\[ E=\\begin{bmatrix} \\frac{1}{\\sqrt{c_{11}}} &amp; 0^T \\\\ r &amp; I_{(n-1)\\times (n-1)} \\end{bmatrix} \\] It can be a problem if \\(c_{11} &lt; 0\\). Proposition: \\(c_{11} &gt; 0\\). Proof: \\[ e_1^TCe_1 = c_{11} &gt; 0, \\] \\(e_1\\) is the first standard basis vector,\\(e_1 = [1, 0, \\cdots, 0]^T\\). \\[ EC = \\begin{bmatrix} \\frac{1}{\\sqrt{c_{11}}} &amp; 0^T \\\\ r &amp; I_{(n-1)\\times (n-1)} \\end{bmatrix} \\begin{bmatrix} c_{11} &amp; v^T \\\\ v &amp; \\tilde{C} \\end{bmatrix} = \\begin{bmatrix} \\sqrt{c_{11}} &amp; \\frac{v^T}{\\sqrt{c_{11}}} \\\\ c_{11}r + v &amp; rv^T + \\tilde{C} \\end{bmatrix} \\] By definition, since \\(EC\\) is the result of a forward substitution, it is lower triangular. So \\(c_{11}r + v = 0\\). To maintain symmetry, we can post-multiply by \\(E^T\\). \\[ E^TCE = \\begin{bmatrix} \\sqrt{c_{11}} &amp; \\frac{v^T}{\\sqrt{c_{11}}} \\\\ 0 &amp; rv^T + \\tilde{C} \\end{bmatrix} \\begin{bmatrix} \\frac{1}{\\sqrt{c_{11}}} &amp; r^T \\\\ 0 &amp; I_{(n-1)\\times (n-1)} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\sqrt{c_{11}}r^T + \\frac{v^T}{\\sqrt{c_{11}}} \\\\ 0 &amp; rv^T + \\tilde{C} \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; rv^T + \\tilde{C} \\end{bmatrix} \\] Since \\(E^TCE\\) is symmetric, \\(\\sqrt{c_{11}}r^T + \\frac{v^T}{\\sqrt{c_{11}}}\\) is zero. What enabled this? Positive definiteness $$ existence of \\(\\sqrt{c_{11}}\\) Symmetry \\(\\rightarrow\\) apply \\(E\\) to both sides. 正定矩阵可以不是对称的。 Cholesky Factorization \\[ E_1^T E_2^T \\cdots E_n^T C E_n \\cdots E_2 E_1 = I\\\\ C = LL^T, L = E_1^{-1}E_2^{-1} \\cdots E_n^{-1} \\] If we replace \\(c_{11}\\) with \\(1\\), The 1 in the first row and column of \\(ECE^T\\) will be another value. So finally we get a diagonal matrix instead of a identity matrix. \\[ C = LDL^T\\] Reason to do this: 1. Get rid of square roots 2. \\(c_{11}\\) might be close to zero. separate row \\(k\\) and column \\(k\\). \\[ L= \\begin{bmatrix} L_{11} &amp; \\vec{0} &amp; 0 \\\\ \\vec{l_k^T} &amp; l_{kk} &amp; \\vec{0} \\\\ L_{31} &amp; \\vec{l_{k}'} &amp; L_{33} \\end{bmatrix} \\] \\[ \\begin{aligned} LL^T &amp;= \\begin{bmatrix} L_{11} &amp; \\vec{0} &amp; 0 \\\\ \\vec{l_k^T} &amp; l_{kk} &amp; \\vec{0} \\\\ L_{31} &amp; \\vec{l_{k}'} &amp; L_{33} \\end{bmatrix} \\begin{bmatrix} L_{11}^T &amp; \\vec{l_k} &amp; L_{31}^T \\\\ \\vec{0} &amp; l_{kk} &amp; \\vec{l_k'}^T \\\\ 0 &amp; \\vec{0} &amp; L_{33}^T \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} .. &amp; .. &amp; .. \\\\ \\vec{l_k}^T L_{11}^T &amp; ||\\vec{l_k}||^2 + l_{kk}^2 &amp; ..\\\\ .. &amp; .. &amp; .. \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} c_{11} &amp; .. &amp; .. \\\\ c_{k}^T &amp; c_{kk} &amp; ..\\\\ c_{31} &amp; c_{k}' &amp; .. \\end{bmatrix}\\\\ \\end{aligned} \\] \\[ L_{11}l_k = c_k\\text{:LT Solve}, O((k-1)^2)\\\\ c_{kk} = ||\\vec{l_k}||^2 + l_{kk}^2 \\\\ \\Rightarrow l_{kk}^2 = c_{kk} - ||\\vec{l_k}||^2 \\\\ \\Rightarrow l_{kk} = \\sqrt{c_{kk} - ||\\vec{l_k}||^2}\\\\ \\] \\(l_{kk}\\)取正负都可以，但是为了方便一般取正的。 算法是逐行进行的。首先\\(L_{11}\\)可以直接求出，然后从第二行开始，根据公式依次： 求解下三角线性方程组，\\(L_{11}l_k = c_k, O((k-1)^2)\\)得到\\(l_k\\)。 计算\\(l_{kk} = \\sqrt{c_{kk} - ||\\vec{l_k}||^2}\\) 因此整个算法的复杂度为\\(O(n^3)\\)。n行乘以每行的复杂度\\(O(n^2)\\)。 Interpretation of Cholesky What is \\(x^TCx\\)? \\[ x^TCx = x^TLL^Tx = ||L^Tx||^2_2 \\geq 0 \\] Storing Sparse Matrices Want \\(O(n)\\) storage if we have \\(O(n)\\) nonzeros. Examples: List of triplets \\((r, c, val)\\) For each row, matrix[r] holds a dictionary \\(c \\rightarrow A[r][c]\\) 直接使用高斯消元法的话，会导致稀疏矩阵变成稠密矩阵。 Avoiding fill Common strategy: Permute rows/columns Mostly heuristics constructions Minimizing fill in Cholesky is NP-complete Alternative strategy: Avoid Gaussian altogether Band Matrices Cyclic Matrices","link":"/2023/12/17/Applied%20Numerical%20Algorithms/Cholesky-factorization-sparse-matrices/"},{"title":"Eigenvalue iteration, deflation","text":"Two basic Properties Lemma: Every matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) has at least one (complex) eigenvalue. Proof: Assume \\(A\\not=0\\). Take any \\(\\mathbf{x} \\not= 0 \\Rightarrow \\{\\mathbf{x}, A\\mathbf{x}, A^2 \\mathbf{x}, \\cdots\\}\\) is linearly dependent. \\(\\Rightarrow \\exists c_i\\) not all zero s.t. \\[ \\sum_{k=0}^n c_k A^k \\mathbf{x} = 0\\\\ \\] For \\(f(z) = c_0 + c_1 z + \\cdots + c_n z^n\\). Fundamental theorem of algebra \\(\\Rightarrow \\exists m\\geq 1\\) roots \\(z_i \\in \\mathbb{C}\\) and \\(c\\not=0\\) s.t. \\(f(z) = c(z-z_1)(z-z_2)\\cdots(z-z_m)\\). So, \\[ \\mathbf{0} = c_0 \\mathbf{x} + c_1 A \\mathbf{x} + \\cdots + c_n A^n \\mathbf{x} = c (A-z_1 I) (A-z_2 I) \\cdots (A-z_m I) \\mathbf{x}, \\mathbf{x} \\not= 0\\\\ \\Rightarrow \\exists i \\text{ s.t. } (A-z_i I) \\mathbf{x} = 0, \\mathbf{x} \\not= 0 \\Leftrightarrow A-z_i I \\text{ has a null space}\\\\ \\Rightarrow (A-z_i I) \\mathbf{v} = 0 \\rightarrow A\\mathbf{v} = z_i \\mathbf{v} \\] Lemma: Eigenvectors corresponding to distinct eigenvalues are linearly independent. (See proof in page 113: Proposition 6.2 in the textbook) \\(\\rightarrow\\) at most \\(n\\) eigenvalues. Example of a matrix with only complex eigenvalues: Rotation matrix. Diagonalizablity Nondefective: \\(A\\in \\mathbb{R}^{n \\times n}\\) is nondefective or diagonalizable if its eigenvectors span \\(\\mathbb{R}^n\\). \\[ A\\mathbf{x}_i = \\lambda_i \\mathbf{x}_i\\\\ \\begin{aligned} A \\mathbf{X} &amp;= A \\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ \\mathbf{x_1} &amp; \\mathbf{x_2} &amp; \\cdots &amp; \\mathbf{x_n} \\\\ | &amp; | &amp; &amp; | \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} A \\mathbf{x_1} &amp; A \\mathbf{x_2} &amp; \\cdots &amp; A \\mathbf{x_n} \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} \\lambda_1 \\mathbf{x_1} &amp; \\lambda_2 \\mathbf{x_2} &amp; \\cdots &amp; \\lambda_n \\mathbf{x_n} \\end{bmatrix}\\\\ &amp;= X \\text{diag}(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n)\\\\ &amp;= \\mathbf{X} D \\end{aligned} \\] If \\(A\\) is nondefective, then \\(\\mathbf{X}\\) is invertible and \\(D = \\mathbf{X}^{-1} A \\mathbf{X}\\). Motivation for Spectral Theorem Is there a class of matrices that is guaranteed to be nondefective? Spoiler alert: Symmetric (and Hermitian) matrices. Extending to \\(\\mathbb{C}^{n \\times n}\\) Complex conjugate The complex conjugate of \\(z = a + bi\\in \\mathbb{C}\\) is \\(\\bar{z} = a - bi\\). Conjugate transpose The conjugate transpose of \\(A \\in \\mathbb{C}^{m \\times n}\\) is \\(A^{\\mathbf{H}}:=\\bar{A}^T\\). Hermitian \\(A \\in \\mathbb{C}^{n \\times n}\\) is Hermitian if \\(A = A^{\\mathbf{H}}\\). Properties Lemma: All eigenvalues of a Hermitian matrix are real. Proof: \\[ A\\in \\mathbb{C}^{n \\times n}, A\\mathbf{x} = \\lambda \\mathbf{x}, ||\\mathbf{x}||_2^2 = 1\\\\ \\begin{aligned} \\lambda &amp;= \\lambda \\cdot \\mathbf{1} \\\\&amp;= \\lambda \\cdot &lt;\\mathbf{x}, \\mathbf{x}&gt; \\\\&amp;= &lt;A\\mathbf{x}, \\mathbf{x}&gt; \\\\&amp;= (A\\mathbf{x})^T \\mathbf{\\overline{x}} \\\\&amp;= \\mathbf{x}^T A^{T} \\mathbf{\\overline{x}} \\\\&amp;= \\mathbf{x}^T \\overline{\\overline{A}^T \\mathbf{x}} \\\\&amp;= \\mathbf{x}^T \\overline{A \\mathbf{x}} \\\\&amp;= \\mathbf{x}^T \\overline{\\lambda \\mathbf{x}} \\\\ &amp;= \\overline{\\lambda} \\mathbf{x}^T \\overline{\\mathbf{x}} \\\\ &amp;= \\overline{\\lambda} \\end{aligned} \\\\ \\Rightarrow \\lambda \\in \\mathbb{R} \\] Lemma: Eigenvectors corresponding to distinct eigenvalues of Hermitian matrices must be orthogonal. Proof: \\[ A\\mathbf{x} = \\lambda \\mathbf{x}, A\\mathbf{y} = \\mu \\mathbf{y}, \\lambda \\not= \\mu\\\\ \\textrm{Prev} \\Rightarrow \\lambda, \\mu \\in \\mathbb{R}\\\\ \\begin{aligned} \\lambda &lt;\\mathbf{x}, \\mathbf{y}&gt; &amp;= &lt;\\lambda \\mathbf{x}, \\mathbf{y}&gt; \\\\ &amp;= &lt;A\\mathbf{x}, \\mathbf{y}&gt; \\\\ &amp;= &lt;\\mathbf{x}, A \\mathbf{y}&gt; \\\\ &amp;= &lt;\\mathbf{x}, \\mu \\mathbf{y}&gt; \\\\ &amp;= \\mu &lt;\\mathbf{x}, \\mathbf{y}&gt; \\end{aligned}\\\\ \\Rightarrow (\\lambda - \\mu) &lt;\\mathbf{x}, \\mathbf{y}&gt; = 0\\\\ \\Rightarrow &lt;\\mathbf{x}, \\mathbf{y}&gt; = 0 \\] Spectral Theorem Suppose \\(A \\in \\mathbb{C}^{n \\times n}\\) is Hermitian (if \\(A \\in \\mathbb{R}^{n \\times n}\\), suppose \\(A\\) is symmetric). Then \\(A\\) has exactly \\(n\\) orthonormal eigenvectors \\(\\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_n\\) with (possibly repeated) eigenvalues \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\). Full set: \\(D = X^T A X\\). We can diagonalize any symmetric matrix and find at least one eigenvector of any asymmetric matrix. Question: How do we find eigenvalues/eigenvectors in practice? Probably different from linear algebra class. Find eigenvalues Setup \\[ A \\in \\mathbb{R}^{n \\times n}, \\text{ symmetric}\\\\ \\mathbf{x}_1, \\mathbf{x}_2, \\cdots, \\mathbf{x}_n \\in \\mathbb{R}^n \\text{ eigenvectors}\\\\ |\\lambda_1| \\geq |\\lambda_2| \\geq \\cdots \\geq |\\lambda_n| \\text{ eigenvalues}\\\\ \\] Usual Trick \\[ \\mathbf{v} \\in \\mathbb{R}^n\\\\ \\Downarrow \\\\ \\mathbf{v} = c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2 + \\cdots + c_n \\mathbf{x}_n\\\\ \\] Only works for nondefective matrices. \\[ A \\mathbf{v} = c_1 A \\mathbf{x}_1 + c_2 A \\mathbf{x}_2 + \\cdots + c_n A \\mathbf{x}_n\\\\ = c_1 \\lambda_1 \\mathbf{x}_1 + c_2 \\lambda_2 \\mathbf{x}_2 + \\cdots + c_n \\lambda_n \\mathbf{x}_n\\\\ A^2 \\mathbf{v} = c_1 \\lambda_1^2 \\mathbf{x}_1 + c_2 \\lambda_2^2 \\mathbf{x}_2 + \\cdots + c_n \\lambda_n^2 \\mathbf{x}_n\\\\ \\vdots\\\\ A^k \\mathbf{v} = c_1 \\lambda_1^k \\mathbf{x}_1 + c_2 \\lambda_2^k \\mathbf{x}_2 + \\cdots + c_n \\lambda_n^k \\mathbf{x}_n\\\\ = \\lambda_1^k (c_1 \\mathbf{x}_1 + c_2 (\\frac{\\lambda_2}{\\lambda_1})^k \\mathbf{x}_2 + \\cdots + c_n (\\frac{\\lambda_n}{\\lambda_1})^k \\mathbf{x}_n)\\\\ \\approx \\lambda_1^k c_1 \\mathbf{x}_1 \\] For large \\(k\\), \\(A^k \\mathbf{v} \\approx \\lambda_1^k c_1 \\mathbf{x}_1\\).(assuming \\(\\lambda_2 &lt; \\lambda_1\\) and \\(c_1 \\not= 0\\)) 可以随机初始化\\(\\mathbf{v}\\)，然后迭代\\(A^k \\mathbf{v}\\)，最后得到和\\(\\mathbf{x}_1\\)平行的向量。然后取单位向量，就是\\(\\mathbf{x}_1\\) Normalized Power Iteration \\[ \\mathbf{w}_k = A \\mathbf{v}_{k-1}\\\\ \\mathbf{v}_k = \\frac{\\mathbf{w}_k}{||\\mathbf{w}_k||}\\\\ \\] Eigenvalues of Inverse Matrix \\[ A\\mathbf{v} = \\lambda \\mathbf{v}\\Rightarrow A^{-1} \\mathbf{v} = \\frac{1}{\\lambda} \\mathbf{v}\\\\ |\\lambda_1| \\geq |\\lambda_2| \\geq \\cdots \\geq |\\lambda_n|\\\\ \\Rightarrow |\\frac{1}{\\lambda_1}| \\leq |\\frac{1}{\\lambda_2}| \\leq \\cdots \\leq |\\frac{1}{\\lambda_n}|\\\\ \\] Inverse Iteration \\[ \\mathbf{w}_k = A^{-1} \\mathbf{v}_{k-1}\\\\ \\mathbf{v}_k = \\frac{\\mathbf{w}_k}{||\\mathbf{w}_k||}\\\\ \\] 直接求解\\(A\\mathbf{w}_k = \\mathbf{v}_{k-1}\\)是\\(O(n^3)\\)的，但是可以用LU分解，可以做到\\(O(n^2)\\): \\[ \\textrm{Solve } L \\mathbf{y_k} = \\mathbf{v}_{k-1}\\\\ \\textrm{Solve } U \\mathbf{w}_k = \\mathbf{y}_k\\\\ \\textrm{Normalize } \\mathbf{v}_k = \\frac{\\mathbf{w}_k}{||\\mathbf{w}_k||}\\\\ \\] Eigenvalues of Shifted Matrix \\[ A\\mathbf{v} = \\lambda \\mathbf{v}\\Rightarrow (A-\\sigma I) \\mathbf{v} = (\\lambda - \\sigma) \\mathbf{v}\\\\ \\] Shifted Inverse Iteration To find eigenvalue closest to \\(\\sigma\\): \\[ \\mathbf{v}_{k-1} = \\frac{(A-\\sigma I)^{-1} \\mathbf{v}_{k-1}}{||(A-\\sigma I)^{-1} \\mathbf{v}_{k-1}||}\\\\ \\] Counterintuitive: Conditioning problems when \\(\\sigma = \\lambda\\) are limited. Heuristic: Convergence Rate \\[ A^k \\mathbf{v} = \\lambda_1^k (c_1 \\mathbf{x}_1 + c_2 (\\frac{\\lambda_2}{\\lambda_1})^k \\mathbf{x}_2 + \\cdots + c_n (\\frac{\\lambda_n}{\\lambda_1})^k \\mathbf{x}_n)\\\\ \\] 要使得收敛速度快，可以选择\\(\\sigma\\)使得: \\[ |\\frac{\\lambda_2-\\sigma}{\\lambda_1-\\sigma}| &lt; |\\frac{\\lambda_2}{\\lambda_1}|\\\\ \\] 可以在每次迭代中更新\\(\\sigma\\)，使得\\(\\sigma\\)逼近\\(\\lambda_1\\)。 Least-Squares Approximation If \\(\\mathbf{v}_0\\) is approximately an eigenvector: \\[ \\min_{\\lambda} ||A\\mathbf{v}_0 - \\lambda \\mathbf{v}_0||_2^2 \\\\ obj = \\mathbf{v}_0^T A^T A \\mathbf{v} - 2 \\lambda \\mathbf{v}_0^T A \\mathbf{v} + \\lambda^2 \\mathbf{v}_0^T \\mathbf{v}_0\\\\ \\nabla_{\\lambda} obj = -2 \\mathbf{v}_0^T A \\mathbf{v} + 2 \\lambda \\mathbf{v}_0^T \\mathbf{v}_0 = 0\\\\ \\lambda = \\frac{\\mathbf{v}_0^T A \\mathbf{v}}{||\\mathbf{v}_0||_2^2}\\\\ \\] Rayleigh Quotient Iteration \\[ \\mathbf{w}_k = (A-\\sigma_k I)^{-1} \\mathbf{v}_{k-1}\\\\ \\mathbf{v}_k = \\frac{\\mathbf{w}_k}{||\\mathbf{w}_k||}\\\\ \\sigma_k = \\frac{\\mathbf{v}_k^T A \\mathbf{v}_k}{\\mathbf{v}_k^T \\mathbf{v}_k}\\\\ \\] Efficiency per iteration vs. number of iterations? 这个收敛速度比shifted inverse iteration快，但是矩阵每次迭代都更新，因此不能用LU分解，复杂度是\\(O(n^3)\\)。 Unlikely Failure Mode for Iteration \\[ \\mathbf{v} = c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2 + \\cdots + c_n \\mathbf{x}_n\\\\A^k \\mathbf{v} = \\lambda_1^k (c_1 \\mathbf{x}_1 + c_2 (\\frac{\\lambda_2}{\\lambda_1})^k \\mathbf{x}_2 + \\cdots + c_n (\\frac{\\lambda_n}{\\lambda_1})^k \\mathbf{x}_n)\\\\ \\] What happens when \\(\\mathbf{v}_0 \\cdot \\mathbf{x}_1 = 0\\)? It converges to \\(\\lambda_2\\). Strategy for Multiple Eigenvalues Compute \\(\\mathbf{x}_0\\) via power iteration. Project \\(\\mathbf{x}_0\\) out of \\(\\mathbf{v}_0\\). Compute \\(\\mathbf{x}_1\\) via power iteration. Project \\(\\text{span}(\\mathbf{x}_0, \\mathbf{x}_1)\\) out of \\(\\mathbf{v}_0\\).(将\\(\\mathbf{v}_0\\)的分量投影到\\(\\mathbf{x}_0\\)和\\(\\mathbf{x}_1\\)的正交补上) ... Assumption: \\(A\\) is nondefective. Another Strategy \\(P\\) projects out \\(\\mathbf{v}_1, \\mathbf{v}_2, \\cdots, \\mathbf{v}_{k}\\). 即\\(P\\mathbf{v}\\)把\\(\\mathbf{v}\\)的分量投影到\\(\\mathbf{v}_1, \\mathbf{v}_2, \\cdots, \\mathbf{v}_{k}\\)的正交补上（投影结果与\\(\\mathbf{v}_1, \\mathbf{v}_2, \\cdots, \\mathbf{v}_{k}\\)垂直）。 \\[ AP\\mathbf{v}_i = \\begin{cases} A\\cdot 0 = 0 &amp; i \\leq k\\\\ A \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i &amp; i &gt; k \\end{cases} \\Rightarrow \\mathbf{v}_i \\cdot \\begin{cases} 0 &amp; i \\leq k\\\\ \\lambda_i &amp; i &gt; k \\end{cases}\\\\ \\] 因此每次要找下一个特征值，可以通过构造\\(AP\\)并且用power iteration对于\\(AP\\)进行迭代。 Avoiding Numerical Drift Do power iteration on \\(P^T A P\\) where \\(P\\) projects out known eigenvectors. Deflation: Modify \\(A\\) so that power iteration reveals another eigenvector you have not yet computed. Similarity Transformations Similar matrices: Two matrices \\(A, B \\in \\mathbb{R}^{n \\times n}\\) are similar if there exists \\(T\\) with \\(B = T^{-1} A T\\). Proposition: Similar matrices have the same eigenvalues. Householder Asymmetric Deflation \\[ H\\mathbf{x}_1 = \\mathbf{e}_1\\\\ \\Rightarrow H A H^T \\mathbf{e}_1 = H A \\mathbf{x}_1 = H \\lambda_1 \\mathbf{x}_1 = \\lambda_1 \\mathbf{e}_1\\\\ H A H^T = \\begin{bmatrix} \\lambda_1 &amp; \\mathbf{b}^T \\\\ \\mathbf{0} &amp; B \\end{bmatrix}\\\\ \\] Similarity transform of \\(A\\Rightarrow\\) same eigenvalues. Next eigenvalue: Power iteration on \\(B\\).","link":"/2023/12/23/Applied%20Numerical%20Algorithms/Eigenvalue-iteration-deflation/"},{"title":"Introduction, number systems, measuring error","text":"Mathematically correct != numerically sound. Using Tolerance: 1234567double x = 1.0;double y = x / 3.0;if(fabs(x-y*3.0) &lt; numeric_limits&lt;double&gt;::epsilon()){ cout &lt;&lt; &quot;They are equal&quot; &lt;&lt; endl;}else cout &lt;&lt; &quot;They are not equal&quot; &lt;&lt; endl; Sources of Error Rounding example: Using IEEE 754 Discretization Modeling Example: Neglecting butterfly wing flapping in a weather model Input Example: Measuring error for initial conditions in physical system Absolute vs. Relative Error Absolute Error: The difference between the approximate value and the underlying true value Relative Error: Absolute error divided by the true value Relative Error: Difficulty Problem: Generally not computable Common fix: Be conservative Computable Measures of Success Root-finding problem: For \\(f: \\mathbb{R} \\rightarrow \\mathbb{R}\\), find \\(x^*\\) such that \\(f(x^*) = 0\\). Actual output: \\(x_{est}\\) with \\(|f(x_{est})| &lt;&lt;1\\). Backward Error The amount the problem statement would have to change to make the approximate solution exact. Conditioning Well conditioned: Small backward error \\(\\xRightarrow{}\\) small relative error Poorly conditioned: Otherwise Condition number: Ratio of forward to backward error (we want it to be small) Root-finding example: \\(\\frac{1}{|f'(x^*)|}\\) Example: \\(||x||_2\\) 12345double normSquared = 0;for(int i = 0; i &lt; n; i++){ normSquared += x[i]*x[i];}return sqrt(normSquared); Overflow, underflow. Improved \\(||x||_2\\) 12345678910double maxElement = epsilon;for(int i = 0; i &lt; n; i++){ maxElement = max(maxElement, fabs(x[i]));}for(int i = 0; i &lt; n; i++){ double scaled = x[i]/maxElement; normSquared += scaled*scaled;}return sqrt(normSquared)*maxElement; Motivation for Kahan Algorithm \\[ ((a+b) - a) - b \\xlongequal{?} 0 \\] Store compensation value!","link":"/2023/12/16/Applied%20Numerical%20Algorithms/Introduction-number-systems-measuring-error/"},{"title":"Linear Systems and LU","text":"Linear Systems \\[ \\begin{aligned} Ax &amp;= b \\\\ A &amp;\\in \\mathbb{R}^{n \\times n} \\\\ x &amp;\\in \\mathbb{R}^n \\\\ b &amp;\\in \\mathbb{R}^n \\end{aligned} \\] Case 1: Solvable \\[ \\begin{aligned} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} &amp;= \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} \\end{aligned} \\] Completely determined Case 2: No solution \\[ \\begin{aligned} \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} &amp;= \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} \\end{aligned} \\] Overdetermined Case 3: Infinitely many solutions \\[ \\begin{aligned} \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} &amp;= \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix} \\end{aligned} \\] Underdetermined No Other Cases Proposition If \\(Ax = b\\) has two distinct solutions \\(x_0\\) and \\(x_1\\), then it has infinitely many solutions. Proof \\[ \\begin{aligned} t\\in \\mathbb{R} \\\\ A(tx_0 + (1-t)x_1) &amp;= tAx_0 + (1-t)Ax_1 \\\\ &amp;= tb + (1-t)b \\\\ &amp;= b \\end{aligned} \\] Dependence on Shape Proposition Tall matrices admit unsolvable right hand sides. Proof \\[ A \\in \\mathbb{R}^{m \\times n}, m &gt; n,\\\\ \\text{every col} \\in \\mathbb{R}^m, \\text{n total}. \\\\ \\begin{aligned} n &lt; m &amp;\\xRightarrow{} \\exists v \\notin col(A)\\\\ &amp;\\xRightarrow{} Ax = v \\text{ unsolvable} \\end{aligned} \\] Proposition Wide matrices admit right hand sides with infinitely many solutions. Proof \\[ \\begin{aligned} m &lt; n &amp;\\xRightarrow{} \\text{cols are linearly dependent} \\\\ &amp;\\xRightarrow{}\\exists y, s.t. Ay = 0, y \\neq 0 \\\\ &amp;\\xRightarrow{} A(x + y) = Ax = b \\end{aligned} \\] For this Lecture All matrices will be: Square Invertible(nonsingular) Foreshadowing Even rounding entries of a matrix can change it from non-invertible to invertible.(but ill-conditioned) \\[ \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; \\epsilon \\end{bmatrix} \\] Row Operations: Permutation \\[ \\sigma: \\{1, 2, \\dots, m\\} \\rightarrow \\{1, 2, \\dots, m\\} \\\\ P_\\sigma := \\begin{bmatrix} - &amp; e_{\\sigma(1)}^T &amp; - \\\\ - &amp; e_{\\sigma(2)}^T &amp; - \\\\ - &amp; \\vdots &amp; - \\\\ - &amp; e_{\\sigma(m)}^T &amp; - \\\\ \\end{bmatrix} \\] \\(e_{i}^TA\\) is the \\(i\\)th row of \\(A\\). Row Operations: Row Scaling \\[ S_a := \\begin{bmatrix} a_1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; a_2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; a_m \\end{bmatrix} \\] \"Scale row k by c and add to row l\": \\[ (I+ce_le_k^T)A \\] Inverting Matrices Common rule of thumb: Do not compute \\(A^{-1}\\) if you do not need it. Not the same as solving \\(Ax = b\\). Can be slow and poorly conditioned. Steps of Gaussian Elimination Forward substitution: For each row \\(i\\): Scale row to get pivot 1 For each j &gt; i, subtract multiple of row i from row j to zero out pivot column Backward substitution: For each row \\(i = m, m-1, \\dots, 1\\): For each j &lt; i, zero out rest of column Total runtime: \\(O(n^3)\\) 总共n行，每一行需要处理其下面的\\(O(n)\\)行，每次处理一行有\\(O(n)\\)次减法，总共\\(O(n^3)\\) Pivoting Permute rows and/or columns to avoid dividing by small numbers or zero. Partial pivoting Full pivoting LU Observation: Triangular systems can be solved in \\(O(n^2)\\) time \\[ E_m \\dots E_2E_1Ax = E_m \\dots E_2E_1b \\\\ \\] \\(E_m \\dots E_2E_1 = L^{-1}\\) is lower triangular. row scaling - diagonal forward substitution - \\(I + ce_le_j^T, l &gt; j\\) - lower triangular \\(L^{-1}A\\) is upper triangular. \\[ L^{-1}A = U \\] Why is \\(L^{-1}\\) triangular? \\[ S:= diag(a_1, a_2, \\dots) \\\\ S^{-1} = diag(\\frac{1}{a_1}, \\frac{1}{a_2}, \\dots) \\\\ E := I + ce_le_k^T, l &gt; k \\\\ E^{-1} = I - ce_le_k^T, l &gt; k \\\\ \\] Proposition: Product of lower triangular matrices is LT proof: \\[ \\text{Take A,B LT} \\xRightarrow{} a_{ij} = b_{ij} = 0 \\forall j &gt; i \\\\ \\text{Define } C = AB. \\text{Suppose } j &gt; i \\\\ C_{ij} = \\sum_{k} A_{ik}B_{kj} = A_{i1}B_{1j} + \\dots + A_{ij}B_{jj} + \\dots + A_{in}B_{nj} = 0 \\\\ \\] Pivoting by Swapping Columns \\[ (E_k \\dots E_2E_1)A(P_1 \\dots P_{l-1}) = U \\\\ \\xRightarrow{} A = LUP \\] elimination, permutation \\[ Ax = b \\\\ LUPx = b \\\\ x = P^TL^{-1}U^{-1}b \\] Linear regression Normal equations: \\(A^TAx = A^Tb\\) \\(A^TA\\) is Gram matrix Regulization Tikhonov regularization(\"ridge regression;\" Gauss prior): \\[ \\min_x ||Ax - b||_2^2 + \\alpha ||x||_2^2 \\] Lasso(Laplace prior): \\[ \\min_x ||Ax - b||_2^2 + \\alpha ||x||_1 \\] Elastic net: \\[ \\min_x ||Ax - b||_2^2 + \\alpha ||x||_1 + \\beta ||x||_2^2 \\] Example: Image Alignedment \\[ y_k = Ax_k + b \\\\ A \\in \\mathbb{R}^{2 \\times 2}, b \\in \\mathbb{R}^2 \\\\ \\min_{A,b} \\sum_{k=1}^n ||Ax_k + b - y_k ||_2^2 \\] Example: Mesh Parameterization \\[ G = (V, E) \\\\ \\textrm{Variable: } x_i \\in \\mathbb{R}^{2}, \\textrm{position of vertex i} \\\\ \\min_{x_1, \\dots, x_n} \\sum_{(i,j) \\in E} ||x_i - x_j||_2^2\\\\ \\textrm{s.t } x_i \\textrm{ fixed } \\forall i \\in \\textrm{boundary} \\] \"Tutte parameterization\" \"Harmonic parameterization\": A secret differential equation model hiding in the objective function \\[ \\int ||\\nabla f||^2 \\]","link":"/2023/12/17/Applied%20Numerical%20Algorithms/Linear-Systems-and-LU/"},{"title":"Column Space QR","text":"High-Level Idea Why QR? \\[ \\begin{aligned} cond A^T A &amp;= ||A^T A|| \\cdot ||(A^T A)^{-1}||\\\\ &amp;\\approx ||A^T|| \\cdot ||A|| \\cdot ||A^{-1}|| \\cdot ||A^{-T}|| &amp;= cond A^2 \\end{aligned} \\] 为了避免计算\\(A^T A\\)，我们可以使用QR分解。 QR Factorization \\[ A = QR\\\\ Q^T Q = I\\\\ R \\text{ is upper triangular} \\\\ A^T A = R^T Q^T Q R = R^T R\\\\ \\] \\[ \\begin{aligned} A^T A x &amp;= A^T b \\\\ R^T R x &amp;= R^T Q^T b \\\\ R x &amp;= Q^T b \\\\ \\rightarrow x &amp;= R^{-1} Q^T b \\end{aligned} \\] 前提是\\(R\\)可逆。已知\\(Q,R\\)之后求解出\\(x\\)的复杂度为\\(O(n^2)\\)（上三角求解）。 \\(Ax = b\\)的几何解释 \\(x\\)可以认为是使得组成\\(A\\)的列向量的加权和这个向量离\\(b\\)最近的向量时对应的权重向量。 Ax = b的几何解释 当\\(A\\)的列向量之间的角度很小时，很小的扰动也会使得结果非常不稳定。 When Isn't \\(A^T A\\) Ill-Conditioned? \\[ \\textrm{cond } I_{n \\times n} = 1\\\\ (\\textrm{w.r.t. } ||\\cdot||_2)\\\\ \\] Desirable: \\(A^T A\\approx I_{n \\times n}(then, \\textrm{cond } A^T A \\approx 1!)\\) Doesn't mean \\(A=I_{n \\times n}\\). Interpreting \\(A^T A = I_{n \\times n}\\) \\[ Q^T Q = \\begin{bmatrix} - &amp; q_1 &amp; - \\\\ - &amp; q_2 &amp; - \\\\ - &amp; \\vdots &amp; - \\\\ - &amp; q_n &amp; - \\end{bmatrix} \\begin{bmatrix} | &amp; | &amp; | &amp; | \\\\ q_1 &amp; q_2 &amp; \\cdots &amp; q_n \\\\ | &amp; | &amp; | &amp; | \\end{bmatrix}= \\begin{bmatrix} q_1 \\cdot q_1 &amp; q_1 \\cdot q_2 &amp; \\cdots &amp; q_1 \\cdot q_n \\\\ q_2 \\cdot q_1 &amp; q_2 \\cdot q_2 &amp; \\cdots &amp; q_2 \\cdot q_n \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ q_n \\cdot q_1 &amp; q_n \\cdot q_2 &amp; \\cdots &amp; q_n \\cdot q_n \\\\ \\end{bmatrix} \\] When \\(Q^T Q = I_{n \\times n}\\) \\[ q_i \\cdot q_j = \\begin{cases} 1 &amp;\\textrm{when } i = j \\\\ 0 &amp;\\textrm{when } i \\neq j \\end{cases} \\] Orthonormal; orthogonal matrix A set of vectors \\(\\{v_1, v_2, \\cdots, v_n\\}\\) is orthonormal if \\(||v_i||=1\\) for all \\(i\\) and \\(v_i \\cdot v_j = 0\\) for all \\(i \\neq j\\). A square matrix whose columns are orthonormal is called an orthogonal matrix. Isometry Property \\[ ||Q\\mathbf{x}||^2 = (Q\\mathbf{x})^T (Q\\mathbf{x}) = \\mathbf{x}^T Q^T Q \\mathbf{x} = \\mathbf{x}^T \\mathbf{x}\\\\ (Q\\mathbf{x})\\cdot(Q\\mathbf{y}) = \\mathbf{x}^T Q^T Q \\mathbf{y} = \\mathbf{x} \\cdot \\mathbf{y} \\] Isometry Alternative Intuition for Least-Squares \\[ A^T A x = A^T b\\leftrightarrow \\min_x ||Ax - b||_2 \\] \\(Ax = b\\)的解是\\(b\\)在\\(A\\)的列空间上的投影。\\(\\textrm{proj}_{\\textrm{col } A} b\\)。 Observation Lemma: Column space invariance For any \\(A \\in \\mathbb{R}^{m \\times n}\\) and invertible \\(B \\in \\mathbb{R}^{n \\times n}\\), \\(\\textrm{col } A = \\textrm{col } AB\\). proof: \\[ \\begin{aligned} \\text{Take } b \\in \\textrm{col } A &amp;\\Rightarrow \\exists x \\textrm{ s.t. } Ax = b\\\\ &amp;\\Rightarrow AB(B^{-1}x) = b\\\\ &amp;\\Rightarrow b \\in \\textrm{col } AB \\end{aligned} \\] \\[ \\begin{aligned} \\text{Take } c \\in \\textrm{col } AB &amp;\\Rightarrow \\exists y \\textrm{ s.t. } ABy = c\\\\ &amp;\\Rightarrow A(By) = c\\\\ &amp;\\Rightarrow c \\in \\textrm{col } A \\end{aligned} \\] New Strategy Apply column operations to \\(A\\) until it is orthogonal; then, solve least-squares on the resulting \\(Q\\). \\[ AR_1R_2 \\cdots R_k = Q \\] Preliminary: Vector Projection \"Which multiple of \\(a\\) is closest to \\(b\\)?\" \\[ \\min_c ||ca - b||_2^2 = c^2 ||a||_2^2 - 2c(a \\cdot b) + ||b||_2^2\\\\ \\xRightarrow{\\text{take derivative and set to 0}} c = \\frac{a \\cdot b}{||a||_2^2}\\\\ \\Rightarrow \\textrm{proj}_a b = ca = \\frac{a \\cdot b}{||a||_2^2}a \\] Check \\[ a \\cdot (b - \\textrm{proj}_a b) = a \\cdot b - \\frac{a \\cdot b}{||a||_2^2}a \\cdot a = 0 \\] Orthonormal Projection Suppose \\(\\hat{a}_1, \\cdots, \\hat{a}_k\\) are orthonormal. \\[ \\textrm{proj}_{\\hat{a}_i} b = (\\hat{a}_i \\cdot b)\\hat{a}_i \\] \\[ ||c_1 \\hat{a}_1 + \\cdots + c_k \\hat{a}_k - b||_2^2 = \\sum_{i=1}^k c_i^2 - 2c_i(\\hat{a}_i \\cdot b) + ||b||_2^2\\\\ \\Rightarrow c_i = b \\cdot \\hat{a}_i \\Rightarrow \\textrm{proj}_{\\textrm{span} \\{\\hat{a}_1, \\cdots, \\hat{a}_k\\}} b = (\\hat{a}_1 \\cdot b)\\hat{a}_1 + \\cdots + (\\hat{a}_k \\cdot b)\\hat{a}_k \\] Gram-Schmidt Orthogonalization To orthogonalize \\(v_1, \\cdots, v_k\\): \\(\\hat{a}_1 := \\frac{v_1}{||v_1||_2}\\) For \\(i\\) from 2 to \\(k\\): \\(p_i := \\textrm{proj}_{\\textrm{span} \\{\\hat{a}_1, \\cdots, \\hat{a}_{i-1}\\}} v_i\\) \\(\\hat{a}_i := \\frac{v_i - p_i}{||v_i - p_i||_2}\\) Claim \\(\\textrm{span} \\{v_1, \\cdots, v_i\\} = \\textrm{span} \\{\\hat{a}_1, \\cdots, \\hat{a}_i\\}\\) for all \\(i\\). Implementation via Column Operations Post-multiplication Rescaling to unit length: diagonal matrix Subtracting off projection: upper triangular substitution matrix \\[ A = QR \\] \\(Q\\) orthogonal \\(R\\) upper-triangular Bad Case 1703182976906 \\[ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\end{bmatrix}, \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 1 + \\epsilon \\\\ \\end{bmatrix} \\] Two strategies for QR Post-multiply by upper triangular matrices Pre-multiply by orthogonal matrices Reflection Matrices Reflection \\[ \\begin{aligned} b+2(\\textrm{proj}_{\\mathbf{v}} b -b) &amp;= 2 \\mathbf{v}\\frac{\\mathbf{v}^T b}{\\mathbf{v}^T \\mathbf{v}} - b\\\\ &amp;= (2\\frac{\\mathbf{v} \\mathbf{v}^T}{\\mathbf{v}^T \\mathbf{v}} - I) b\\\\ \\end{aligned} \\] \\[ H_\\mathbf{v}:= (I-2\\frac{\\mathbf{v} \\mathbf{v}^T}{\\mathbf{v}^T \\mathbf{v}}) \\] Analogy to Forward Substitution If \\(a\\) is first column, \\[ c\\mathbf{e}_1 = H_\\mathbf{v} a \\Rightarrow c\\mathbf{e}_1 = (I-2\\frac{\\mathbf{v} \\mathbf{v}^T}{\\mathbf{v}^T \\mathbf{v}}) a\\\\ = a - 2\\frac{\\mathbf{v} \\mathbf{v}^T a}{\\mathbf{v}^T \\mathbf{v}}\\\\ \\Rightarrow \\mathbf{v} = (a - c\\mathbf{e}_1) \\frac{\\mathbf{v}^T \\mathbf{v}}{2 \\mathbf{v}^T a} \\] Choose \\(\\mathbf{v}=a-c\\mathbf{e}_1\\). \\[ \\begin{aligned} &amp;\\Rightarrow \\mathbf{v} = \\mathbf{v} \\cdot \\frac{\\mathbf{v}^T \\mathbf{v}}{2 \\mathbf{v}^T a}\\\\ &amp;\\Rightarrow 1 = \\frac{\\mathbf{v}^T \\mathbf{v}}{2 \\mathbf{v}^T a}= \\frac{||a||_2^2 - 2c \\mathbf{e}_1 \\cdot a + c^2}{2 (||a||_2^2 - c \\mathbf{e}_1 \\cdot a)}\\\\ &amp;\\Rightarrow c^2 = ||a||_2^2\\\\ &amp;\\Rightarrow c = \\pm ||a||_2 \\end{aligned} \\] \\(c\\)可以是0，所以即使\\(A\\)的列向量线性相关也可以进行QR分解。 这样找出的\\(\\mathbf{v}\\)其实是一个对称轴，使得\\(a\\)关于这个轴对称之后得到\\(\\mathbf{e}_1\\)。 \\[ H_\\mathbf{v} A = \\begin{bmatrix} c &amp; \\times &amp; \\times &amp; \\times \\\\ 0 &amp; \\times &amp; \\times &amp; \\times \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; \\times &amp; \\times &amp; \\times \\\\ \\end{bmatrix} \\] Householder QR \\[ \\begin{aligned} R &amp;= H_{v_n} \\cdots H_{v_1} A\\\\ Q &amp;= H_{v_1}^T \\cdots H_{v_n}^T \\end{aligned} \\] Can store \\(Q\\) implicitly by storing \\(v_i\\)'s. Slightly Different Output Gram-Schmidt: \\(Q\\in \\mathbb{R}^{m \\times n}, R \\in \\mathbb{R}^{n \\times n}\\) Householder: \\(Q\\in \\mathbb{R}^{m \\times m}, R \\in \\mathbb{R}^{m \\times n}\\) Typical least-squares case: \\(A \\in \\mathbb{R}^{m \\times n}\\) has \\(m \\gg n\\). Reduced QR 如果我们想尽量节省空间并且利用Householder，可以只存储\\(Q\\)的部分内容。 \\[ \\begin{aligned} A &amp;= QR\\\\ &amp;= \\begin{bmatrix} &amp; Q_1 &amp; | &amp; Q_2 &amp; \\\\ \\end{bmatrix} \\begin{bmatrix} \\times &amp; \\times &amp; \\times \\\\ 0 &amp; \\times &amp; \\times \\\\ 0 &amp; 0 &amp; \\times \\\\ \\hline 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\\\ &amp;= \\begin{bmatrix} &amp; Q_1 &amp; | &amp; Q_2 &amp; \\\\ \\end{bmatrix} \\begin{bmatrix} R_1 \\\\ \\hline 0 \\\\ \\end{bmatrix}\\\\ &amp;= Q_1 R_1 \\end{aligned} \\] 即存储\\(Q=Q_1\\)，在这种情况下\\(Q\\)满足： \\[ Q^T Q = I_{n \\times n} \\] 但是不满足 \\[ Q Q^T = I_{m \\times m} \\]","link":"/2023/12/21/Applied%20Numerical%20Algorithms/QR-factorization/"},{"title":"QR iteration","text":"Assumption: \\(A \\in \\mathbb{R}^{n \\times n}\\) is symmetric, no repeated eigenvalues \\(\\lambda\\)s. QR iteration \\[ (Q, R) \\Leftarrow \\text{QR}(A)\\\\ A \\Leftarrow RQ \\] \\[ \\begin{aligned} A_1 &amp;= A\\\\ \\text{Factor} \\quad A_k &amp;= Q_k R_k\\\\ \\text{Multiply} \\quad A_{k+1} &amp;= R_k Q_k\\\\ R_k Q_k &amp;= Q_{k+1} R_{k+1}\\\\ \\end{aligned} \\] Commutativity Lemma: Take \\(A, B \\in \\mathbb{R}^{n \\times n}\\). Suppose that the eigenvectors of \\(A\\) span \\(\\mathbb{R}^n\\) and have distinct eigenvalues. Then, \\(AB = BA\\) if and only if \\(A\\) and \\(B\\) have the same set of eigenvectors (with possibly different eigenvalues). Proof: Suppose \\(A, B\\) have eigenvectors \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\) with eigenvalues \\(\\lambda_1^{A}, \\dots, \\lambda_n^{A}\\) and \\(\\lambda_1^{B}, \\dots, \\lambda_n^{B}\\). Take \\(y\\in \\mathbb{R}^n \\Rightarrow y = \\sum_{i=1}^n a_i \\mathbf{x}_i\\). \\[ BAy = BA\\sum_{i=1}^n a_i \\mathbf{x}_i = \\sum_{i=1}^n a_i BA \\mathbf{x}_i = \\sum_{i=1}^n a_i \\lambda_i^A B \\mathbf{x}_i = \\sum_{i=1}^n a_i \\lambda_i^A \\lambda_i^B \\mathbf{x}_i\\\\ ABy = AB\\sum_{i=1}^n a_i \\mathbf{x}_i = \\sum_{i=1}^n a_i AB \\mathbf{x}_i = \\sum_{i=1}^n a_i \\lambda_i^B A \\mathbf{x}_i = \\sum_{i=1}^n a_i \\lambda_i^A \\lambda_i^B \\mathbf{x}_i\\\\ \\] So, \\(BAy = ABy\\) for all \\(y \\in \\mathbb{R}^n \\Rightarrow BA = AB\\). Suppose \\(AB = BA\\), Take \\(A\\mathbf{x} = \\lambda \\mathbf{x}\\). \\[ \\Rightarrow A(B\\mathbf{x}) = B(A\\mathbf{x}) = B(\\lambda \\mathbf{x}) = \\lambda B\\mathbf{x}\\\\ 1. B\\mathbf{x} \\neq 0 \\Rightarrow B\\mathbf{x} \\text{ is an eigenvector of } A \\text{ with eigenvalue } \\lambda.\\\\ B\\mathbf{x} = c\\mathbf{x} \\text{ for some } c \\in \\mathbb{R}.\\\\ \\] The last equality is because there is an assumption that \\(A\\) has no repeated eigenvalues. \\(B\\mathbf{x}\\) and \\(\\mathbf{x}\\) are both eigenvectors of \\(A\\) with the same eigenvalue \\(\\lambda\\). So, \\(B\\mathbf{x} = c\\mathbf{x}\\) for some \\(c \\in \\mathbb{R}\\). \\[ 2. B\\mathbf{x} = 0 \\Rightarrow \\mathbf{x} \\text{ is an eigenvector of } B \\text{ with eigenvalue } 0. \\] So, for any eigenvector \\(\\mathbf{x}\\) of \\(A\\), \\(\\mathbf{x}\\) is an eigenvector of \\(B\\). If QR iteration converges \\[ A_{\\infty} = Q_{\\infty} R_{\\infty} = R_{\\infty} Q_{\\infty} \\leftrightarrow A_{\\infty} = Q_{\\infty}^T A_{\\infty} Q_{\\infty} \\Rightarrow Q_{\\infty} A_{\\infty} = A_{\\infty} Q_{\\infty} \\] Viewpoint 1 The eigenvalues of \\(A\\) equal the diagonal elements of \\(R_{\\infty}\\) Claim: \\(Q_{\\infty}\\) has all eigenvalues w/ magnitude 1. Suppose \\(A_{\\infty} \\mathbf{x} = \\lambda \\mathbf{x} = Q_{\\infty} R_{\\infty} \\mathbf{x} = R_{\\infty} Q_{\\infty} \\mathbf{x} = \\pm R_{\\infty} \\mathbf{x}\\). It is easy to prove that determinant of a triangular matrix is the product of its diagonal entries. So, The eigenvalues of \\(A_{\\infty}-\\) and hence the eigenvalues of \\(A-\\) equal the diagonal elements of \\(R_{\\infty}\\) up to sign. Viewpoint 2 \\(A_k = QR\\), \\(Q\\) is a set of near-eigenvectors of \\(A\\) Suppose the columns of \\(A\\) are given by \\(\\vec{a}_1, \\dots, \\vec{a}_n\\), and consider the matrix \\(A_k\\) for large \\(k\\). We can write: \\[ A^k = A^{k-1} \\cdot A = \\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ A^{k-1} \\vec{a}_1 &amp; A^{k-1} \\vec{a}_2 &amp; \\cdots &amp; A^{k-1} \\vec{a}_n \\\\ | &amp; | &amp; &amp; | \\end{bmatrix} \\] By our derivation of power iteration, in the absence of degeneracies, the first column of \\(A^k\\) will become more and more parallel to the eigenvector \\(\\vec{x}_1\\) of A with largest magnitude \\(|\\lambda_1|\\) as \\(k\\rightarrow \\infty\\), since we took a vector \\(\\vec{a}_1\\) and multiplied it by \\(A\\) many times. 把\\(A^k\\)的第二列投影到第一列的正交补上，然后左乘\\(A^{k-1}\\)，这样就会收敛到第二大的特征值对应的特征向量上。这正好和QR分解的步骤一致。因此，\\(A_k = QR\\)可以得到\\(Q\\)的列向量是\\(A\\)的特征向量的近似，并且是按照特征值的大小从大到小排列，而\\(R\\)的对角线元素是特征值的近似。 \\[ \\begin{aligned} A^1 &amp;= Q_1 R_1\\\\ A^2 &amp;= Q_1 R_1 Q_1 R_1 = Q_1 Q_2 R_2 R_1\\\\ A^3 &amp;= Q_1 Q_2 R_2 R_1 Q_1 R_1 = Q_1 Q_2 R_2 Q_2 R_2 R_1 = Q_1 Q_2 Q_3 R_3 R_2 R_1\\\\ \\vdots\\\\ A^k &amp;= Q_1 Q_2 \\cdots Q_k R_k R_{k-1} \\cdots R_1\\\\ A^k &amp;= Q_{\\infty}^{''} R_{\\infty}^{''} \\end{aligned} \\] 因此按照之前关于Power Iteration的讨论，\\(Q_{\\infty}^{''}=Q_1 Q_2 \\cdots Q_k\\)会收敛到\\(A\\)的特征向量上，而\\(R_{\\infty}^{''}=R_k R_{k-1} \\cdots R_1\\)会收敛到\\(A\\)的特征值上。 \\[ \\begin{aligned} A_1 &amp;= A\\\\ A_2 &amp;= R_1 Q_1 = Q_1^T A Q_1 (R_k = Q_k^T A)\\\\ A_3 &amp;= R_2 Q_2 = Q_2^T A_2 Q_2 = Q_2^T Q_1^T A Q_1 Q_2 \\\\ \\vdots\\\\ A_k &amp;= R_k Q_k = (Q_1 \\cdots Q_k)^T A (Q_1 \\cdots Q_k)\\rightarrow \\text{diag}(\\lambda) \\end{aligned} \\] 由于\\(Q_1 \\cdots Q_k\\)收敛到\\(A\\)的特征向量上，根据对角化的定义，\\(A_k\\)收敛到\\(A\\)的特征值上。 In practice Tridiagonalize before eigenvalue iteration More advanced Krylov subspace methods Use existing software as you can!","link":"/2023/12/23/Applied%20Numerical%20Algorithms/QR-iteration/"},{"title":"SVD","text":"Understading the Geometry of \\(A\\in \\mathbb{R}^{m\\times n}\\) \\[ A\\in \\mathbb{R}^{m\\times n} \\\\ x\\in \\mathbb{R}^n \\mapsto Ax \\in \\mathbb{R}^m \\] What we are formalizing is that there is a major axis and a minor axis to this ellipse and we are stretching in the major and minor axes and rotating the ellipse out of the coordinate plane. Critical point of the ratio: \\[ R(\\mathbf{v}) = \\frac{||A\\mathbf{v}||_2}{||\\mathbf{v}||_2} \\] Since \\(R(\\alpha \\mathbf{v}) = R(\\mathbf{v})\\), we can assume \\(||\\mathbf{v}||_2 = 1\\). \\[ \\max_{\\mathbf{v}}||A\\mathbf{v}||_2 \\quad \\text{subject to } ||\\mathbf{v}||_2 = 1 \\rightarrow A^TA\\mathbf{v} = \\lambda \\mathbf{v}, \\lambda \\geq 0 \\] According to Spectral Theorem, \\(A^TA\\) is symmetric and has \\(n\\) real eigenvalues and \\(n\\) orthogonal eigenvectors \\(\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\) which span \\(\\mathbb{R}^n\\). Singular Value Decomposition Define \\(u_i^* = A\\mathbf{v}_i\\). \\[ \\begin{aligned} &amp;A^TA\\mathbf{v}_i = \\lambda_i \\mathbf{v}_i \\\\ &amp;\\Rightarrow A^Tu_i^* = \\lambda_i \\mathbf{v}_i \\\\ &amp;\\Rightarrow A A^Tu_i^* = \\lambda_i A\\mathbf{v}_i = \\lambda_i u_i^* \\\\ \\end{aligned} \\] So, \\(u_i^*\\) is an eigenvector of \\(AA^T\\) when it is nonzero. \\[ ||u_i^*||_2 = ||A\\mathbf{v}_i||_2 = \\sqrt{\\mathbf{v}_i^T A^TA \\mathbf{v}_i} = \\sqrt{\\mathbf{v}_i^T \\lambda_i \\mathbf{v}_i} = \\sqrt{\\lambda_i} \\] Two cases: \\(\\lambda_i \\neq 0 \\Rightarrow u_i^* = A\\mathbf{v}_i\\) is an eigenvector of \\(AA^T\\) with \\(||u_i^*||_2 = \\sqrt{\\lambda_i}\\) \\(\\lambda_i = 0, u_i^* = 0\\) Take \\(k:= \\text{\\# nonzero eigenvalues of } A^TA\\). \\[ \\exists \\mathbf{v}_1, \\dots, \\mathbf{v}_k \\in \\mathbb{R}^n, u_1, \\dots, u_k \\in \\mathbb{R}^m \\text{ with: }\\\\ \\begin{aligned} A^TA\\mathbf{v}_i &amp;= \\lambda_i \\mathbf{v}_i\\\\ AA^Tu_i &amp;= \\lambda_i u_i(u_i = u_i^* / ||u_i^*||_2)\\\\ A\\mathbf{v}_i &amp;= \\sqrt{\\lambda_i} u_i\\\\ A^Tu_i &amp;= \\sqrt{\\lambda_i} \\mathbf{v}_i\\\\ \\end{aligned}\\\\ \\] Observation \\[ u_i^TA\\mathbf{v}_j = (A^Tu_i)^T \\mathbf{v}_j = \\sqrt{\\lambda_i} \\mathbf{v}_i^T \\mathbf{v}_j = \\sqrt{\\lambda_i} \\delta_{ij}\\\\ \\] \\[ \\overline{U} = \\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ u_1 &amp; u_2 &amp; \\dots &amp; u_k \\\\ | &amp; | &amp; &amp; | \\\\ \\end{bmatrix} \\in \\mathbb{R}^{m\\times k} \\\\ \\overline{V} = \\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ \\mathbf{v}_1 &amp; \\mathbf{v}_2 &amp; \\dots &amp; \\mathbf{v}_k \\\\ | &amp; | &amp; &amp; | \\\\ \\end{bmatrix} \\in \\mathbb{R}^{n\\times k} \\\\ \\overline{U}^TA\\overline{V} = \\overline{\\Sigma},\\\\ \\overline{\\Sigma} = \\text{diag}(\\sqrt{\\lambda_1}, \\dots, \\sqrt{\\lambda_k}) \\in \\mathbb{R}^{k\\times k} \\] Complete \\(\\overline{U} \\rightarrow U \\in \\mathbb{R}^{m\\times m}\\) and \\(\\overline{V} \\rightarrow V \\in \\mathbb{R}^{n\\times n}\\) by adding nullspace vectors (orthonormal columns). Define \\(\\Sigma \\in \\mathbb{R}^{m\\times n}\\) s.t. \\[ \\Sigma_{ij} = \\begin{cases} \\sqrt{\\lambda_i}=\\sigma_i &amp; i = j \\leq k \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\\\ A = U\\Sigma V^T \\\\ U^TU = I_{m\\times m}, V^TV = I_{n\\times n} \\] \\(\\Sigma\\) is a diagonal matrix with nonnegative entries. Geometry of SVD \\[ A = U\\Sigma V^T \\\\ \\] Rotate (\\(V^T\\)) Scale (\\(\\Sigma\\)) Rotate (\\(U\\)) SVD Vocabulary Left singular vectors: Columns of \\(U\\); span \\(\\text{col } A\\) Right singular vectors: Columns of \\(V\\); span \\(\\text{row } A\\) Singular values: Diagonal \\(\\sigma_i\\) of \\(\\Sigma\\); sort \\(\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq 0\\) Second Approach to SVD: Algebraic \\[ B:=\\begin{bmatrix} 0 &amp; A^T \\\\ A &amp; 0 \\end{bmatrix} \\] Proposition: Take \\(\\mathbf{x} = (\\mathbf{x}_1, \\mathbf{x}_2) \\in \\mathbb{R}^{m+n}\\) to be an eigenvector of \\(B\\) with eigenvalue \\(\\lambda\\), where \\(\\mathbf{x}_1 \\in \\mathbb{R}^m, \\mathbf{x}_2 \\in \\mathbb{R}^n\\). Then, \\(\\mathbf{x}':=(\\mathbf{x}_1, -\\mathbf{x}_2)\\) is an eigenvector of \\(B\\) with eigenvalue \\(-\\lambda\\). On the board: \\(A=U\\Sigma V^T\\) for diagonal \\(\\Sigma\\)(\"economy\" SVD) Many SVD's Full SVD: \\(U \\in \\mathbb{R}^{m\\times m}, \\Sigma \\in \\mathbb{R}^{m\\times n}, V \\in \\mathbb{R}^{n\\times n}\\) Thin SVD: \\(U \\in \\mathbb{R}^{m\\times k}, \\Sigma \\in \\mathbb{R}^{k\\times k}, V \\in \\mathbb{R}^{n\\times k}\\), where \\(k = \\min\\{m, n\\}\\) Compact SVD: \\(U \\in \\mathbb{R}^{m\\times k}, \\Sigma \\in \\mathbb{R}^{k\\times k}, V \\in \\mathbb{R}^{n\\times k}\\), where \\(k = \\text{rank } (A)\\) Computing SVD: Two Strategies Build from eigenvectors of \\(A^TA\\) and \\(AA^T\\) Build from eigenvectors of \\(\\begin{bmatrix} 0 &amp; A^T \\\\ A &amp; 0 \\end{bmatrix}\\) \\(\\exists\\) more specialized methods! Solving Linear Systems with \\(A=U\\Sigma V^T\\) \\[ A\\mathbf{x} = \\mathbf{b}\\\\ \\Rightarrow U\\Sigma V^T \\mathbf{x} = \\mathbf{b}\\\\ \\Rightarrow \\mathbf{x} = V\\Sigma^{-1} U^T \\mathbf{b} \\] Uniting Short/Tall Matrices \\[ \\min_{\\mathbf{x}} ||A\\mathbf{x} - \\mathbf{b}||_2\\rightarrow A^TA\\mathbf{x} = A^T\\mathbf{b}\\\\ \\] 该约束可以转换为： \\[ \\textrm{minimize } ||\\mathbf{x}||_2^2 \\quad \\text{subject to } A^T A \\mathbf{x} = A^T \\mathbf{b} \\] 原因是原问题有三种情况： 1. \\(A\\)可逆(completely determined)，有唯一解 2. \\(A\\)超定(overdetermined)，有最小二乘解 3. \\(A\\)欠定(underdetermined)，有无穷多解 任何情况都满足\\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\)这个约束。 \\[ A=U\\Sigma V^T \\Rightarrow A^TA \\mathbf{x} = V\\Sigma^T U^T U\\Sigma V^T \\mathbf{x} = V\\Sigma^T \\Sigma V^T \\mathbf{x},\\\\ A^T \\mathbf{b} = V\\Sigma^T U^T \\mathbf{b}\\\\ \\therefore A\\mathbf{x} = \\mathbf{b} \\Rightarrow \\Sigma^T \\Sigma V^T \\mathbf{y} = \\mathbf{d}, \\mathbf{y}:= V^T \\mathbf{x}, \\mathbf{d}:= U^T \\mathbf{b}\\\\ \\] 可以转化为新的问题： \\[ \\min ||\\mathbf{y}||_2^2 \\quad \\text{s.t. } \\Sigma^T \\Sigma \\mathbf{y} = \\mathbf{d}\\\\ \\rightarrow \\sigma_i^2 y_i = d_i \\quad \\text{for } i = 1, \\dots, k \\Rightarrow y_i = \\begin{cases} \\frac{d_i}{\\sigma_i} &amp; \\sigma_i \\neq 0 \\\\ \\mathbf{0} &amp; \\sigma_i = 0 \\end{cases}\\\\ \\Rightarrow \\mathbf{y} = \\Sigma^{+} \\mathbf{d}, \\Sigma^{+} = \\text{diag}(\\frac{1}{\\sigma_i} \\text{if } \\sigma_i \\neq 0, 0 \\text{ otherwise})\\\\ \\Rightarrow \\mathbf{x} =V \\mathbf{y} = V\\Sigma^{+} U^T \\mathbf{b} \\] Pseudoinverse \\[ A^+ = V\\Sigma^{+} U^T \\] Properties \\(A\\) square and invertible \\(\\Rightarrow A^+ = A^{-1}\\) \\(A\\) overdetermined \\(\\Rightarrow A^+ \\mathbf{b}\\) gives least-squares solution to \\(A\\mathbf{x} \\approx \\mathbf{b}\\) \\(A\\) underdetermined \\(\\Rightarrow A^+ \\mathbf{b}\\) gives least-squares solution to \\(A\\mathbf{x} \\approx \\mathbf{b}\\) with least (Euclidean) norm Alternative Form \\[ A=U\\Sigma V^T \\Rightarrow A = \\sum_{i=1}^l \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T\\\\ l:= \\min\\{m, n\\}\\\\ A\\mathbf{x} = \\sum_{i=1}^l \\sigma_i \\mathbf{u}_i (\\mathbf{v}_i^T \\mathbf{x}) \\approx \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T \\mathbf{x} \\] Outer Product \\[ \\mathbf{u} \\otimes \\mathbf{v} := \\mathbf{u} \\mathbf{v}^T \\] Computing \\(A\\mathbf{x}\\): \\[ A\\mathbf{x} = \\sum_{i=1} \\sigma_i (\\mathbf{v}_i \\cdot \\mathbf{x}) \\mathbf{u}_i \\] Trick: Ignore small \\(\\sigma_i\\). \\[ A^+ = \\sum_{\\sigma_i \\neq 0} \\frac{\\mathbf{v}_i \\mathbf{u}_i^T}{\\sigma_i} \\] Trick: Ignore large \\(\\sigma_i\\). Eckart-Young Theorem Suppose \\(\\tilde{A}\\) is obtained from \\(A=U\\Sigma V^T\\) by truncating all but the \\(k\\) largest singular values \\(\\sigma_i\\) of \\(A\\) to zero. Then, \\(\\tilde{A}\\) minimizes both \\(||A-\\tilde{A}||_{\\text{Fro}}\\) and \\(||A-\\tilde{A}||_2\\) subject to the constant that the column space of \\(\\tilde{A}\\) has at most dimension \\(k\\). Proof: \\[ A=U\\Sigma V^T, \\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_l \\] Define \\(A_k = \\sum_{i=1}^k \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T, k \\leq l\\). \\[ \\begin{aligned} ||A-A_k||_2 &amp;= ||\\sum_{i=k+1}^l \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T||_2 = ||M||_2 = \\sigma_{k+1} \\end{aligned} \\] The last equality is because: \\[ \\mathbf{x} = \\mathbf{v}_{k+1} \\Rightarrow M\\mathbf{x} = \\sum_{i=k+1}^l \\sigma_i \\mathbf{u}_i (\\mathbf{v}_i^T \\mathbf{v}_{k+1}) = \\sigma_{k+1} \\mathbf{u}_{k+1} \\Rightarrow ||M||_2 = \\sigma_{k+1} \\] Suppose \\(B = XY^T\\) (B is rank \\(k\\)), where \\(X \\in \\mathbb{R}^{m\\times k}, Y \\in \\mathbb{R}^{n\\times k}\\). Define \\(V_{k+1} = \\begin{bmatrix} \\mathbf{v}_1 &amp; \\dots &amp; \\mathbf{v}_{k+1} \\end{bmatrix} \\in \\mathbb{R}^{n\\times (k+1)}\\). We know \\(Y^T V_{k+1}\\) has a nullspace \\(\\Rightarrow \\exists \\mathbf{z} \\neq \\mathbf{0}\\) s.t. \\(Y^T V_{k+1} \\mathbf{z} = \\mathbf{0}\\). Scale \\(\\mathbf{z}\\) s.t. \\(||\\mathbf{z}||_2 = 1\\). \\[ \\begin{aligned} ||A-B||_2^2 &amp;\\geq \\max_{||\\mathbf{q}||_2 = 1} ||(A-B)\\mathbf{q}||_2^2 \\\\ &amp;\\geq ||(A-B)V_{k+1} \\mathbf{z}||_2^2 = ||(U\\Sigma V^T - XY^T)V_{k+1} \\mathbf{z}||_2^2 \\\\ &amp;= ||U\\Sigma V^T V_{k+1} \\mathbf{z} - XY^T V_{k+1} \\mathbf{z}||_2^2 \\\\ &amp;= ||U\\Sigma V^T V_{k+1} \\mathbf{z}||\\\\ &amp;= \\sum_{i=1}^{k+1} z_i^2 \\sigma_i^2 \\geq \\sigma_{k+1}^2 = ||A-A_k||_2^2 \\end{aligned} \\] Matrix Norm Expressions $$ ||A||_{}^2 = _i^2\\ ||A||_2 = {_i}\\ (A) =","link":"/2023/12/24/Applied%20Numerical%20Algorithms/SVD/"},{"title":"Hermite Interpolation","text":"Hermite插值多项式 Hermite插值是一种插值方法，可以通过给定的点和导数值构造插值多项式。给定点\\((x_0,y_0),(x_1,y_1),...,(x_n,y_n)\\)和导数值\\(y'_0,y'_1,...,y'_n\\)，可以构造插值多项式。本文介绍如何使用Newton差商生成Hermite插值多项式。 Newton差商 Newton差商公式是一个递归定义的公式，可以写成一个表格的形式： \\[ f[x_i,x_{i+1},...,x_{i+j}]=\\frac{f[x_{i+1},x_{i+2},...,x_{i+j}]-f[x_i,x_{i+1},...,x_{i+j-1}]}{x_{i+j}-x_i}\\tag{1} \\] 给定一组点\\((x_0,y_0),(x_1,y_1),...,(x_n,y_n)\\)，可以通过差商表格计算出插值多项式的系数。第一列和第二列分别是\\(x_i\\)和\\(y_i\\)，后面的列是差商，通过递归公式一列一列地计算出来。下面是一个生成差商表格的Python函数： 123456789101112131415161718192021import numpy as npimport matplotlib.pyplot as pltimport pandas as pdimport sympy as spdef generate_newton_diff_quotient_table(x, y): max_order = len(x) - 1 newton_diff_quotient_table = pd.DataFrame(np.zeros((len(x), max_order + 2))) for i in range(len(x)): newton_diff_quotient_table.iloc[i, 0] = x[i] newton_diff_quotient_table.iloc[i, 1] = y[i] for i in range(0, len(newton_diff_quotient_table.columns) - 2): for j in range(i + 1, len(newton_diff_quotient_table)): newton_diff_quotient_table.iloc[j, i + 2] = ( newton_diff_quotient_table.iloc[j, i + 1] - newton_diff_quotient_table.iloc[j - 1, i + 1] ) / ( newton_diff_quotient_table.iloc[j, 0] - newton_diff_quotient_table.iloc[j - i - 1, 0] ) return newton_diff_quotient_table 比如给定点\\((0,0),(1,16),(2,46),(3,94),(4,160)\\)，可以生成差商表格： 1234x = [0, 1, 2, 3, 4]y = [0, 16, 46, 94, 160]newton_diff_quotient_table = generate_newton_diff_quotient_table(x, y)print(newton_diff_quotient_table) 输出结果： 123456 0 1 2 3 4 50 0.0 0.0 0.0 0.0 0.000000 0.0000001 1.0 16.0 16.0 0.0 0.000000 0.0000002 2.0 46.0 30.0 7.0 0.000000 0.0000003 3.0 94.0 48.0 9.0 0.666667 0.0000004 4.0 160.0 66.0 9.0 0.000000 -0.166667 Newton插值 用Newton差商可以得到插值多项式的系数，然后可以用这些系数构造插值多项式。插值多项式的形式是： \\[ p(x)=f[x_0]+\\sum_{i=1}^{n}f[x_0,x_1,...,x_i]\\prod_{j=0}^{i-1}(x-x_j)\\tag{2} \\] 12345678910def newton_interpolation(tb): x = sp.symbols(&quot;x&quot;) n = len(tb.columns) - 2 p = tb.iloc[0, 1] for i in range(1, n + 1): term = tb.iloc[i, i + 1] for j in range(0, i): term *= x - tb.iloc[j, 0] p += term return p 绘制插值多项式： 12345678def plot_interpolation(p, x, y): xx = np.linspace(x[0], x[-1], 100) plt.plot(xx, [p.subs(&quot;x&quot;, i) for i in xx]) plt.scatter(x, y) plt.show()p = newton_interpolation(newton_diff_quotient_table)plot_interpolation(p, x, y) Hermite插值 在Newton差商表格的基础上，重复每个点两次，并且设置其导数值作为一阶差商。然后使用原来的计算方法生成差商表格，跳过已经计算过的差商（已知的导数），使用和Newton插值多项式一样的方法就可以得到Hermite插值多项式。 下面是一个用于生成Hermite插值多项式的生成Newton差商表格的Python函数： 123456789101112131415161718192021def generate_hermite_diff_quotient_table(x, y, y_prime): max_order = len(x) * 2 - 1 newton_diff_quotient_table = pd.DataFrame(np.zeros((len(x) * 2, max_order + 2))) newton_diff_quotient_table.iloc[:, :] = 999999 for i in range(len(x)): newton_diff_quotient_table.iloc[i * 2, 0] = x[i] newton_diff_quotient_table.iloc[i * 2 + 1, 0] = x[i] newton_diff_quotient_table.iloc[i * 2, 1] = y[i] newton_diff_quotient_table.iloc[i * 2 + 1, 1] = y[i] newton_diff_quotient_table.iloc[i * 2 + 1, 2] = y_prime[i] for i in range(0, len(newton_diff_quotient_table.columns) - 2): for j in range(i + 1, len(newton_diff_quotient_table)): if newton_diff_quotient_table.iloc[j, i + 2] == 999999: newton_diff_quotient_table.iloc[j, i + 2] = ( newton_diff_quotient_table.iloc[j, i + 1] - newton_diff_quotient_table.iloc[j - 1, i + 1] ) / ( newton_diff_quotient_table.iloc[j, 0] - newton_diff_quotient_table.iloc[j - i - 1, 0] ) return newton_diff_quotient_table 比如给定点\\((0,0),(1,16),(2,46),(3,94),(4,160)\\)和导数值\\(0.5,0.8,1.2,1.8\\)，可以生成差商表格： 12345x = [0, 1, 2, 3, 4]y = [0, 16, 46, 94, 160]y_prime = [0.5, 0.5, 0.8, 1.2, 1.8]hermite_diff_quotient_table = generate_hermite_diff_quotient_table(x, y, y_prime)print(hermite_diff_quotient_table) 1234567891011 0 1 2 3 4 5 6 \\0 0.0 0.0 999999.0 999999.0 999999.0 999999.00 999999.000000 1 0.0 0.0 0.5 999999.0 999999.0 999999.00 999999.000000 2 1.0 16.0 16.0 15.5 999999.0 999999.00 999999.000000 3 1.0 16.0 0.5 -15.5 -31.0 999999.00 999999.000000 4 2.0 46.0 30.0 29.5 22.5 26.75 999999.000000 5 2.0 46.0 0.8 -29.2 -58.7 -40.60 -33.675000 6 3.0 94.0 48.0 47.2 38.2 48.45 29.683333 7 3.0 94.0 1.2 -46.8 -94.0 -66.10 -57.275000 8 4.0 160.0 66.0 64.8 55.8 74.90 47.000000 9 4.0 160.0 1.8 -64.2 -129.0 -92.40 -83.650000 然后可以用和Newton插值多项式一样的方法生成Hermite插值多项式： 12p = newton_interpolation(hermite_diff_quotient_table)plot_interpolation(p, x, y) 测试该插值多项式在给定点的导数值： 1234567891011121314151617181920212223def test_interpolation(p, x, y, y_prime=None): if ( abs(p.subs(&quot;x&quot;, x) - y) &lt; 1e-10 and (y_prime is None or abs(p.diff(&quot;x&quot;).subs(&quot;x&quot;, x) - y_prime) &lt; 1e-10) ): return True else: print(&quot;p.subs('x',x) = {0}, y = {1}&quot;.format(p.subs(&quot;x&quot;, x), y)) if y_prime is not None: print( &quot;p.diff('x').subs('x',x) = {0}, y_prime = {1}&quot;.format( p.diff(&quot;x&quot;).subs(&quot;x&quot;, x), y_prime ) ) return Falsedef test_interpolations(p, x, y, y_prime=None): for i in range(len(x)): if not test_interpolation(p, x[i], y[i], y_prime[i] if y_prime is not None else None): return False return Truetest_interpolations(p, x, y, dydx) 返回True，说明插值多项式在给定点的导数值是正确的。","link":"/2024/10/30/hermite-interpolation/"}],"tags":[{"name":"Bspline","slug":"Bspline","link":"/tags/Bspline/"},{"name":"数学","slug":"数学","link":"/tags/%E6%95%B0%E5%AD%A6/"},{"name":"几何处理","slug":"几何处理","link":"/tags/%E5%87%A0%E4%BD%95%E5%A4%84%E7%90%86/"},{"name":"计算机图形学","slug":"计算机图形学","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"CIS-5620","slug":"CIS-5620","link":"/tags/CIS-5620/"},{"name":"仿真","slug":"仿真","link":"/tags/%E4%BB%BF%E7%9C%9F/"},{"name":"FEM","slug":"FEM","link":"/tags/FEM/"},{"name":"Projective Dynamics","slug":"Projective-Dynamics","link":"/tags/Projective-Dynamics/"},{"name":"软体模拟","slug":"软体模拟","link":"/tags/%E8%BD%AF%E4%BD%93%E6%A8%A1%E6%8B%9F/"},{"name":"图像动画","slug":"图像动画","link":"/tags/%E5%9B%BE%E5%83%8F%E5%8A%A8%E7%94%BB/"},{"name":"计算机视觉","slug":"计算机视觉","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"swapchian","slug":"swapchian","link":"/tags/swapchian/"},{"name":"交换链","slug":"交换链","link":"/tags/%E4%BA%A4%E6%8D%A2%E9%93%BE/"},{"name":"Vulkan","slug":"Vulkan","link":"/tags/Vulkan/"},{"name":"cmake","slug":"cmake","link":"/tags/cmake/"},{"name":"微积分","slug":"微积分","link":"/tags/%E5%BE%AE%E7%A7%AF%E5%88%86/"},{"name":"计算机组成原理","slug":"计算机组成原理","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"},{"name":"随笔","slug":"随笔","link":"/tags/%E9%9A%8F%E7%AC%94/"},{"name":"LLVM","slug":"LLVM","link":"/tags/LLVM/"},{"name":"vscode","slug":"vscode","link":"/tags/vscode/"},{"name":"静态分析","slug":"静态分析","link":"/tags/%E9%9D%99%E6%80%81%E5%88%86%E6%9E%90/"},{"name":"latex","slug":"latex","link":"/tags/latex/"},{"name":"plotneuralnet","slug":"plotneuralnet","link":"/tags/plotneuralnet/"},{"name":"神经网络可视化","slug":"神经网络可视化","link":"/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"name":"文章排版","slug":"文章排版","link":"/tags/%E6%96%87%E7%AB%A0%E6%8E%92%E7%89%88/"},{"name":"几何","slug":"几何","link":"/tags/%E5%87%A0%E4%BD%95/"},{"name":"光线跟踪","slug":"光线跟踪","link":"/tags/%E5%85%89%E7%BA%BF%E8%B7%9F%E8%B8%AA/"},{"name":"概率论","slug":"概率论","link":"/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"},{"name":"蒙特卡洛","slug":"蒙特卡洛","link":"/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B/"},{"name":"最优化","slug":"最优化","link":"/tags/%E6%9C%80%E4%BC%98%E5%8C%96/"},{"name":"模拟","slug":"模拟","link":"/tags/%E6%A8%A1%E6%8B%9F/"},{"name":"刚体","slug":"刚体","link":"/tags/%E5%88%9A%E4%BD%93/"},{"name":"数值计算","slug":"数值计算","link":"/tags/%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97/"},{"name":"线性代数","slug":"线性代数","link":"/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"},{"name":"Hermite","slug":"Hermite","link":"/tags/Hermite/"}],"categories":[{"name":"学习","slug":"学习","link":"/categories/%E5%AD%A6%E4%B9%A0/"},{"name":"计算机视觉","slug":"学习/计算机视觉","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"计算机图形学","slug":"学习/计算机图形学","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"MISC","slug":"学习/MISC","link":"/categories/%E5%AD%A6%E4%B9%A0/MISC/"},{"name":"数学","slug":"学习/数学","link":"/categories/%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6/"},{"name":"计算机组成原理","slug":"学习/计算机组成原理","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"图像动画","slug":"学习/计算机视觉/图像动画","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%83%8F%E5%8A%A8%E7%94%BB/"},{"name":"软件分析","slug":"学习/软件分析","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%BD%AF%E4%BB%B6%E5%88%86%E6%9E%90/"},{"name":"CIS-5620","slug":"学习/计算机图形学/CIS-5620","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/CIS-5620/"},{"name":"图形API","slug":"学习/计算机图形学/图形API","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E5%9B%BE%E5%BD%A2API/"},{"name":"CIS-5610","slug":"学习/计算机图形学/CIS-5610","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/CIS-5610/"},{"name":"项目管理","slug":"学习/MISC/项目管理","link":"/categories/%E5%AD%A6%E4%B9%A0/MISC/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"},{"name":"仿真","slug":"学习/计算机图形学/仿真","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E4%BB%BF%E7%9C%9F/"},{"name":"文章排版","slug":"学习/MISC/文章排版","link":"/categories/%E5%AD%A6%E4%B9%A0/MISC/%E6%96%87%E7%AB%A0%E6%8E%92%E7%89%88/"},{"name":"Applied Numerical Algorithms","slug":"学习/数学/Applied-Numerical-Algorithms","link":"/categories/%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6/Applied-Numerical-Algorithms/"},{"name":"Vulkan","slug":"学习/计算机图形学/图形API/Vulkan","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E5%9B%BE%E5%BD%A2API/Vulkan/"},{"name":"cmake","slug":"学习/MISC/项目管理/cmake","link":"/categories/%E5%AD%A6%E4%B9%A0/MISC/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/cmake/"},{"name":"GAMES103","slug":"学习/计算机图形学/仿真/GAMES103","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E4%BB%BF%E7%9C%9F/GAMES103/"},{"name":"FEM","slug":"学习/计算机图形学/仿真/FEM","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E4%BB%BF%E7%9C%9F/FEM/"},{"name":"latex","slug":"学习/MISC/文章排版/latex","link":"/categories/%E5%AD%A6%E4%B9%A0/MISC/%E6%96%87%E7%AB%A0%E6%8E%92%E7%89%88/latex/"},{"name":"plotneuralnet","slug":"学习/MISC/文章排版/plotneuralnet","link":"/categories/%E5%AD%A6%E4%B9%A0/MISC/%E6%96%87%E7%AB%A0%E6%8E%92%E7%89%88/plotneuralnet/"},{"name":"几何","slug":"学习/计算机图形学/几何","link":"/categories/%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/%E5%87%A0%E4%BD%95/"}],"pages":[{"title":"about","text":"喜欢看动画，懂一点点AI的图形学爱好者。如果我的文章有错误，或者是想交流，可以在下面留言或者发邮件与我联系。","link":"/about/index.html"},{"title":"","text":"google-site-verification: googlefd6bdd99b89cec5b.html","link":"/googlefd6bdd99b89cec5b.html"},{"title":"所有分类","text":"","link":"/categories/index.html"},{"title":"所有标签","text":"","link":"/tags/index.html"}]}